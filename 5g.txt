What is 5G?
Well, the initials stand for the fifth generation, more specifically fifth generation wireless. The latest iteration in cellular technology allows for even faster speeds and responsiveness regarding wireless networks, meaning that, by some accounts, connections could travel at rates as high as  Gbps, which largely exceeds wireline network speeds. When it comes to latency, it can go for 1 millisecond (ms) or even lower for uses that require real-time feedback. These numbers compare to 4G as a horse carriage compares to a jet plane, as the former peaked at about Mbps with a -second latency. But let’s further compare the two generations.
Beyond all this, 5G is able to offer network management features like network slicing, allowing mobile operators to create multiple virtual networks within a single physical 5G network. This feature will make possible for wireless network connections to support specific uses or business cases, as well as being sold on an as-a-service basis.
How does 5G work?
Wireless networks are made of cell sites which, in turn, are divided into sectors that distribute data through radio waves. 5G wireless signals are transmitted through various small cell stations located in infrastructures already set in place, like building roofs or light poles. This comes as a big improvement over 4G, which demands enormous high-power cell towers in order to radiate signals over longer distances, making it a burden upon installation.
The use of multiple small cells in 5G is absolutely mandatory because the millimeter wave spectrum (between  GHz and  GHz) can only travel over short distances and is especially sensitive to interference from nature’s elements and large physical obstacles, like buildings.
3G and 4G have previously used lower-frequency bands of spectrum. To offset millimeter wave challenges relating to the previously mentioned problems, like distance and interference, the wireless industry is seriously considering the use of lower-frequency spectrum for 5G networks. This way, network operators can use the spectrum they already own to build out their new networks. This is actually a compromised solution because a lower-frequency spectrum reaches greater distances but has lower speed and capacity than millimeter wave.
Where is 5G available at?￼
The first 5G buildouts are being driven by wireless networks in four major countries: the United States, Japan, South Korea and China. According to Technology Business Research Inc., by , network operators are expected to spend billions of dollars on 5G capital expenses, all of this without having a clear knowledge of how the service can make a proper return on the investment. Fortunately for operators, there have been some use cases and business models taking advantage of the technology, giving them a more precise understanding of where the future may lead them.
While all of this is happening, some were also done on creating universal 5G equipment standards. In December , the 3rd Generation Partnership Project (3GPP) approved 5G New Radio (NR) standards and, as of now, has successfully completed the 5G mobile core standard required for 5G cellular services. The 5G radio system isn’t compatible with 4G radios, but network operators may be able to upgrade to the new 5G system via software with no need for new equipment.
With 5G wireless equipment standards complete and the first 5G-compliant smartphones and associated wireless devices commercially available, 5G use cases will begin to emerge between  and , according to Technology Business Research projections. By , 5G services are expected to become mainstream and will range from the delivery of virtual reality (VR) content to autonomous vehicle navigation enabled by real-time communications (RTC) capabilities.
What kind of services can we expect from 5G?
Basically, you can expect 5G to come in two forms, wireless broadband and cellular.
Fixed wireless broadband services can deliver internet access to any home or business, exactly like your ISP provides to you right now, the difference being that you won’t need any wires connected to the facility. In order to achieve that, operators will need to deploy NRs in small cell sites in nearby infrastructures, this way the signal can be beamed to a receiver on a rooftop or a windowsill that in turn is amplified within the premises. This would significantly reduce costs for operators as they wouldn’t need to roll out fiber-optic lines to every residence. This being the case, operators only need to install fiber optics to cell sites, and customers would then receive broadband services through wireless modems located in their residences or businesses.
5G cellular services are exactly what the name says, they will provide user access to operators’ 5G cellular networks. These services are expected to roll out starting this year, when the first 5G-enabled (or -compliant) devices become available in stores. The cellular service delivery is also dependent upon the completion of mobile core standards by 3GPP.
How will 5G be used?
Autonomous Vehicles will probably see in 5G a driving force (no pun intended) for their rise in the near future. These vehicles will communicate with other vehicles on the road, warn about road conditions for others using the same route, and provide performance information to automakers in order to enhance driver experience. For example, if a car brakes in front of you, your car can learn from that immediately and prevent the accident waiting to happen. This kind of implementation can, ultimately, save lives.
Cities will be able to operate more efficiently. Several companies will be able to track the usage of their products, sensors can help public works departments when there is a risk of flood or of a blackout. This also makes way for the installation of more efficient surveillance cameras, with all the pros and cons it represents.
￼It will be possible to remotely operate heavy machinery due to the remarkable low latency of the technology. While this can significantly reduce work risks for all the humans involved, it can also allow highly qualified technicians to operate these machines from anywhere in the world.
As mentioned before, low latency offers a world of new possibilities, one of those being fundamental changes to the way healthcare works. Expect to see a lot of improvements regarding telemedicine, remote recovery and even physical therapy through augmented reality, not to mention remote precision surgery.
Needless to say that the Internet of Things will be greatly affected by 5G. While we already have sensors successfully communicating with each other, they usually require a lot of resources and are experts in depleting LTE data capacity. With 5G, the IoT will very likely be powered by communications among sensors and smart devices. These so-called Massive Machine-Type Communications, or mMTC for short, will require fewer resources, since they can connect to a single base station, making them extremely efficient.
Final Thoughts
5G has all the potential to be a game changer when it comes to global connectivity. If territorial governments are willing to provide populations with a true worldwide connection, then 5G can be as revolutionary as the advent of the internet itself, empowering previously untapped territories when it comes to video coverage and opening a floodgate of new businesses coming their way. In already developed territories it will undoubtedly improve video coverage and transmission quality, due to the low latency values, also enhancing the visibility and awareness of solutions like WeCast, tailor-made to provide tools for companies willing to improve their video content visibility.
Furthermore, a better and more trustworthy network will empower the desire for connectivity between home devices, work devices, and even smart city projects. Hence, it will promote the IoT ecosystem and spread its use throughout society. All of this, not mentioning the certain improvement over mobile broadband and general internet connections.
At the heart of the 4th industrial revolution, 5G is the fifth generation of mobile phone standards. Each generation has come with its share of evolutions and use cases:
1G thus enabled analog telecommunications when it was introduced in .
2G paved the way for text messaing as early as .
3G has made mobile and wireless internet connection available to all since .
4G has been a real innovation for the Cloud, IP telephony and gave access to a real high-speed mobile internet when it was launched in .
With a speed  times faster than 4G, latency times reduced to the narrower portion and a minimal energy consumption, 5G will be a real revolution opening the way to a whole host of new uses.
In order to work properly, 5G is based on 3 master technologies:
eMBB for enhanced Mobile Broadband. This technology will be used by end users to download very high quality movies in just a few seconds.
URLLC for Ultra Reliable & Low Latency Communications. This technology will allow the 5G to have an ultra-low latency time while remaining ultra reliable.
mMTC for massive Machine-Type Communications. This last technological base allows to have a higher density of objects in 5G.
Finally, the 5G will benefit from millimetre waves in order to use a much wider frequency spectrum than the 4G because it can go from GHz to GHz.
This technical preamble completed, I can present you the 5 ways the 5G will revolutionize your life in the future.
Self-Driving Cars
The autonomous car is an area where giants such as Tesla, Google or Uber are already engaged in a merciless struggle. Progress is promising, but to be able to operate fully, autonomous cars will need 5G.Indeed, self-driving cars will consume a lot of bandwidth while requiring very fast response times. It is obvious that an autonomous car launched at high speed cannot wait several seconds before deciding whether the object crossing the road is a danger or not. Most of the intelligence of autonomous cars is located in the cloud, which means that cars will need a continuous connection to the network to be able to function properly.
Due to its technical characteristics, the 5G will open up such possibilities for autonomous cars. It will not be too much to absorb the flow of autonomous cars that may multiply within cities in the future.
Health
The impacts of 5G on the world of health will be multiple. Initially, 5G will make it possible to streamline all communications and data transfers within hospitals. This will not be a luxury because hospitals will have to treat more and more patients in the future.
Secondly, we can imagine 5G benefiting the e-health world with many connected objects that will transmit information in real time to hospitals by allowing automatic patient monitoring. This will allow patients to be equipped with connected pacemakers or insulin pumps and react by alerting emergency services if a problem is detected.
All this will make it easier for patients to be hospitalized at home while solving the problem of lack of space that is being felt in many hospitals.
Finally, 5G can also be used to perform remote surgical operations using specialized robots. You could have surgery in a Los Angeles hospital by a great New York professor of hand surgery. To do this, reduced latency to less than 1ms of 5G will be a requirement.
Internet Of Things
Everyone has been talking about the Internet of Things for many months or even years. With the advent of 5G, the Internet of Things will be able to move up a gear. The Internet of Things combined with 5G will open up new opportunities in the world of health as I explained earlier.Better yet, it will pave the way for smart cities. The connected objects will be everywhere within a city and everything can be programmed or controlled remotely. For example, a plant can be fully remotely controlled in real time. Real-time traffic monitoring will also benefit. Examples are legion.
At the consumer level, 5G will pave the way for smart homes. Each of the objects in your house will be connected: your oven, your refrigerator, your television of course,… All this without counting the new connected gadgets that are multiplying within homes. These objects will be able to communicate together and such a density of objects connected in 5G in such a close way will not be a problem thanks to mMTC technology.
Let’s take the example of the refrigerator. He will know exactly what is in it and will propose meals to prepare for you. Even better, all this will communicate with your health sensors and the meals offered can really take into account what you need to stay healthy.
Seamless Virtual Reality / Augmented Reality Experiences
The world of virtual reality and augmented reality will also be boosted by 5G. Experiments in VR/AR are already multiplying but the amount of data required makes these experiments not always very reactive. 5G will fix this problem with the instant streaming capabilities it will offer.
A few months ago, Vodafone demonstrated a holographic call based on 5G. This kind of call allows you to create real-time interactions with people on the other side of the world. This can be applied in the family context but also in companies, for example, in meetings. Even better, it will save employees at remote sites of a company from having to travel more to attend certain meetings.
For consumers, these 5G-based VR/AR experiences will pave the way for them to visit exceptional sites around the world while staying at home. This will also have strong impacts in the world of online video games. Finally, why not playing sports with some of your friends on the other side of the world. 5G will make all this possible.
Broadband Internet Everywhere
4G has already democratized the use of high-speed Internet in mobile situations in public transport, for example. Nevertheless, the maximum throughput of 4G is a problem from time to time, as are latency times.
5G will clearly make very high speed internet really accessible everywhere at all times. You will be able to watch a HD movie in streaming during your train trips without any interruptions. Live television services will also benefit from 5G networks. Online gaming on smartphones or tablets too.
Fixed Wireless Services, such as Verizon’s, to put an end to these isolated areas where people do not have access to very high bandwidth. Indeed, Fixed Wireless Services make it possible to provide very high speed Internet access without the need to deploy fibre or cable to the end user. It will be possible by placing 5G antennas to reach these people in a less expensive and easier way. Indeed, there will no longer be the need for technicians to travel to each home to install high-speed Internet equipment.
Finally, smartphones will benefit enormously from the 5G and the new possibilities it will offer in terms of uses. Nevertheless, it is principally the manufacturers who intend to take advantage of it for the moment. Indeed, they hope that 5G and its possibilities will be sufficiently convincing arguments to push consumers to invest more than $1, to renew their smartphones.
This is clearly a huge challenge for Apple, Samsung or Huawei in the coming months.
Conclusion
5G is still far from a reality for a large majority of people around the world. Nevertheless, its deployment has begun in several countries and the uses mentioned in this article will gradually become a reality. In 5 to  years, all these uses will probably have become a reality and they will have definitely changed your life.
Why machine learning matters
Artificial intelligence will shape our future more powerfully than any other innovation this century. Anyone who does not understand it will soon find themselves feeling left behind, waking up in a world full of technology that feels more and more like magic.
The rate of acceleration is already astounding. After a couple of AI winters and periods of false hope over the past four decades, rapid advances in data storage and computer processing power have dramatically changed the game in recent years.
In , Google trained a conversational agent (AI) that could not only convincingly interact with humans as a tech support helpdesk, but also discuss morality, express opinions, and answer general facts-based questions.(Vinyals & Le, )
The same year, DeepMind developed an agent that surpassed human-level performance at  Atari games, receiving only the pixels and game score as inputs. Soon after, in , DeepMind obsoleted their own achievement by releasing a new state-of-the-art gameplay method called A3C.
Meanwhile, AlphaGo defeated one of the best human players at Go — an extraordinary achievement in a game dominated by humans for two decades after machines first conquered chess. Many masters could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient Chinese war strategy game, with its ¹⁷⁰ possible board positions (there are only ⁸⁰atoms in the universe).Professional Go player Lee Sedol reviewing his match with AlphaGo after defeat. Photo via The Atlantic.
In March , OpenAI created agents that invented their own language to cooperate and more effectively achieve their goal. Soon after, Facebook reportedly successfully training agents to negotiate and even lie.
Just a few days ago (as of this writing), on August , , OpenAI reached yet another incredible milestone by defeating the world’s top professionals in 1v1 matches of the online multiplayer game Dota 2.See the full match at The International , with Dendi (human) vs. OpenAI (bot), on YouTube.
Much of our day-to-day technology is powered by artificial intelligence. Point your camera at the menu during your next trip to Taiwan and the restaurant’s selections will magically appear in English via the Google Translate app.Google Translate overlaying English translations on a drink menu in real time using convolutional neural networks.
Today AI is used to design evidence-based treatment plans for cancer patients, instantly analyze results from medical tests to escalate to the appropriate specialist immediately, and conduct scientific research for drug discovery.A bold proclamation by London-based BenevolentAI (screenshot from About Us page, August ).
In everyday life, it’s increasingly commonplace to discover machines in roles traditionally occupied by humans. Really, don’t be surprised if a little housekeeping delivery bot shows up instead of a human next time you call the hotel desk to send up some toothpaste.In this series, we’ll explore the core machine learning concepts behind these technologies. By the end, you should be able to describe how they work at a conceptual level and be equipped with the tools to start building similar applications yourself.
The semantic tree: artificial intelligence and machine learning
One bit of advice: it is important to view knowledge as sort of a semantic tree — make sure you understand the fundamental principles, ie the trunk and big branches, before you get into the leaves/details or there is nothing for them to hang on to. — Elon Musk, Reddit AMAMachine learning is one of many subfields of artificial intelligence, concerning the ways that computers learn from experience to improve their ability to think, plan, decide, and act.
Artificial intelligence is the study of agents that perceive the world around them, form plans, and make decisions to achieve their goals. Its foundations include mathematics, logic, philosophy, probability, linguistics, neuroscience, and decision theory. Many fields fall under the umbrella of AI, such as computer vision, robotics, machine learning, and natural language processing.
Machine learning is a subfield of artificial intelligence. Its goal is to enable computers to learn on their own. A machine’s learning algorithm enables it to identify patterns in observed data, build models that explain the world, and predict things without having explicit pre-programmed rules and models.
The AI effect: what actually qualifies as “artificial intelligence”?
The exact standard for technology that qualifies as “AI” is a bit fuzzy, and interpretations change over time. The AI label tends to describe machines doing tasks traditionally in the domain of humans. Interestingly, once computers figure out how to do one of these tasks, humans have a tendency to say it wasn’t really intelligence. This is known as the AI effect.
For example, when IBM’s Deep Blue defeated world chess champion Garry Kasparov in , people complained that it was using "brute force" methods and it wasn’t “real” intelligence at all. As Pamela McCorduck wrote, “It’s part of the history of the field of artificial intelligence that every time somebody figured out how to make a computer do something — play good checkers, solve simple but relatively informal problems — there was chorus of critics to say, ‘that’s not thinking’”(McCorduck, ).
Perhaps there is a certain je ne sais quoi inherent to what people will reliably accept as “artificial intelligence”:
"AI is whatever hasn't been done yet." - Douglas Hofstadter
So does a calculator count as AI? Maybe by some interpretation. What about a self-driving car? Today, yes. In the future, perhaps not. Your cool new chatbot startup that automates a flow chart? Sure… why not.
Strong AI will change our world forever; to understand how, studying machine learning is a good place to start
The technologies discussed above are examples of artificial narrow intelligence (ANI), which can effectively perform a narrowly defined task.
Meanwhile, we’re continuing to make foundational advances towards human-level artificial general intelligence (AGI), also known as strong AI. The definition of an AGI is an artificial intelligence that can successfully perform any intellectual task that a human being can, including learning, planning and decision-making under uncertainty, communicating in natural language, making jokes, manipulating people, trading stocks, or… reprogramming itself.
And this last one is a big deal. Once we create an AI that can improve itself, it will unlock a cycle of recursive self-improvement that could lead to an intelligence explosion over some unknown time period, ranging from many decades to a single day.
Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an ‘intelligence explosion,’ and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. — I.J. Good, 
You may have heard this point referred to as the singularity. The term is borrowed from the gravitational singularity that occurs at the center of a black hole, an infinitely dense one-dimensional point where the laws of physics as we understand them start to break down.We have zero visibility into what happens beyond the event horizon of a black hole because no light can escape. Similarly, after we unlock AI’s ability to recursively improve itself, it’s impossible to predict what will happen, just as mice who intentionally designed a human might have trouble predicting what the human would do to their world. Would it keep helping them get more cheese, as they originally intended? (Image via WIRED)
A recent report by the Future of Humanity Institute surveyed a panel of AI researchers on timelines for AGI, and found that “researchers believe there is a  chance of AI outperforming humans in all tasks in  years” (Grace et al, ). We’ve personally spoken with a number of sane and reasonable AI practitioners who predict much longer timelines (the upper limit being “never”), and others whose timelines are alarmingly short — as little as a few years.Image from Kurzweil’s The Singularity Is Near, published in . Now, in , only a couple of these posters could justifiably remain on the wall.
The advent of greater-than-human-level artificial superintelligence (ASI) could be one of the best or worst things to happen to our species. It carries with it the immense challenge of specifying what AIs will want in a way that is friendly to humans.
While it’s impossible to say what the future holds, one thing is certain:  is a good time to start understanding how machines think. To go beyond the abstractions of a philosopher in an armchair and intelligently shape our roadmaps and policies with respect to AI, we must engage with the details of how machines see the world — what they “want”, their potential biases and failure modes, their temperamental quirks — just as we study psychology and neuroscience to understand how humans learn, decide, act, and feel.
There are complex, high-stakes questions about AI that will require  our careful attention in the coming years.
How can we combat AI’s propensity to further entrench systemic biases evident in existing data sets? What should we make of fundamental disagreements among the world’s most powerful technologists about the potential risks and benefits of artificial intelligence? What will happen to humans' sense of purpose in a world without work?
Machine learning is at the core of our journey towards artificial general intelligence, and in the meantime, it will change every industry and have a massive impact on our day-to-day lives. That’s why we believe it’s worth understanding machine learning, at least at a conceptual level — and we designed this series to be the best place to start.
How to read this series
You don’t necessarily need to read the series cover-to-cover to get value out of it. Here are three suggestions on how to approach it, depending on your interests and how much time you have:
T-shaped approach. Read from beginning to end. Summarize each section in your own words as you go (see: Feynman technique); this encourages active reading & stronger retention. Go deeper into areas that are most relevant to your interests or work. We’ll include resources for further exploration at the end of each section.
Focused approach. Jump straight to the sections you’re most curious about and focus your mental energy there.
/ approach. Skim everything in one go, make a few notes on interesting high-level concepts, and call it a night. 
About the authors“Ok, we have to be done with gradient descent by the time we finish this ale.” @ The Boozy Cow in Edinburgh
Vishal most recently led growth at Upstart, a lending platform that utilizes machine learning to price credit, automate the borrowing process, and acquire users. He spends his time thinking about startups, applied cognitive science, moral philosophy, and the ethics of artificial intelligence.
Samer is a Master’s student in Computer Science and Engineering at UCSD and co-founder of Conigo Labs. Prior to grad school, he founded TableScribe, a business intelligence tool for SMBs, and spent two years advising Fortune  companies at McKinsey. Samer previously studied Computer Science and Ethics, Politics, and Economics at Yale.
Most of this series was written during a -day trip to the United Kingdom in a frantic blur of trains, planes, cafes, pubs and wherever else we could find a dry place to sit. Our aim was to solidify our own understanding of artificial intelligence, machine learning, and how the methods therein fit together — and hopefully create something worth sharing in the process.
Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data.
For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It’s the same algorithm but it’s fed different training data so it comes up with different classification logic.This machine learning algorithm is a black box that can be re-used for lots of different classification problems.
“Machine learning” is an umbrella term covering lots of these kinds of generic algorithms.
Two kinds of Machine Learning Algorithms
You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. The difference is simple, but really important.
Supervised Learning
Let’s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses.
To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it’s size, neighborhood, etc, and what similar houses have sold for.
So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details — number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price:This is our “training data.”
Using that training data, we want to create a program that can estimate how much any other house in your area is worth:We want to use the training data to predict the prices of other houses.
This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic.
To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out.
This kind of like having the answer key to a math test with all the arithmetic symbols erased:Oh no! A devious student erased the arithmetic symbols from the teacher’s answer key!
From this, can you figure out what kind of math problems were on the test? You know you are supposed to “do something” with the numbers on the left to get each answer on the right.
In supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type!
Unsupervised Learning
Let’s go back to our original example with the real estate agent. What if you didn’t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning.Even if you aren’t trying to predict an unknown number (like price), you can still do interesting things with machine learning.
This is kind of like someone giving you a list of numbers on a sheet of paper and saying “I don’t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something — good luck!”
So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you’d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts.
Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions.
Supervised learning is what we’ll focus on for the rest of this post, but that’s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer.
Side note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start.
That’s cool, but does being able to estimate the price of a house really count as “learning”?
As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a “feel” for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers.
But current machine learning algorithms aren’t that good yet — they only work when focused a very specific, limited problem. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”.
Unfortunately “Machine Figuring out an equation to solve a specific problem based on some example data” isn’t really a great name. So we ended up with “Machine Learning” instead.
Of course if you are reading this  years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human.
Let’s write that program!
So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further.
If you didn’t know anything about machine learning, you’d probably try to write out some basic rules for estimating the price of a house like this.
If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change.
Wouldn’t it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number.
One way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there’s an exact ratio of ingredients to stir in to make the final price.
That would reduce your original function (with all those crazy if’s and else’s) down to something really simple like this:
def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood).
Notice the magic numbers in bold — ., ., 2., and .. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices!
A dumb way to figure out the best weights would be something like this:
Step 1:
Start with each weight set to 1.0:
def estimate_house_sales_price(num_of_bedrooms, sqft, neighborhood):
Run every house you know about through your function and see how far off the function is at guessing the correct price for each house:Use your function to predict a price for each house.
For example, if the first house really sold for $,, but your function guessed it sold for $,, you are off by $, for that single house.
Now add up the squared amount you are off for each house you have in your data set. Let’s say that you had  home sales in your data set and the square of how much your function was off for each house was a grand total of $,,. That’s how “wrong” your function currently is.
Now, take that sum total and divide it by  to get an average of how far off you are for each house. Call this average error amount the cost of your function.
If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that’s our goal — get this cost to be as low as possible by trying different weights.
Step 3:
Repeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you’ve solved the problem!
Mind Blowage Time
That’s pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow!
But here’s a few more facts that will blow your mind:
Research in many fields (like linguistics/translation) over the last  years has shown that these generic learning algorithms that “stir the number stew” (a phrase I just made up) out-perform approaches where real people try to come up with explicit rules themselves. The “dumb” approach of machine learning eventually beats human experts.
The function you ended up with is totally dumb. It doesn’t even know what “square feet” or “bedrooms” are. All it knows is that it needs to stir in some amount of those numbers to get the correct answer.
It’s very likely you’ll have no idea why a particular set of weights will work. So you’ve just written a function that you don’t really understand but that you can prove will work.
Imagine that instead of taking in parameters like “sqft” and “num_of_bedrooms”, your prediction function took in an array of numbers. Let’s say each number represented the brightness of one pixel in an image captured by camera mounted on top of your car. Now let’s say that instead of outputting a prediction called “price”, the function outputted a prediction called “degrees_to_turn_steering_wheel”. You’ve just made a function that can steer your car by itself!
Pretty crazy, right?
What about that whole “try every number” bit in Step 3?
Ok, of course you can’t just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you’d never run out of numbers to try.
To avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here’s one way:
First, write a simple equation that represents Step #2 above:This is your cost function.
Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now):θ is what represents your current weights. J(θ) means the ‘cost for your current weights’.
This equation represents how wrong our price estimating function is for the weights we currently have set.
If we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we’d get a graph that might look something like this:The graph of our cost function looks like a bowl. The vertical axis represents the cost.
In this graph, the lowest point in blue is where our cost is the lowest — thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we’ll have our answer!So we just need to adjust our weights so we are “walking down hill” on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we’ll eventually get there without having to try too many different weights.
If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function’s tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill.
So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we’ll reach the bottom of the hill and have the best possible values for our weights. (If that didn’t make sense, don’t worry and keep reading).
That’s a high level summary of one way to find the best weights for your function called batch gradient descent. Don’t be afraid to dig deeper if you are interested on learning the details.
When you use a machine learning library to solve a real problem, all of this will be done for you. But it’s still useful to have a good idea of what is happening.
What else did you conveniently skip over?
The three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you’ve never seen before based where that house would appear on your line. It’s a really powerful idea and you can solve “real” problems with it.
But while the approach I showed you might work in simple cases, it won’t work in all cases. One reason is because house prices aren’t always simple enough to follow a continuous line.
But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies.
Also, I ignored the idea of overfitting. It’s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren’t in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully.
In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it’s a skill that any developer can learn!
Is machine learning magic?
Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data!
But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have.
For example, if you build a model that predicts home prices based on the type of potted plants in each house, it’s never going to work. There just isn’t any kind of relationship between the potted plants in each house and the home’s sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two.You can only model relationships that actually exist.
So remember, if a human expert couldn’t use the data to solve the problem manually, a computer probably won’t be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly.
How to learn more about Machine Learning
In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn’t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it’s getting a little better every day.
If you want to try out what you’ve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try!
If you want to go deeper, Andrew Ng’s free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math.
Also, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It’s a python framework that has “black box” versions of all the standard algorithms.
In a grand event at its headquarters in San Diego, Qualcomm’s executives recalled how the seeds of 5G were sown decades ago and claimed the future of 5G will ride on its technologies. The speakers, including CTO Jim Thompson and the company’s R&D and product heads, time-traveled us back to the s when EV-DO first introduced the basic concepts of wireless Internet, to LTE which brought techniques such as OFDMA used in 5G today, and concepts such as direct device-to-device that will fuel the future of 5G.
In the times when we are busy with immediacy, this was a refreshing perspective, realizing where we started, how far we have come, and how much more we have to go. Personally, I came out feeling very confident about the future of wireless technology in general, and 5G evolution in particular.
EV-DO was the first real mobile wireless broadband solution. Although other technologies such as GPRS existed, EV-DO, for the first time, introduced the concept of an All-IP data channel optimized for the extreme conditions of wireless. Features such as hybrid ARQ (HARQ), turbo coding, adaptive modulation, and opportunistic scheduling introduced during those times were used for 3G, 4G, and are a basic staple for 5G and beyond. Techniques such as carrier aggregation, VoIP over wireless and many others are also still being used and will continue to be used. OFDM/A that revolutionized mobile broadband with 4G LTE is the choice of waveform and access technology for 5G and its evolutions. The technologies behind C-V2X, and Slide-link, usage of unlicensed/shared spectrum, and many more that are part of the future of 5G, all came from cellular evolution. All of these are Qualcomm’s innovations.
The technologies that built 5G
While 5G has become almost a household term, the challenges of 5G and the innovations that solved them are still not very well known. And they look like “ wizardry” as The Wall Street Journal put it. Years back when 5G was being mulled over, many experts knew about the limitations of 4G, and the extreme needs of the next-generation wireless technology. But the challenge was not only developing a platform that addressed the current needs but also making it future proof.
In my view, developing a unified, flexible design based on scalable OFDMA and slot-based framework is one of the key contributions of Qualcomm to 5G. When I heard about it from engineers for the first time, it sounded almost utopian — “everything for everyone!” But kudos to the Qualcomm experts, it was standardized and commercialized in record time (remember 5G acceleration?). The framework is so flexible and scalable that any application that can be imagined now and in the foreseeable future can be supported with it.
Another path-breaking Qualcomm 5G invention was solving the millimeter wave (mmW) puzzle. It was an impossible challenge. Trust me when I say this because I have hands-on experience dealing with mmW. Making it work for mobiles and in outdoor environments is nothing short of a miracle and is in the same league as making CDMA work. Even after six months of first 5G deployments, nobody other than Qualcomm has been able to show a working device, let alone a commercial phone! This indeed is a testament to the complex art involved in making mmW work in phones. Notwithstanding the initial hiccups of one operator that is giving mmW some bad rap, very soon, all those bashers will realize the pivotal role mmW will play in making 5G a universal solution for everything connected. During the event, through multiple demos, Qualcomm went to lengths to show that mmW indeed works in non-line of sight (LoS) scenarios.“The key to Qualcomm’s success is our systems approach,” said John Smee, Qualcomm VP of Engineering and Wireless R&D head, “We are first to develop the end-to-end prototypes and demonstrate the feasibility of any new technology.” To prove that point, they took us on an exclusive behind the scene tour, to the roof of their HQ building to show the 5G sub-6GHz end-to-end system they have built, including their own test base station and the core network.
Future of 5G
Before jumping ahead to the question “what’s next for 5G?” we’ve got realize that 5G is still in its infancy, barely a few months after the first commercial launch. I know the staggering speed of deployments and all the news might make you think it’s years old! In the words of Durga Malladi, Qualcomm SVP and GM of 5G, “it feels like the entire life of 4G has happened in the first few months of 5G.” In comparison, six months after the launch of 4G, people had barely even noticed it! What is commercial now is just the first prong of what I call a three-pointed star — enhanced mobile (and fixed) broadband (eMBB), along with massive IoT and mission-critical services (MCS). The future of 5G lies in realizing those latter two prongs, mostly connecting and transforming the score of verticals that are primed for the 5G magic. Of course, the enhancements to eMBB will continue as well.
From the perspective of the applications, you will see the continuance of the massive surge of IoT devices across many verticals. 5G will support extreme density — up to a million devices in a square kilometer. You will see the emergence of MCS that need . or better reliability and millisecond latency. These will ring in a slew of industrial IoT applications and use cases that were not possible with 4G. 5G, along with the other major technologies such as AI and edge compute will herald “Industry 4.0” — the next industrial revolution. The factories will be untethered and smart, which will allow them to be nimble, modular, and much more efficient. The consumer applications such as photorealistic VR, AR, and others will become mainstream and extend into the industrial sector as well.
You might say, you have heard them all, well, so far it was just a vision, but now the technologies are being developed to realize them. I attended the 3GPP’s Plenary and RAN Working Group meetings a few of weeks ago, where there were spirited discussions about Rel.  and Rel.. Rel  is expected to be finalized in March , and the study and work items for Rel.  are expected in December . Rel.  includes features such as eURLLC (enhanced Ultra-Reliable Low Latency Communication), 5G C-V2X, 5G-NR-Unlicensed, Integrated Access and Backhaul (IAB), positioning and others. Rel.  is evaluating XR, NR-Light for wearable, etc., spectrum above  GHz, centimeter-level positioning, expanded use of slide-link, and many more.
From the network perspective, you will see that the network will move from today’s non-stand alone (NSA) to stand-alone SA configuration, which is needed to support all the low-latency and high-reliability capabilities and applications. You will also see increasingly more spectrum allocated to 5G, through the newly allocated spectrum, through the shared spectrum as well as existing spectrum using features such as Dynamic Spectrum Selection (DSS).
5G and its evolutions are built on the solid foundational technologies that Qualcomm provided and has been improving over the last three decades. This event, showed a glimpse of the future, while also taking us through the memory lanes of yesteryears. The vision for the future of 5G is very clear, and a series of live demos we saw clearly indicate the solid progress being made to deliver on that vision.
If you’ve heard about Artificial Intelligence, Machine Learning, or Deep Learning recently, then you might have heard of a Neural Network.
Neural Networks are a key piece of some of the most successful machine learning algorithms. The development of neural networks have been key to teaching computers to think and understand the world in the way that humans do. Essentially, a neural network emulates the human brain. Brains cells, or neurons, are connected via synapses. This is abstracted as a graph of nodes (neurons) connected by weighted edges (synapses).
So let’s dive in. What is a neural network? The human brain consists of  billion cells called neurons, connected together by synapses. If sufficient synaptic inputs fire to a neuron, that neuron will also fire. We call this process “thinking”. We can model this process by creating a neural network on a computer. A neural network has input and output neurons, which are connected by weighted synapses. The weights affect how much of the forward propagation goes through the neural network. The weights can then be changed during the back propagation — this is the part where the neural network is now learning. This process of forward propagation and backward propagation is conducted iteratively on every piece of data in a training data set. The greater the size of the data set and the greater the variety of data set that there is, the more that the neural network will learn, and the better that the neural network will get at predicting outputs.This neural network has one layer, three inputs, and one output. Any neural network can have any number of layers, inputs, or outputs.
Simply put, a neural network is a connected graph with input neurons, output neurons, and weighted edges. Let’s go into detail about some of these components:
1) Neurons. A neural network is a graph of neurons. A neuron has inputs and outputs. Similarly, a neural network has inputs and outputs. The inputs and outputs of a neural network are represented by input neurons and output neurons. Input neurons have no predecessor neurons, but do have an output. Similarly, an output neuron has no successor neuron, but does have inputs.
2) Connections and Weights. A neural network consists of connections, each connection transferring the output of a neuron to the input of another neuron. Each connection is assigned a weight.
3) Propagation Function. The propagation function computes the input of a neuron from the outputs of predecessor neurons. The propagation function is leveraged during the forward propagation stage of training.
4) Learning Rule. The learning rule is a function that modifies the weights of the connections. This serves to produce a favored output for a given input for the neural network. The learning rule is leveraged during the backward propagation stage of training.
Deep Neural Networks
So now that we know what a Neural Network is. What is a Deep Neural Network? A Deep Neural Network simply has more layers than smaller Neural Networks. A smaller Neural Network might have 1–3 layers of neurons. However, a Deep Neural Network (DNN) has more than a few layers of neurons. A DNN might have  or 1, layers of neurons.This neural network has two layers, three inputs, and one output. Any neural network can have any number of layers, inputs, or outputs. The layers between the input neurons and the final layer of output neurons are hidden layers of a deep neural network.
Conclusion
That’s basically it for a Neural Network. A neural network is just a core architecture. There are different types of neural networks. For example, Convolutional Neural Networks have been very effective for Computer Vision applications. Recurrent Neural Networks are also very popular. If anything here was unclear or you need a broader overview, then feel free to read this Overview of Artificial Intelligence, Machine Learning, and Deep Learning.
What is Deep Learning?
By Kate Reyes
Last updated on Aug , 
What is Deep Learning?
Artificial Intelligence and machine learning are the cornerstones of the next revolution in computing. These technologies hinge on the ability to recognize patterns then, based on data observed in the past, predict future outcomes. This explains the suggestions Amazon offers as you shop online or how Netflix knows your penchant for bad s movies. Although machines utilizing AI principles are often referred to as “smart,” most of these systems don’t learn on their own; the intervention of human programming is necessary. Data scientists prepare the inputs, selecting the variables to be used for predictive analytics. Deep learning, on the other hand, can do this job automatically.Deep learning can be considered as a subset of machine learning. It is a field which is based on learning and improving on its own by examining computer algorithms. While machine learning uses simpler concepts, deep learning works with artificial neural networks which are designed to imitate how humans think and learn. Until recently, neural networks were limited by computing power and thus were limited in complexity. However, advancements in big data analytics have permitted larger, sophisticated neural networks, allowing computers to observe, learn and react to complex situations faster than humans. Deep learning has aided image classification, language translation, speech recognition. It can be used to solve any pattern recognition problem and without human intervention.Artificial neural networks, comprising many layers, drive deep learning. Deep Neural Networks (DNNs) are such type of networks where each layer can perform complex operations such as representation and abstraction that make sense of images, sound, and text. Considered the fastest-growing field in machine learning, deep learning represents a truly disruptive digital technology, and it is being used by increasingly more companies to create new business models.Master deep learning concepts and the TensorFlow open source framework with the Deep Learning Training Course. Get skilled today!
How Deep Learning Does What it Does
Neural networks are comprised of layers of nodes, much like the human brain is made up of neurons. Nodes within individual layers are connected to adjacent layers. The network is said to be deeper based on the number of layers it has. A single neuron in the human brain receives thousands of signals from other neurons. In an artificial neural network, signals travel between nodes and assign corresponding weights. A heavier weighted node will exert more effect on the next layer of nodes. The final layer compiles the weighted inputs to produce an output. Deep learning systems require powerful hardware because they have a large amount of data being processed and involves a number of complex mathematical calculations. Even with such advanced hardware, however, deep learning training computations can take weeks.Deep learning systems require large amounts of data to return accurate results; accordingly, information is fed as huge data sets. When processing the data, artificial neural networks are able to classify data with the answers received from a series of binary true or false questions involving highly complex mathematical calculations. For example, a facial recognition program works by learning to detect and recognize edges and lines of faces, then larger parts of the faces and, finally, the overall representations of faces. Over time, the program trains itself and the probability of correct answers increases. In this case, the facial recognition program will accurately identify faces with time.Let’s say the goal is to have a neural network recognize photos that contain a dog. Clearly, all dogs don’t look exactly alike – consider a Rottweiler and a Poodle, for instance. Furthermore, photos show dogs at different angles and with varying amounts of light and shadow. So, a training set of images must be compiled, including many examples of dog faces which any person would label as “dog”, and pictures of objects that aren’t dogs, labeled (as one might expect) “not dog.” The images, fed into the neural network, are converted into data. These data moves through the network and various nodes assign weights to different elements. The final output layer compiles the seemingly disconnected information – furry, has a snout, has four legs, etc. – and delivers the output: dog.Now, this answer received from the neural network will be compared to the human-generated label. If there is a match, then the output is confirmed. If not, the neural network notes the error and adjusts the weightings. The neural network tries to improve its dog-recognition skills by repeatedly adjusting its weights over and over again. This training technique is called supervised learning, which occurs even when the neural networks are not explicitly told what "makes" a dog. They must recognize patterns in data over time and learn on their own.How Did Deep Learning Come About?
Machine learning is said to have occurred in the s when Alan Turing, a British mathematician proposed his artificially intelligent “learning machine.” The first computer learning program was written by Arthur Samuel. His program made an IBM computer improve at the game of checkers the longer it played. In the decades that followed, various machine learning techniques came in and out of vogue.Neural networks were mostly ignored by machine learning researchers, as they were plagued by the ‘local minima’ problem in which weightings incorrectly appeared to give the fewest errors. However, some machine learning techniques like computer vision and facial recognition moved forward. In , a machine learning algorithm called Adaboost was developed to detect faces within an image in real time. It filtered images through decision sets such as “does the image have a bright spot between dark patches, possibly denoting the bridge of a nose?” When the data moved further down the decision tree, the probability of selecting the right face from an image grew.Neural networks did not return to favor for several more years, when powerful graphics processing units finally entered the market.The new hardware enabled researchers to use desktop computers instead of supercomputers to run, manipulate and process images.. The greatest leap forward for neural networks happened because of the introduction of great amounts of labelled data with ImageNet, a database of millions of labelled images from the Internet. The cumbersome task of manually labelling images was replaced by crowdsourcing, giving networks a virtually unlimited source of training materials. In the years since, technology companies have made their deep learning libraries open source. Examples include: Google Tensorflow, Facebook open source modules for Torch, Amazon DSSTNE on GitHub and Microsoft CNTK.Deep Learning in Action
Aside from your favorite music streaming service suggesting tunes you might enjoy, how is deep learning impacting people lives? As it turns out, deep learning is finding its way into applications of all sizes. Anyone using Facebook cannot help but notice that the social platform commonly identifies and tags your friends when you upload new photos. Digital assistants like Siri, Cortana, Alexa and Google Now use deep learning for natural language processing and speech recognition. Skype translates spoken conversations in real time. Many email platforms have become adept at identifying spam messages before they even reach the inbox. PayPal has implemented deep learning to prevent fraudulent payments. Apps like CamFind allow users to simply take a picture of any object and, using mobile visual search technology, discover what the object is.Google, in particular, is leveraging deep learning to deliver solutions. Google Deepmind’s AlphaGo computer program recently defeated standing champions at the game of Go. DeepMind’s WaveNet can generate speech mimicking human voice that sounds more natural than speech systems presently on the market. Google Translate is using deep learning and image recognition to translate voice and written languages. Google PlaNet can identify where any photo was taken. Google developed the deep learning software database, Tensorflow, to help produce A.I. applications.Deep learning is only in its infancy and, in the decades to come, will transform society. Self-driving cars are being tested worldwide; the complex layer of neural networks are being trained to determine objects to avoid, recognize traffic lights and know when to adjust speed. Neural networks are becoming adept at forecasting everything from stock prices to the weather. Consider the value of digital assistants who can recommend when to sell shares or when to evacuate ahead of a hurricane. Deep learning applications will even save lives as they develop the ability to design evidence-based treatment plans for medical patients and help detect cancers early.
Most modern deep learning models are based on artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. (Of course, this does not completely eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)[1]The word "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[2] No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[citation needed] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.Deep learning architectures are often constructed with a greedy layer-by-layer method.[clarification needed][further explanation needed][citation needed] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[1]For supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.[1]Interpretations
Deep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.[1][2]The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In , the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in  by Kurt Hornik.The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.The probabilistic interpretation derives from the field of machine learning. It features inference,[1][2] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.History
The term Deep Learning was introduced to the machine learning community by Rina Dechter in , and to artificial neural networks by Igor Aizenberg and colleagues in , in the context of Boolean threshold neurons.The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in . A  paper described a deep network with 8 layers trained by the group method of data handling algorithm.Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in . In , Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since , to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.By  such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng et al. suggested that a human brain does not use a monolithic 3-D object model and in  they published Cresceptron, a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.In , André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In , Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton. Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in  by Sepp Hochreiter.Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the s and s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck achieved the first significant success with deep neural networks in speech processing in the  National Institute of Standards and Technology Speaker Recognition evaluation. While SRI experienced success with deep neural networks in speaker recognition, they were unsuccessful in demonstrating similar success in speech recognition. The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in . LSTM RNNs avoid the vanishing gradient problem and can learn "Very Deep Learning" tasks[2] that require memories of events that happened thousands of discrete time steps before, which is important for speech. In , LSTM started to become competitive with traditional speech recognizers on certain tasks. Later it was combined with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In , Google's speech recognition reportedly experienced a dramatic performance jump of  through CTC-trained LSTM, which they made available through Google Voice Search.In , publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh  showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks (CNNs) were superseded for ASR by CTC for LSTM. but are more successful in computer vision.The impact of deep learning in industry began in the early s, when CNNs already processed an estimated  to  of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around .The  NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around -, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition, eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5 in error rate) between discriminative DNNs and generative models.In , researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Advances in hardware have enabled renewed interest in deep learning. In , Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).” That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about  times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.Deep learning revolutionHow deep learning is a subset of machine learning and how machine learning is a subset of artificial intelligence (AI).
In , a team led by George E. Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the biomolecular target of one drug. In , Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the "Tox Data Challenge" of NIH, FDA and NCATS.Significant additional impacts in image or object recognition were felt from  to . Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision.[2] In , this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in , it won the ICDAR Chinese handwriting contest, and in May , it won the ISBI image segmentation contest. Until , CNNs did not play a major role at computer vision conferences, but in June , a paper by Ciresan et al. at the leading conference CVPR[4] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October , a similar system by Krizhevsky et al.[5] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November , Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In  and , the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.Some researchers assess that the October  ImageNet victory anchored the start of a "deep learning revolution" that has transformed the AI industry.In March , Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.Neural networks
Artificial neural networks
Main article: Artificial neural network
Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.As of , neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing "Go" ).Deep neural networksThis section may be too technical for most readers to understand. Please help improve it to make it understandable to non-experts, without removing the technical details. (July ) (Learn how and when to remove this template message)
A deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers.[2] The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name "deep" networks.DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.Recurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.Convolutional deep neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).Challenges
As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay ({\displaystyle \ell _{2}}\ell _{2}-regularization) or sparsity ({\displaystyle \ell _{1}}\ell _{1}-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.Applications
Automatic speech recognition
Main article: Speech recognition
Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn "Very Deep Learning" tasks[2] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about  ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains  speakers from eight major dialects of American English, where each speaker reads  sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 
The concept of a network of smart devices was discussed as early as , with a modified Coke vending machine at Carnegie Mellon University becoming the first Internet-connected appliance,[6] able to report its inventory and whether newly loaded drinks were cold or not.[7] Mark Weiser's  paper on ubiquitous computing, "The Computer of the st Century", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IoT.[8][9] In , Reza Raji described the concept in IEEE Spectrum as "[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories". Between  and , several companies proposed solutions like Microsoft's at Work or Novell's NEST. The field gained momentum when Bill Joy envisioned device-to-device communication as a part of his "Six Webs" framework, presented at the World Economic Forum at Davos in .The term "Internet of things" was likely coined by Kevin Ashton of Procter & Gamble, later MIT's Auto-ID Center, in , though he prefers the phrase "Internet for things". At that point, he viewed radio-frequency identification (RFID) as essential to the Internet of things, which would allow computers to manage all individual things.Defining the Internet of things as "simply the point in time when more 'things or objects' were connected to the Internet than people", Cisco Systems estimated that the IoT was "born" between  and , with the things/people ratio growing from 0. in  to 1. in .ApplicationsA Nest learning thermostat reporting on energy usage and local weather.A Ring doorbell connected to the InternetAn August Home smart lock connected to the Internet
The extensive set of applications for IoT devices is often divided into consumer, commercial, industrial, and infrastructure spaces.Consumer applications
A growing portion of IoT devices are created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities.Smart home
IoT devices are a part of the larger concept of home automation, which can include lighting, heating and air conditioning, media and security systems. Long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off.A smart home or automated home could be based on a platform or hubs that control smart devices and appliances. For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch. This could be a dedicated app or iOS native applications such as Siri. This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge. There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products and these include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub. In addition to the commercial systems, there are many non-proprietary, open source ecosystems; including Home Assistant, OpenHAB and Domoticz. Elder care
One key application of a smart home is to provide assistance for those with disabilities and elderly individuals. These home systems use assistive technology to accommodate an owner's specific disabilities. Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by hearing-impaired users. They can also be equipped with additional safety features. These features can include sensors that monitor for medical emergencies such as falls or seizures. Smart home technology applied in this way can provide users with more freedom and a higher quality of life.The term "Enterprise IoT" refers to devices used in business and corporate settings. By , it is estimated that the EIoT will account for 9.1 billion devices.Commercial application
Medical and healthcare
The Internet of Medical Things (also called the internet of health things) is an application of the IoT for medical and health related purposes, data collection and analysis for research, and monitoring. This 'Smart Healthcare', as it is also called, led to the creation of a digitised healthcare system, connecting available medical resources and healthcare services.IoT devices can be used to enable remote health monitoring and emergency notification systems. These health monitoring devices can range from blood pressure and heart rate monitors to advanced devices capable of monitoring specialised implants, such as pacemakers, Fitbit electronic wristbands, or advanced hearing aids. Some hospitals have begun implementing "smart beds" that can detect when they are occupied and when a patient is attempting to get up. It can also adjust itself to ensure appropriate pressure and support is applied to the patient without the manual interaction of nurses. A  Goldman Sachs report indicated that healthcare IoT devices "can save the United States more than $ billion in annual healthcare expenditures by increasing revenue and decreasing cost." Moreover, the use of mobile devices to support medical follow-up led to the creation of 'm-health', used "to analyse, capture, transmit and store health statistics from multiple resources, including sensors and other biomedical acquisition systems".Specialized sensors can also be equipped within living spaces to monitor the health and general well-being of senior citizens, while also ensuring that proper treatment is being administered and assisting people regain lost mobility via therapy as well. These sensors create a network of intelligent sensors that are able to collect, process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems. Other consumer devices to encourage healthy living, such as connected scales or wearable heart monitors, are also a possibility with the IoT. End-to-end health monitoring IoT platforms are also available for antenatal and chronic patients, helping one manage health vitals and recurring medication requirements.[citation needed]Advances in plastic and fabric electronics fabrication methods have enabled ultra-low cost, use-and-throw IoMT sensors. These sensors, along with the required RFID electronics, can be fabricated on paper or e-textiles for wirelessly powered disposable sensing devices. Applications have been established for point-of-care medical diagnostics, where portability and low system-complexity is essential.As of  IoMT was not only being applied in the clinical laboratory industry, but also in the healthcare and health insurance industries. IoMT in the healthcare industry is now permitting doctors, patients, and others involved (i.e. guardians of patients, nurses, families, etc.) to be part of a system, where patient records are saved in a database, allowing doctors and the rest of the medical staff to have access to the patient's information. Moreover, IoT-based systems are patient-centered, which involves being flexible to the patient's medical conditions. IoMT in the insurance industry provides access to better and new types of dynamic information. This includes sensor-based solutions such as biosensors, wearables, connected health devices, and mobile apps to track customer behaviour. This can lead to more accurate underwriting and new pricing models.The application of the IOT in healthcare plays a fundamental role in managing chronic diseases and in disease prevention and control. Remote monitoring is made possible through the connection of powerful wireless solutions. The connectivity enables health practitioners to capture patient's data and applying complex algorithms in health data analysis.TransportationDigital variable speed-limit sign.
The IoT can assist in the integration of communications, control, and information processing across various transportation systems. Application of the IoT extends to all aspects of transportation systems (i.e. the vehicle, the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication, smart traffic control, smart parking, electronic toll collection systems, logistics and fleet management, vehicle control, safety, and road assistance. In Logistics and Fleet Management, for example, an IoT platform can continuously monitor the location and conditions of cargo and assets via wireless sensors and send specific alerts when management exceptions occur (delays, damages, thefts, etc.). This can only be possible with the IoT and its seamless connectivity among devices. Sensors such as GPS, Humidity, and Temperature send data to the IoT platform and then the data is analyzed and then sent to the users. This way, users can track the real-time status of vehicles and can make appropriate decisions. If combined with Machine Learning, then it also helps in reducing traffic accidents by introducing drowsiness alerts to drivers and providing self-driven cars too.V2X communications
Main article: V2X
In vehicular communication systems, vehicle-to-everything communication (V2X), consists of three main components: vehicle to vehicle communication (V2V), vehicle to infrastructure communication (V2I) and vehicle to pedestrian communications (V2P). V2X is the first step to autonomous driving and connected road infrastructure.[citation needed]Building and home automation
IoT devices can be used to monitor and control the mechanical, electrical and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential) in home automation and building automation systems. In this context, three main areas are being covered in literature:The integration of the Internet with building energy management systems in order to create energy efficient and IOT-driven "smart buildings".
The possible means of real-time monitoring for reducing energy consumption and monitoring occupant behaviors.
The integration of smart devices in the built environment and how they might to know how to be used in future applications.
Industrial applications
Main article: Industrial Internet of Things
Also known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, (OT) operational technology, locations and people. Combined with operational technology (OT) monitoring devices, IIOT helps regulate and monitor industrial systems.Manufacturing
The IoT can realize the seamless integration of various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities. Based on such a highly integrated smart cyber-physical space, it opens the door to create whole new business and market opportunities for manufacturing. Network control and management of manufacturing equipment, asset and situation management, or manufacturing process control bring the IoT within the realm of industrial applications and smart manufacturing as well. The IoT intelligent systems enable rapid manufacturing of new products, dynamic response to product demands, and real-time optimization of manufacturing production and supply chain networks, by networking machinery, sensors and control systems together.Digital control systems to automate process controls, operator tools and service information systems to optimize plant safety and security are within the purview of the IoT. But it also extends itself to asset management via predictive maintenance, statistical evaluation, and measurements to maximize reliability. Industrial management systems can also be integrated with smart grids, enabling real-time energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by a large number of networked sensors.Industrial IoT (IIoT) in manufacturing could generate so much business value that it will eventually lead to the Fourth Industrial Revolution, also referred to as Industry 4.0. The potential for growth from implementing IIoT may generate $ trillion of global GDP by .
Design architecture of cyber-physical systems-enabled manufacturing system
Industrial big data analytics will play a vital role in manufacturing asset predictive maintenance, although that is not the only capability of industrial big data. Cyber-physical systems (CPS) is the core technology of industrial big data and it will be an interface between human and the cyber world. Cyber-physical systems can be designed by following the 5C (connection, conversion, cyber, cognition, configuration) architecture, and it will transform the collected data into actionable information, and eventually interfere with the physical assets to optimize processes.An IoT-enabled intelligent system of such cases was proposed in  and later demonstrated in  by the National Science Foundation Industry/University Collaborative Research Center for Intelligent Maintenance Systems (IMS) at the University of Cincinnati on a bandsaw machine in IMTS  in Chicago. Bandsaw machines are not necessarily expensive, but the bandsaw belt expenses are enormous since they degrade much faster. However, without sensing and intelligent analytics, it can be only determined by experience when the band saw belt will actually break. The developed prognostics system will be able to recognize and monitor the degradation of band saw belts even if the condition is changing, advising users when is the best time to replace the belt. This will significantly improve user experience and operator safety and ultimately save on costs.Agriculture
There are numerous IoT applications in farming such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, take informed decisions to improve quality and quantity, minimize risk and waste, and reduce effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar, and even apply IoT-acquired data to precision fertilization programs.In August , Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide. The specific computer programs used in the process fall under the Azure Machine Learning and the Azure IoT Hub platforms.Infrastructure applications
Monitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind-farms is a key application of the IoT. The IoT infrastructure can be used for monitoring any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and save money with Real-Time Data Analytics. It can also be used for scheduling repair and maintenance activities in an efficient manner, by coordinating tasks between different service providers and users of these facilities. IoT devices can also be used to control critical infrastructure like bridges to provide access to ships. Usage of IoT devices for monitoring and operating infrastructure is likely to improve incident management and emergency response coordination, and quality of service, up-times and reduce costs of operation in all infrastructure related areas. Even areas such as waste management can benefit from automation and optimization that could be brought in by the IoT.Metropolitan scale deployments
There are several planned or ongoing large-scale deployments of the IoT, to enable better management of cities and systems. For example, Songdo, South Korea, the first of its kind fully equipped and wired smart city, is gradually being built, with approximately  percent of the business district completed as of June . Much of the city is planned to be wired and automated, with little or no human intervention.Another application is a currently undergoing project in Santander, Spain. For this deployment, two approaches have been adopted. This city of , inhabitants has already seen , downloads of its city smartphone app. The app is connected to , sensors that enable services like parking search, environmental monitoring, digital city agenda, and more. City context information is used in this deployment so as to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification.Other examples of large-scale deployments underway include the Sino-Singapore Guangzhou Knowledge City; work on improving air and water quality, reducing noise pollution, and increasing transportation efficiency in San Jose, California; and smart traffic management in western Singapore. Using its RPMA (Random Phase Multiple Access) technology, San Diego-based Ingenu has built a nationwide public network  for low-bandwidth data transmissions using the same unlicensed 2.4 gigahertz spectrum as Wi-Fi. Ingenu's “Machine Network” covers more than a third of the US population across  major cities including San Diego and Dallas. French company, Sigfox, commenced building an Ultra Narrowband wireless data network in the San Francisco Bay Area in , the first business to achieve such a deployment in the U.S. It subsequently announced it would set up a total of  base stations to cover a total of  cities in the U.S. by the end of , making it the largest IoT network coverage provider in the country thus far. Cisco also participates in smart cities projects. Cisco has started deploying technologies for Smart Wi-Fi, Smart Safety & Security, Smart Lighting, Smart Parking, Smart Transports, Smart Bus Stops, Smart Kiosks, Remote Expert for Government Services (REGS) and Smart Education in the five km area in the city of Vijaywada.Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live . The network was designed and engineered by Fluidmesh Networks, a Chicago-based company developing wireless networks for critical applications. The NYWW network is currently providing coverage on the Hudson River, East River, and Upper New York Bay. With the wireless network in place, NY Waterway is able to take control of its fleet and passengers in a way that was not previously possible. New applications can include security, energy and fleet management, digital signage, public Wi-Fi, paperless ticketing and others.Energy management
Significant numbers of energy-consuming devices (e.g. switches, power outlets, bulbs, televisions, etc.) already integrate Internet connectivity, which can allow them to communicate with utilities to balance power generation and energy usage and optimize energy consumption as a whole. These devices allow for remote control by users, or central management via a cloud-based interface, and enable functions like scheduling (e.g., remotely powering on or off heating systems, controlling ovens, changing lighting conditions etc.). The smart grid is a utility-side IoT application; systems gather and act on energy and power-related information to improve the efficiency of the production and distribution of electricity. Using advanced metering infrastructure (AMI) Internet-connected devices, electric utilities not only collect data from end-users, but also manage distribution automation devices like transformers.Environmental monitoring
Environmental monitoring applications of the IoT typically use sensors to assist in environmental protection by monitoring air or water quality, atmospheric or soil conditions, and can even include areas like monitoring the movements of wildlife and their habitats. Development of resource-constrained devices connected to the Internet also means that other applications like earthquake or tsunami early-warning systems can also be used by emergency services to provide more effective aid. IoT devices in this application typically span a large geographic area and can also be mobile. It has been argued that the standardization IoT brings to wireless sensing will revolutionize this area.Living LabAnother example of integrating the IoT is Living Lab which integrates and combines research and innovation process, establishing within a public-private-people-partnership. There are currently  Living Labs that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. For companies to implement and develop IoT services for smart cities, they need to have incentives. The governments play key roles in smart cities projects as changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the government provides tax incentives and cheap rent, improves public transports, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share common infrastructure and labor markets, and take advantages of locally embedded technologies, production process, and transaction costs. The relationship between the technology developers and governments who manage city's assets, is key to provide open access of resources to users in an efficient way.Trends and characteristicsTechnology roadmap: Internet of things.
The IoT's major significant trend in recent years is the explosive growth of devices connected and controlled by the Internet. The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.The IoT creates opportunities for more direct integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions.The number of IoT devices increased  year-over-year to 8.4 billion in the year  and it is estimated that there will be  billion devices by . The global market value of IoT is projected to reach $7.1 trillion by .Intelligence
Ambient intelligence and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as Intel) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT. A promising approach in this context is deep reinforcement learning where most of IoT systems provide a dynamic and interactive environment. Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as supervised learning. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn HVAC on or off) and learn through the maximizing accumulated rewards it receives in long term.IoT intelligence can be offered at three levels: IoT devices, Edge/Fog nodes, and Cloud computing. The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time obstacle detection to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder.In the future, the Internet of Things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend, clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments.ArchitectureThis section needs attention from an expert in Technology. The specific problem is: The information is partially outdated, unclear, and uncited. Requires more details, but not so technical that others won't understand it.. WikiProject Technology may be able to help recruit an expert. (July )
IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud. Devices include networked things, such as the sensors and actuators found in IIoT equipment, particularly those that use protocols such as Modbus, Zigbee, or proprietary protocols, to connect to an Edge Gateway. The Edge Gateway consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing. The final tier includes the cloud application built for IIoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various database systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, Postgres). The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers. Some experts classified the three-tiers in the IIoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively.Building on the Internet of things, the web of things is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices.[citation needed]Network architecture
The Internet of things requires huge scalability in the network space to handle the surge of devices. IETF 6LoWPAN would be used to connect devices to IP networks. With billions of devices being added to the Internet space, IPv6 will play a major role in handling the network layer scalability. IETF's Constrained Application Protocol, ZeroMQ, and MQTT would provide lightweight data transport.Fog computing is a viable alternative to prevent such large burst of data flow through Internet. The edge devices' computation power to analyse and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the internet to a server with sufficient processing power.Complexity
In semi-open or closed loops (i.e. value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a complex system due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality). As a practical approach, not all elements in the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability. For example, domestic robotics (domotics) running inside a smart home might only share data within and be available via a local network. Managing and controlling a high dynamic ad hoc IoT things/devices network is a tough task with the traditional networks architecture, Software Defined Networking (SDN) provides the agile dynamic solution that can cope with the special requirements of the diversity of innovative IoT applications.Size considerations
The Internet of things would encode  to  trillion objects, and be able to follow the movement of those objects. Human beings in surveyed urban environments are each surrounded by  to  trackable objects. In  there were already  million smart devices in people's homes. This number is expected to grow to  million devices by .The figure of online capable devices grew  from  to 8.4 billion in .Space considerations
In the Internet of things, the precise geographic location of a thing—and also the precise geographic dimensions of a thing—will be critical. Therefore, facts about a thing, such as its location in time and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things in the Internet of things will be sensors, and sensor location is usually important.) The GeoWeb and Digital Earth are promising applications that become possible when things can become organized and connected by location. However, the challenges that remain include the constraints of variable spatial scales, the need to handle massive amounts of data, and an indexing for fast search and neighbor operations. In the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role in the Internet and the Web, geospatial standards will play a key role in the Internet of things.A solution to "basket of remotes"
Many IoT devices have a potential to take a piece of this market. Jean-Louis Gassée (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note, where he predicts that the most likely problem will be what he calls the "basket of remotes" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another. For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, "where collected data is used to predict and trigger actions on the specific devices" while making them work together.Enabling technologies for IoT
There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:Addressability
The original idea of the Auto-ID Center is based on RFID-tags and distinct identification through the Electronic Product Code. This has evolved into objects having an IP address or URI. An alternative view, from the world of the Semantic Web focuses instead on making all things (not just those electronic, smart, or RFID-enabled) addressable by the existing naming protocols, such as URI. The objects themselves do not converse, but they may now be referred to by other agents, such as powerful centralized servers acting for their human owners. Integration with the Internet implies that devices will use an IP address as a distinct identifier. Due to the limited address space of IPv4 (which allows for 4.3 billion different addresses), objects in the IoT will have to use the next generation of the Internet protocol (IPv6) to scale to the extremely large address space required. Internet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6, as it reduces the configuration overhead on the hosts, and the IETF 6LoWPAN header compression. To a large extent, the future of the Internet of things will not be possible without the support of IPv6; and consequently, the global adoption of IPv6 in the coming years will be critical for the successful development of the IoT in the future.Short-range wireless
The increase of IoT devices at the edge of the network is producing a massive amount of data to be computed to data centers, pushing network bandwidth requirements to the limit. Despite the improvements of network technology, data centers cannot guarantee acceptable transfer rates and response times, which could be a critical requirement for many applications.[2] Furthermore devices at the edge constantly consume data coming from the cloud, forcing companies to build content delivery networks to decentralize data and service provisioning, leveraging physical proximity to the end user. In a similar way, the aim of Edge Computing is to move the computation away from data centers towards the edge of the network, exploiting smart objects, mobile phones or network gateways to perform tasks and provide services on behalf of the cloud. By moving services to the edge, it is possible to provide content caching, service delivery, storage and IoT management resulting in better response times and transfer rates. At the same time, distributing the logic in different network nodes introduces new issues and challenges.Privacy and security
The distributed nature of this paradigm introduces a shift in security schemes used in cloud computing. Not only data should be encrypted, but different encryption mechanism should be adopted, since data may transit between different distributed nodes connected through the internet before eventually reaching the cloud. Edge nodes may also be resource constrained devices, limiting the choice in terms of security methods. Moreover a shift from centralized top-down infrastructure to a decentralized trust model is required.[3] On the other hand by keeping data at the edge it is possible to shift ownership of collected data from service providers to end-users.Scalability
Scalability in a distributed network must face different issues. First, it must take into account the heterogeneity of the devices, having different performance and energy constraints, the highly dynamic condition and the reliability of the connections, compared to more robust infrastructure of cloud data centers. Moreover, security requirements may introduce further latency in the communication between nodes, which may slow down the scaling process.[2]Reliability
Management of failovers is crucial in order to maintain a service alive. If a single node goes down and is unreachable, users should still be able to access a service without interruptions. Moreover, edge computing systems must provide actions to recover from a failure and alerting the user about the incident. To this aim, each device must maintain the network topology of the entire distributed system, so that detection of errors and recovery become easily applicable. Other factors that may influence this aspect are the connection technology in use, which may provide different levels of reliability, and the accuracy of the data produced at the edge that could be unreliable due to particular environment conditions. [2]Applications
Edge application services reduce the volumes of data that must be moved, the consequent traffic, and the distance that data must travel. That provides lower latency and reduces transmission costs. Computation offloading for real-time applications, such as facial recognition algorithms, showed considerable improvements in response times as demonstrated in early research.[4] Further research showed that using resource rich machines near mobile users, called cloudlets, offering services typically found in the cloud, provided improvements in execution time when some of the tasks are offloaded to the edge node.[5]. On the other hand, offloading every task may result in a slowdown due to transfer times between device and nodes, so depending on the workload an optimal configuration can be defined.
Another vision of the architecture is to give a re-birth for cloud gaming where the game simulations are run in the cloud and the rendered video is transferred to lightweight clients such as mobile, VR glasses, etc. Such type of streaming is also known as pixel streaming.[6] Conventional cloud games may suffer from high latency and insufficient bandwidth, since the amount of data transferred is huge due to the high resolutions required by some services.[7] As real-time games such as FPS (First person Shooting) games have strict constraints on latency, processing game simulation at the edge node is necessary for the immersive game plays. Edge nodes when used for game streaming, are known as Gamelets,[6] which are usually one or two hops away from the client.[8]
Other notable applications include connected, autonomous cars.[9], smart cities, smart notifications and home automation systems.
XR is an emerging umbrella term for all the immersive technologies. The ones we already have today—augmented reality (AR), virtual reality (VR), and mixed reality (MR) plus those that are still to be created. All immersive technologies extend the reality we experience by either blending the virtual and “real” worlds or by creating a fully immersive experience. Recent research revealed that more than  of respondents believed XR will be mainstream in the next five years. To get a better picture of XR, let’s review each of the existing technologies that exist today. Augmented reality (AR)In augmented reality, virtual information and objects are overlaid on the real world. This experience enhances the real world with digital details such as images, text, and animation. You can access the experience through AR glasses or via screens, tablets, and smartphones. This means users are not isolated from the real world and can still interact and see what’s going on in front of them. The most well-known examples of AR are the Pokémon GO game that overlays digital creatures onto the real world or Snapchat filters that put digital objects such as hats or glasses onto your head.Virtual reality (VR)In contrast to augmented reality, in a virtual reality experience, users are fully immersed in a simulated digital environment. Individuals must put on a VR headset or head-mounted display to get a  -degree view of an artificial world that fools their brain into believing they are, e.g., walking on the moon, swimming under the ocean or stepped into whatever new world the VR developers created. The gaming and entertainment industry were early adopters of this technology; however, companies in several industries such as healthcare, construction, engineering, the military, and more are finding VR to be very useful.Today In: Innovation
Mixed reality (MR)In mixed reality, digital and real-world objects co-exist and can interact with one another in real-time. This is the latest immersive technology and is sometimes referred to as hybrid reality. It requires an MR headset and a lot more processing power than VR or AR. Microsoft's HoloLens is a great example that, e.g., allows you to place digital objects into the room you are standing in and give you the ability to spin it around or interact with the digital object in any way possible. Companies are exploring ways they can put mixed reality to work to solve problems, support initiatives, and make their businesses better.Extended Reality Applications for BusinessThere are many practical applications of XR. Here are a few:·    Retail: XR gives customers the ability to try before they buy. Watch manufacturer Rolex has an AR app that allows you to try on watches on your actual wrist, and furniture company IKEA gives customers the ability to place furniture items into their home via their smartphone.·    Training: Especially in life-and-death circumstances, XR can provide training tools that are hyper-realistic that will help soldiers, healthcare professionals, pilots/astronauts, chemists, and more figure out solutions to problems or learn how to respond to dangerous circumstances without putting their lives or anyone else's at risk.·    Remote work: Workers can connect to the home office or with professionals located around the world in a way that makes both sides feel like they are in the same room.·    Marketing: The possibilities to engage with prospective customers and consumers through XR will have marketing professionals pondering all the potential of using XR to their company’s advantage.·    Real estate: Finding buyers or tenants might be easier if individuals can “walk through” spaces to decide if they want it even when they are in some other location.·    Entertainment: As an early adopter, the entertainment industry will continue to find new ways of utilizing immersive technologies.Challenges of XRThose developing XR technologies are battling with some of the challenges to mainstream adoption. First, XR technologies collect and process huge amounts of very detailed and personal data about what you do, what you look at, and even your emotions at any given time, which has to be protected.In addition, the cost of implementing the technology needs to come down; otherwise, many companies will be unable to invest in it. It is essential that the wearable devices that allow a full XR experience are fashionable and comfortable as well as always connected, intelligent, and immersive. There are significant technical and hardware issues to solve that include but are not limited to the display, power and thermal, motion tracking, connectivity and common illumination—where virtual objects in a real world are indistinguishable from real objects especially as lighting shifts.As each day passes, we are one step closer to solving these issues so that we will see many more mainstream applications of all XR technologies over the coming years.
In the simplest possible definition, 5G is the fifth generation of cellular networking. It’s the next step in mobile technology, what the phones and tablets of the future will use for data, and it should make our current LTE networks feel as slow and irrelevant as 3G data seems now.IT’S THE NEXT STEP IN MOBILE TECHNOLOGY
To recap, the first generation of mobile networks (retroactively referred to as 1G) came out in around . It was a fully analog system until the launch of 2G (second generation networks), which made the jump to digital when it launched in . 2G also added cellular data in the forms of GPRS and EDGE technologies. Roughly  years later, 3G networks launched, offering an even faster data rate than 2G. Around  years after that, our current LTE networks — what we call 4G, although there’s some contention on what that really means — is the fourth generation of networking. Historically, that works out to a new generation of networking technology every decade or so. 5G networks will presumably offer a similar leap forward when it comes to things like data speed.When is 5G coming?Working off that model, in the best case scenario, we could see commercial 5G phones in the early s, assuming the same “every  years” pattern as previous generations holds through. LTE began to roll out (at least in the United States) in around –, so some simple math shows that we should expect to see 5G in , which is coming up quick. Chances are, we’ll see some earlier deployments even sooner than that, if the network providers, modem manufacturers, and wireless carriers are able to live up to their early projected roadmaps. Qualcomm plans to make its early 5G products available to the public as soon as the  Winter Olympics in South Korea. Like the jump from 3G to LTE, you will need a compatible phone to take advantage of 5G when it does roll around, but you’ve still got a few years to figure that out — obviously, we’re not expecting to see any 5G phones launching at MWC this year.How is 5G different from 4G?The most important thing to know about 5G is that there is no official “5G” yet. No matter what we hear at MWC this year, no matter how fast the speed test demos, or how different the networking technologies that companies use are, 5G is still a glimmer of an idea in the distance.THERE IS NO OFFICIAL “5G” YET
A 5G network will have specifications beyond those for 4G, but it hasn’t even been been agreed upon yet what those technical goalposts should even be (there is, however, a logo.) As former FCC chairman Tom Wheeler noted last summer, “If anyone tells you they know the details of what 5G will deliver, walk the other way." Expectations for commercial 5G range from internet speeds in the gigabit or even tens of gigabits range and vague goals of lower latency, but at this point in time we simply don’t know what 5G will truly look like.That said, there are some ideas of what we can expect. Companies like Verizon, AT&T, Intel, and Qualcomm are already spinning up tests for 5G technology, and it’s these early experiments that will likely shape what the formal international standard for 5G becomes. One of the commonly cited features for 5G is the use of millimeter wave (mmWave) band transmission, which could be the key to unlocking the blazing-fast internet speeds that 5G promises.What is mmWave technology? Why is it better?Cellular technology transmits data over radio waves, which depending on the type of electromagnetic signal is measured as a different frequency. The higher the frequency, the smaller the wavelength, so millimeter wave technology refers to signals with a wavelength that’s measured in millimeters, and is generally defined as between  GHz and  GHz. For 5G, the FCC has already made available swaths of the spectrum in the millimeter wave range for both licensed and unlicensed use as of last summer for companies to begin exploring 5G options (specifically, licensed use in the  GHz,  GHz, and  GHz bands, unlicensed use in the - GHz band, and shared access in the -.6 GHz band).MILLIMETER WAVE TECHNOLOGY PROMISES HIGHER DATA CAPACITY THAN WE CURRENTLY HAVE NOW
Why do we care? Because millimeter wave technology promises higher data capacity than we currently have now. A simplified rule of thumb to go by is the higher the frequency, the more data it can transmit. So, FM radio, which transmits just audio, typically broadcasts at between .5 to .0 MHz, but LTE — which is responsible for far larger data — streams between  MHz to 2, MHz (i.e., 2.1 GHz). Millimeter wave technology would offer the bandwidth for orders of magnitude of improvement over LTE. We’ve already even seen commercial use of millimeter wave technology in things like the Starry Beam. (This trend continues up the electromagnetic spectrum into visible light, which has a frequency between – THz — that’s up to , GHz — which is one of the reasons why fiber optic technology is so fast.)Another advantage to the shorter wavelengths found in millimeter wave technology is that antennas used to transmit and receive the signals can be made comparably smaller. That means that phones that use millimeter wave technology could take advantage of multiple antennas for different millimeter wave bands in a single device, which could result in a more efficient use of the available spectrum and faster internet when multiple users are connected.Millimeter wave technology comes with its own challenges, however. With higher frequencies comes shorter transmission ranges, and shorter wavelengths tend to experience greater issues when there’s no direct line of sight, along with interference from walls, buildings, window panes, and even raindrops. Whereas older radio and cellular technology were able to rely on a comparatively smaller amount of larger antenna towers, millimeter wave would need lots of smaller antennas peppered around cities and countries to function well. It’s technological issues like these that the early 5G tests will be looking to explore and solve.Gigabit LTE / LTE Advanced / LTE Advanced Pro (or, I want it now!)Usable 5G technology is still years away, though (again, there isn’t even a defined specification yet). And while LTE doesn’t deliver gigabit speeds, it’s possible that LTE Advanced and the recently finalized LTE Advanced Pro might serve as a stopgap. LTE Advanced is already available on a variety of phones, and carriers like Verizon, AT&T, and Sprint are beginning to support it on their networks. LTE Advanced Pro is the next evolution of LTE that might make practical gigabit mobile internet a reality, as well as begin to lay the groundwork for technologies that for 5G, including things like MIMO (multiple antennas) technology and use of unused spectrum in the 5 GHz LTE-U band. LTE Advanced Pro is also being set up to be a more widespread alternative build on existing technology to offer potentially gigabit level speeds for when 5G rolls out, to ensure a similar networking experience when outside of the fledging 5G areas (similar to how HSPA+ 3G networks helped bolster connectivity while LTE was rolling out).LTE Advanced is also taking advantage of a technology called carrier aggregation as a stopgap for existing LTE to reach higher speeds. It works by allowing a device to use multiple LTE bands simultaneously to allow for increased bandwidth, and therefore, increased speed. LTE is theoretically capable of aggregating up to five channels for the best speed rates, but the most we’ve seen on the market yet is three-channel aggregation, which Sprint recently rolled out.The 4G problem, or tempering expectationsWHAT YOU THINK OF AS “4G” ISN’T REALLY 4G
It’s also worth remembering to temper expectations. While on paper, LTE Advanced could offer gigabit speeds, and LTE Advanced Pro is specced for up to 3 gigabits per second, that almost certainly won’t translate directly to the real world. In fact, what you (and cellular network marketing departments) think of as “4G” or LTE isn’t really 4G according to the agreed upon standards from the International Telecommunication Union (the ITU) and the 3GPP. Per those standards, a 4G network (among other things) would provide a  megabit/s data rate when moving and a 1 gigabit/s while stationary, something that our current networks certainly aren’t capable of yet (LTE Advanced and Advanced Pro are hoping to succeed as the true candidates for a “real” fourth-generation mobile network.) So it’s possible that the dreams of gigabit LTE and 5G may not quite pan out as promised, or if they do, that the timetable could be longer than expected.Where do we stand now?Well, going into MWC, expect to hear a lot of news about ongoing 5G developments as both network and hardware companies work to have the technology in place (whether it turns out to be millimeter wave or something else entirely) to build a true fifth-generation network. We’re already starting to see news in that vein this week — Verizon announced plans for 5G testing with millimeter wave hardware in multiple cities across the US, AT&T is planning to test its own more unspecified “5G Evolution” network later this year, and Intel, Qualcomm, and Samsung all announced new chipsets that can support gigabit LTE speeds.
So while the exact details of future cellular networks — whether LTE, 5G, or beyond — may still be a little hazy, there’s one thing we can say for certain: the future will be fast.