"Question Answering is a Format; When is it Useful? Matt Gardner♠ , Jonathan Berant♠,♣ , Hannaneh Hajishirzi♠,♦, Alon Talmor♣ , and Sewon Min♦ ♠ Allen Institute for Artificial Intelligence ♣ Tel Aviv University ♦ University of Washington mattg@allenai.org  arXiv:1909.11291v1 [cs.CL] 25 Sep 2019  1 Introduction “Question answering” (QA) is a deceptively simple description of an incredibly broad range of phenomena. Its original use in the natural language processing (NLP) and information retrieval (IR) literature had a very narrow scope: answering open-domain factoid questions that a person might pose to a retrieval system (Voorhees, 1999; Kwok et al., 2001). As NLP systems have improved, people have started using question answering as a format to perform a much wider variety of tasks, leading to a dilution of the term “question answering”. This dilution is natural: questions are simply a class of sentences that can have arbitrary semantics, so “question answering” per se also has arbitrary scope. In this paper we aim to give some clarity to what “question answering” is and when it is useful in the current NLP and computer vision literature. Some researchers advocate for only using question answering when the task involves questions that humans would naturally ask in some setting  (Kwiatkowski et al., 2019; Yogatama et al., 2019; Clark et al., 2019), while others push for treating every NLP task, even classification and translation, as question answering (Kumar et al., 2016; McCann et al., 2018). Additionally, some in the NLP community have expressed fatigue at the proliferation of question answering datasets, with some conference presentations and paper submissions feeling like they need to address head-on the complaint of “yet another QA dataset”. We argue that “question answering” should be considered a format (as opposed to other formats such as slot filling) instead of a phenomenon or task in itself. Question answering is mainly a useful format in tasks that require question understanding i.e., understanding the language of the question is a non-trivial part of the task itself (detailed in Section 3). If the questions can be replaced by integer ids without meaningfully altering the nature of the task, then question understanding is not required. Sometimes, question answering is a useful format even for datasets that do not require question understanding, and we elaborate on this in Section 3.3. We argue that there are three broad motivations for using question answering as a format for a particular task. The first, and most obvious, motivation is (1) to fill human information needs, where the data is naturally formatted as question answering, because a person is asking a question. This is not the only valid use of question answering, however. It is also useful to pose a task as question answering (2) to probe a system’s understanding of some context (such as an image, video, sentence, paragraph, or table) in a flexible way that is easy to annotate, and (3) to transfer learned parameters or model architectures from one task to another (in certain limited cases). We give a detailed discussion of and motivation for these additional uses. In short: question answering is a format, not a  task. Calling a dataset a “QA dataset” is not an informative description of the nature of the task, nor is it meaningful to talk about “question answering datasets” as a cohesive group without some additional qualifier describing what kind of question answering is being performed. The community should not be concerned that many different datasets choose to use this format, but we should be sure that all datasets purporting to be “QA datasets” are in fact reasonably described this way, and that the choice of question answering as a format makes sense for the task at hand.  2 Question Answering is a Format A question is just a particular kind of natural language sentence. The space of things that can be asked in a natural language question is arbitrarily large. Consider a few examples: “What is 2 + 3?”, “What is the sentiment of this sentence?”, “What’s the fastest route to the hospital?”, and “Who killed JFK?”. It seems ill-advised to treat all of these questions as conceptually similar; just about the only unifying element between them is the question mark. “Question answering” is clearly not a cohesive phenomenon that should be considered as a “task” by NLP researchers. What, then, is question answering? It is a format, a way of posing a particular problem to a machine, just as classification or natural language inference are formats. The phrase “yet another question answering dataset” is similar in meaning to the phrase “yet another classification dataset”— both question answering and classification are formats for studying particular phenomena. Just as classification tasks differ wildly in their complexity, domain, and scope, so do question answering tasks. Question answering tasks additionally differ in their output types (consider the very different ways that one would provide an answer to the example questions above), so it is not even a particularly unified format. The community should stop thinking of “question answering” as a task and recognize it as a format that is useful in some situations and not in others. Instead, the community should consider finding useful cases of whether to pose a task as a question answering format or not. Question answering is mainly useful in tasks where understanding the question language is itself part of the task. This typically means that the phenomena being queried (i.e., the questions  in the dataset) do not lend themselves well to enumeration, because the task is either unbounded or inherently compositional. If every question in the data can be replaced by an integer id without fundamentally changing the nature of the task, then it is usually not useful to pose the task as question answering. To demonstrate this point, we begin with an extreme example. Some works treat traditional classification or tagging problems as question answering, using questions such as “What’s the sentiment?” or “What are the POS tags?” (Kumar et al., 2016; McCann et al., 2018). In these cases, not only can every question be replaced by a single integer, they can all be replaced by the same integer. There is no meaningful question understanding component in this formulation. This kind of reformulation of classification or tagging as question answering is occasionally useful, but only in rare circumstances when trying to transfer models across related datasets (Section 3.3). As a less extreme example, consider the WikiReading dataset (Hewlett et al., 2016). In this dataset, a system must read a Wikipedia page and predict values from a structured knowledge base. The type of the value, or “slot” in the knowledge base, can be represented by an integer id. One could also pose this task as question answering, by writing a question template for each slot. These templates, however, are easily memorized by a learning system given enough data, meaning that understanding the language in the templates is not a significant part of the underlying task. The template could be replaced by the integer id of the slot without changing the task; the purpose of the template is largely for humans to understand the example, not the machines.1 Attempts to have multiple surface realizations of each template do not help here; the system can still memorize template cluster ids. Even when formatted as question answering, we argue this kind of dataset is more appropriately called “slot filling”. Similarly, a dataset with a question template that involves an entity variable (e.g., When was [PERSON] born?) is simply a pairing of an integer id with an entity id, and does not require meaningful question understanding. This is still appropriately called “slot filling”. 1 Or for transfer from a QA dataset; c.f. (Levy et al., 2017), discussed in §3.3.  QA-ZRE  Some templated language datasets can be considered “question answering”, however. The CLEVR and GQA datasets (Johnson et al., 2017; Hudson and Manning, 2019), for example, use synthetic questions generated from a complex grammar. While these certainly aren’t natural language questions, the dataset is still requires question understanding, because the questions are complex, compositional queries and replacing them with single integer ids misses the opportunity of modeling the compositional structure and dramatically reducing sample complexity. There is admittedly a fuzzy line between complex slot filling cases with multiple variables and grammarbased templated generation, but we believe the basic principle is still valid: if it is reasonable to solve the problem by assigning each question an id (or an id paired with some variable instantiations), then the task does not require significant question understanding and is likely more usefully posed with a format other than question answering.  3 When QA is useful In the previous section we argued that question answering is best thought of as a format for posing particular tasks, and we gave a concrete definition for when a task should be called question answering. In this section we move to a discussion of when this format is useful. There are three very different motivations from which researchers arrive at question answering as a format for a particular task, all of which are potentially useful. The first is when the goal is to fill human information needs, where the end task involves humans naturally asking questions. The second is when the complexity or scope of a task go beyond the bounds of a fixed formalism, requiring the flexibility of natural language as an annotation and/or querying mechanism. The third is that question answering, or a unified input/output format in general, might be a way to transfer learned knowledge from one task to a related task. 3.1  Filling human information needs  There are many scenarios where people naturally pose questions to machines, wanting to receive an answer. These scenarios are too varied to enumerate, but a few examples are search queries (Dunn et al., 2017; Kwiatkowski et al., 2019), natural language interfaces to databases (Zelle and Mooney, 1996;  Berant et al., 2013; Iyer et al., 2017), and virtual assistants (Dahl et al., 1994). In addition to practical usefulness, natural questions prevent some of the biases found in artificial settings, as analyzed by Lee et al. (2019) (though they will naturally have their own biased distribution). These are “natural” question answering settings, and keeping the natural format of the data is an obvious choice that does not need further justification, so we will not dwell on this section. The danger is to think that this is the only valid use of question answering as a format. It is not, as the next two sections will show. 3.2 QA as annotation / probe When considering building a dataset to study a particular phenomenon, a researcher has many options for how that dataset should be formatted. The most common approach in NLP is to define a formalism, typically drawn from linguistics, and train people to annotate data in that formalism (e.g., part of speech tagging, syntactic parsing, coreference resolution, etc.). In computer vision research, a similar approach is taken for image classification, object detection, scene recognition, etc. When the phenomenon being studied can be cleanly captured by a fixed formalism, this is a natural approach. There are times, however, when defining a formalism is not optimal. This can be either because the formalism is too expensive to annotate, or because the phenomenon being annotated does not fit nicely into a fixed formalism. In these cases, the flexibility and simplicity of natural language annotations can be incredibly useful. For example, researchers often rely on crowd workers when constructing datasets, and training them in a linguistic formalism can be challenging. However, there are many areas of semantics that any native speaker could easily annotate in natural language, without needing to be taught a formalism (c.f. QASRL, described below). Having decided on natural language annotations instead of a fixed formalism still leaves a lot of room for choice of formats. Free-form generation, such as in image captioning and summarization, or natural language inference, are also flexible formats that use natural language as a primary annotation mechanism (Poliak et al., 2018). In what circumstances should one use question answering instead of these other options?  Question answering is often a good choice over summarization or captioning-style formats when (1) there are many things about a given context that could be queried. In summarization and captioning, only one output per input image or passage is generated. Question answering allows the dataset designer to query several different aspects of the context. Question answering may also be preferred over summarization-style formats (2) for easier evaluation. Current metrics for automatically evaluating natural language generation are not very robust (Edunov et al., 2019, inter alia). In question answering formats, restricted answer types, such as span extraction, are often available with more straightforward evaluation metrics, though those restrictions often come with their own problems, such as reasoning shortcuts or other biases (Jia and Liang, 2017; Min et al., 2019). Question answering is strictly more general than natural language inference (NLI) as a format, as an NLI example can always be converted to a QA example by phrasing the hypothesis as a question and using yes, no, or maybe as the answer. The opposite is not true, as questions with answers other than yes, no, or maybe are challenging to convert to NLI format without losing information. The question and answer can be converted into a declarative hypothesis with label “entailed”, but coming up with a useful negative training signal is non-trivial and very prone to introducing biases. Because the output space is larger in QA, there is a richer learning signal from each example. We recommend using QA over NLI as a format for new datasets in almost all cases. The remainder of this section looks at specific examples (in a non-exhaustive manner) where question answering is usefully used as an annotation mechanism for particular phenomena. In none of these cases would a human seeking information actually ask any of the questions in the dataset; the person would just look at the given context (sentence, image, paragraph, etc.) to answer their own question. There is no ”natural distribution” of questions for this kind of task. QASRL / QAMR The motivation for this work is explicitly to make annotation cheaper by having crowd workers label semantic dependencies in sentences using natural language instead of training them in a formalism (He et al., 2015; FitzGerald et al., 2018; Michael et al., 2018). In  addition, the QA pairs can also be seen as a probe for understanding the structure of the sentence. Visual question answering The motivation for this task is to evaluate understanding images and videos—the point is that QA allows capturing a much richer set of phenomena than using a fixed formalism (Antol et al., 2015; Zellers et al., 2019). Reading comprehension The motivation for this task is to demonstrate understanding of a passage of text, using various kinds of questions (Rajpurkar et al., 2016; Joshi et al., 2017; Dua et al., 2019; Amini et al., 2019). These questions aim at evaluating different phenomena, from understanding simple relationships about entities, to numerical analysis, to multi-hop reasoning. There are two recent surveys that give a good overview of the kinds of reading comprehensions that have been built so far (Liu et al., 2019; Zhang et al., 2019). The open-domain nature of the reading comprehension task makes it very unlikely that a formalism could be developed, leaving question answering as the natural way to probe a system’s understanding of longer passages of text. Background knowledge and common sense The motivation for this task is to probe whether a machine exhibits the type of background knowledge and common sense exhibited by a typical human (Mihaylov et al., 2018; Talmor et al., 2019; Sap et al., 2019), which, again, is difficult to capture with a fixed formalism. 3.3 As a transfer mechanism There has been a lot of work on transferable representation learning, trying to share linguistic or other information learned between a diverse set tasks. The dominant, and most successful, means of doing this is by sharing a common language representation layer, and having several different task-specific heads that output predictions in particular formats. An alternative approach is to pose a large number of disparate tasks in the same format. This has generally been less successful, though there are a few specific scenarios in which it appears promising. Below, we highlight two of them. The first case in which it helps to pose a nonQA task as QA is when the non-QA task is closely related to a QA task, and one can reasonably hope to get few-shot (or even zero-shot) transfer from  a trained QA model to the non-QA task. This model transfer was successfully demonstrated by Levy et al. (2017), who took a relation extraction task and used templates to pose it as QA in order to transfer a SQuAD-trained model to the task. However, as in other transfer learning or multi-task learning scenarios, this is only likely to succeed when the source and target tasks are very similar to each other. McCann et al. (2018) attempted to do multi-task learning with many diverse tasks, including machine translation, summarization, sentiment analysis, and more, all posed as question answering. In most cases, this hurt performance over training on each task independently. It seems likely that having a shared representation layer and separate prediction heads would be a more fruitful avenue for achieving this kind of transfer than posing everything as QA. The second case in which it helps to pose a nonQA task as QA is when the model architectures used in QA are helpful for the task. Das et al. (2019) and Li et al. (2019) achieve significant improvement by converting the initial format of their data (entity tracking and relation extraction, respectively) to a QA format via question templates and using a QA model with no pretraining. We hypothesize that in these cases, forcing the model to compute similarities between the input text and the words in the human-written templates provides a useful inductive bias. For example, the template “Where is CO2 created?” will encourage the model to map “where” to locations in the passage and to find synonyms of “CO2”, inductive biases which may be difficult to inject in other model architectures.  4 Conclusion In this paper, we argued that the community should think of question answering as a format, not a task, and that we should not be concerned when many datasets choose to use this format. Question answering is a useful format predominantly when the task has a non-trivial question understanding component, and the questions cannot simply be replaced with integer ids. We observed three different situations in which posing a task as question answering is useful: (1) when filling human information needs, and the data is already naturally formatted as QA; (2) when the flexibility inherent in natural language annotations is desired, either because the task does not fit into a  formalism, or training people in the formalism is too expensive; and (3) to transfer learned representations or model architectures from a QA task to another task. As NLP moves beyond sentencelevel linguistic annotation, many new datasets are being constructed, often without well-defined formalisms backing them. We encourage those constructing these datasets to think carefully about what format is most useful for them, and we have given some guidance about when question answering might be appropriate.  References Aida Amini, Saadia Gabriel, Peter Lin, Rik KoncelKedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In NAACL. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In ICCV. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In EMNLP. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In NAACL. D. A. Dahl, M. Bates, M. Brown, W. Fisher, K. Hunicke-Smith, D. Pallett, C. Pao, A. Rudnicky, and E. Shriberg. 1994. Expanding the scope of the ATIS task: The ATIS-3 corpus. In Workshop on Human Language Technology, pages 43–48. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, and Andrew McCallum. 2019. Building dynamic knowledge graphs from text using machine reading comprehension. In ICLR. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In NAACL. Matthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. Searchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint arXiv:1704.05179. Sergey Edunov, Myle Ott, Marc’Aurelio Ranzato, and Michael Auli. 2019. On the evaluation of machine translation systems trained with back-translation.  Nicholas FitzGerald, Julian Michael, Luheng He, and Luke Zettlemoyer. 2018. Large-scale qa-srl parsing. In ACL. Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015. Question-answer driven semantic role labeling: Using natural language to annotate natural language. In EMNLP.  Shanshan Liu, Xin Zhang, Sheng Zhang, Hui Wang, and Weiming Zhang. 2019. Neural machine reading comprehension: Methods and trends. ArXiv, abs/1907.01118. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering. ArXiv, abs/1806.08730.  Daniel Hewlett, Alexandre Lacoste, Llion Jones, Illia Polosukhin, Andrew Fandrianto, Jay Han, Matthew Kelcey, and David Berthelot. 2016. WikiReading: A novel large-scale language understanding task over wikipedia. In ACL.  Julian Michael, Gabriel Stanovsky, Luheng He, Ido Dagan, and Luke Zettlemoyer. 2018. Crowdsourcing question-answer meaning representations. In NAACL-HLT.  Drew A. Hudson and Christopher D. Manning. 2019. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR.  Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.  S. Iyer, I. Konstas, A. Cheung, J. Krishnamurthy, and L. Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. In ACL. Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In EMNLP. Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR. Mandar S. Joshi, Eunsol Choi, Daniel S. Weld, and Luke S. Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. Ankit Kumar, Ozan Irsoy, Jonathan Su, James Bradbury, Robert English, Brian Pierce, Peter Ondruska, Ishaan Gulrajani, and Richard Socher. 2016. Ask me anything: Dynamic memory networks for natural language processing. In ICML. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. TACL.  Sewon Min, Eric Wallace, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019. Compositional questions do not necessitate multi-hop reasoning. In ACL. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. 2018. Collecting diverse natural language inference problems for sentence representation evaluation. In BlackboxNLP@EMNLP. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In EMNLP. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Social IQa: Commonsense reasoning about social interactions. In EMNLP. A. Talmor, J. Herzig, N. Lourie, and J. Berant. 2019. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In NAACL. Ellen M. Voorhees. 1999. The TREC-8 question answering track report. In TREC.  C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scaling question answering to the web. ACM Transactions on Information Systems (TOIS), 19:242–262.  Dani Yogatama, Cyprien de Masson d’Autume, Jerome Connor, Tomás Kociský, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. 2019. Learning and evaluating general linguistic intelligence. ArXiv, abs/1901.11373.  Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In ACL.  M. Zelle and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In AAAI.  Omer Levy, Minjoon Seo, Eunsol Choi, and Luke S. Zettlemoyer. 2017. Zero-shot relation extraction via reading comprehension. In CoNLL.  Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. In CVPR.  Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, and Jiwei Li. 2019. Entity-relation extraction as multi-turn question answering. In ACL.  Xin Zhang, An Yang, Sujian Li, and Yizhong Wang. 2019. Machine reading comprehension: a literature review. ArXiv, abs/1907.01686.  "
"A generic framework for task selection driven by synthetic emotions Claudius Gros  arXiv:1909.11700v1 [cs.AI] 25 Sep 2019  Institute for theoretical physics Goethe University Frankfurt Frankfurt a.M., Germany Email: http://itp.uni-frankfurt.de/∼gros   I. I NTRODUCTION A wide range of specific tasks can be handled efficiently by present day robots and machine learning algorithm. Playing a game of Go is a typical example [1], with CT-scan classification [2] and micro-helicopter piloting [3] being two other tasks that are accomplished with comparative modest computational resources. As a next step one may consider agents that have not just a one, but a range of capabilities. like being able to play several distinct board games. Classically, as in most today’s applications, a human operator interacting with the program decides which of the agents functionality he or she wants to access. Humanized agents may however be envisioned to function without direct supervision, also with respect to task selection [4]. Autonomous agents that are endorsed with the capability to connect on their own to either a Go or a chess server could be matched f.i. with either a human opponent, upon entering the queuing system, or with another boardplaying program. The problem is then how to allocate time [5], namely how to decide which type of game to play. Deciding what to do is a cornerstone of human activities, which implies that frameworks for multi-task situations deserve attention. A possible route is to define and to maximize an overarching objective function, which could be, for example, to play alternatively Go or chess in order to improve the respective levels of expertise, as measured, f.i., by the respective win rates. This example suggest that time-allocation frameworks need two components, see Fig. 1:  A set of criteria characterizing tasks that have been executed, the experience of the agent. • A set of rules for task switching that are based fully or in part on experiences. Implementations may distinguish further between dedicated and humanized settings. For a specific dedicated application appropriate hand-picked sets of evaluation criteria and switching rules can be selected. Within this approach the resulting overall behavior can be predicted and controlled to a fair extend. Being hand-crafted, the disadvantage is that extensions and transfer to other domains demand in general extensive reworks. Consider an agent with two initial abilities, to play Go and chess via internet servers. The respective winning rates could be taken in this case as appropriate evaluation criteria, other may be a challenge (close games) and boredom (games lasting forever). As an extension, the agent is provided with a connection to a chat room, where the task is to answer questions. Humans would sent in chess board positions and the program provide in return the appropriate analysis, e.g. in terms of possible moves and winning probabilities. The program has then three possibilities, as shown in Fig. 1, to play Go or chess, and to connect to the chat room, with the third task differing qualitatively from the first two. Time allocation frameworks designed specifically for the first two options, to play Go or chess, would most probably cease to work when the chat room is added, f.i. because winning ratios are not suitable for characterizing a chat session. Here we argue that a characteristic trait of humanized computing is universality, which translates in the context of time allocation frameworks to the demand that extensions to new domains should be a minor effort. Of particular interest to humanized time allocation frameworks is emotional control, which is known to guide human decision making. Starting with an overview, we will discuss first the computational and neurobiological role of mammalian emotions, stressing that algorithmic implementations need to reproduce functionalities and not qualia like fear and joy. A concrete implementation based on the stationarity principle is then presented in a second step. Synthetic emotions correspond in this framework to a combination of abstract evaluation criteria and motivational drives that are derived from the objective to achieve a predefined time-averaged distribution •  evaluate Go  do chess  chat  Fig. 1. Illustration of a general time-allocation framework. The different options, here to play Go, to play chess and to chat, are evaluated once selected, with the evaluation results feeding back into the decision what to do next.  of emotional activities, the ‘character’ of the agent. An alternative to the here explored route to multi-task problems is multi-objective optimization [6], a setting in which distinct objectives dispose of individual utility function that need to be optimized while respecting overall resource limitation, like the availability of time. We focus here on emotional control schemes, noting that emotional control and multi-objective optimization are not mutually exclusive. II. C OMPUTATIONAL ROLE OF EMOTIONS Emotions have emerged in the last decades as indispensable preconditions for higher cognition [7], [8], with the reason being that the core task of emotional response is not direct causation of the type “fleeing when afraid”, akin to a reflex, but the induction of anticipation, reflection and cognitive feedback [9]. In general, being afraid will not result in a direct behavioral response, but in the allocation of cognitive resources to the danger at hand. The interrelation between emotion and cognition is twofaced. Emotions prime cognitive processes [10], being controlled in return by cognition [11]. The latter capability, to regulate emotions [12], f.i. when restraining one’s desire for unhealthy food, is so pronounced that it can be regarded to be a defining characteristics our species [13]. With regard to synthetic emotions, it is important to note that the cognoemotional feedback loop present in our brain implies that emotional imprints are induced whenever cognitive capabilities are used to pursue a given goal, such as playing and winning a game of Go [14]. On a neuronal level one may argue [15], that the classical distinction between affective and cognitive brain regions is misleading [16]. Behavior should be viewed instead as a complex cogno-emotional process that is based on dynamic coalitions of brain areas [17], and not on the activation of a specific structure, such as the amygdala [18]. This statement holds for the neural representations of the cognitive activity patters regulating emotional reactions, which are not localized in specific areas, but distributed within temporal, lateral frontal and parietal regions [19]. The mutual interrelation of cognitive and emotional brain states suggests a corresponding dual basis for decision making  [20]. Alternative choices are analyzed using logical reasoning, with the outcome being encoded affectively [21]. Here we use ‘evaluation criteria’ as a generic term for the associated emotional values. Risk weighting has similarly both cognitive and emotional components [22], where the latter are of particular importance for long-term, viz strategic decision taking [23]. One feels reassured if a specific outlook is both positive and certain, and uncomfortable otherwise. The picture emerging from affective neuroscience studies is that the brain uses deductive reasoning for the analysis of behavioral options and emotional states for the respective weighting. A larger number of distinct types of emotional states [24], like anger, pride, fear, trust, etc, is consequently needed when the space of accessible behavioral options increases [25]. III. C OGNO - EMOTIONAL ARCHITECTURES A minimal precondition for application scenaria incorporating a basic cogno-emotional feedback loop is the option for the program to switch between tasks [26]. An example is a multi-gaming environment for which the program decides on its own, as detailed out further below, which game to play next. A. Multi-gaming environments We consider an architecture able to play several games, such as Go, chess, Starcraft or console games like Atari. The opponents may be either human players that are drawn from a standard internet-based matchmaking systems, standalone competing algorithms or agents participating in a multi-agent challenge setup [27]. Of minor relevance is the expertise level of the architecture and whether game-specific algorithms are used. A single generic algorithm [1], such as standard Monte Carlo tree search supplemented by a value and policy generating deep network [28], would do the job. For our purpose, a key issue is the question whether the process determining which game to play is universal, in the sense that it can be easily adapted when the palette of tasks is enlarged, f.i. when the option to connect to a chat room is added. For a complete cogno-emotional feedback loop an agent able to reason logically on an at least rudimentary level would be needed. This does not hold for the application scenario considered here. As a consequence, one may incorporate the feedback of the actions of the agent onto its emotional states and the emotional priming of the decision process, but not a full-fledged cognitive control of emotions. B. Emotional evaluation criteria In a first step one has to define the qualia of the emotional states and how they are evaluated, viz the relation of distinct emotions to experiences. The following definitions are examples. – Satisfaction. Winning a game raises the satisfaction level. This could hold in particular for complex games, that is for games that are characterized, f.i., by an elevated diversity of game situations.  – Challenge. Certain game statistics may characterize a game as challenging. An example would be games for which the probability to win dropped temporarily precariously low. – Boredom. Games for which the probability to win remains constantly high could be classified as boring or, alternatively, as relaxing. The same holds for overly long games. Emotions correspond to value-encoding variables, denoted here with S, C and B, for satisfaction, challenge and boredom. Games played are evaluated using a set of explicit evaluation criteria, as formulated above. An important note is that the aim of our framework is to model key functional aspects of human emotions, which implies that there is no need, as a matter of principle, for the evaluation criteria to resemble human emotions in terms of their qualia. The latter is however likely to make it easier to develop an intuitive understanding of emotionally-driven robotic behavior. C. Direct emotional drivings vs. emotional priming Standard approaches to modeling synthetic approaches often assume that emotional state variables are explicit drivers of actions [29], either directly or via a set of internal motivations [30]. Here we are interested in contrast in frameworks that are generic in the sense that behavior is only indirectly influenced by emotional states [10]. In our case the agent updates in a first step its experience. For every type of activity, say when playing Go, the probability that a game of this type is challenging, boring or satisfying is continuously updated. It could be, e.g., that Go games are typically more challenging and less boring than chess games. Based on this set of data, the experience, the next game will be selected with the aim to align experience as close as possible with the ‘character’ of the agent, as defined in the following. D. Aligning experience with character We define the character CA of the agent as a probability distribution of emotional states,  CA = PS , PC , PB , PS + PC + PB = 1 , (1) where PS , PC , PB > 0 are the target frequencies to experience a given emotional state. Agents with a large PC would prefer for example challenging situations. The overall objective function of the agent is to align experience with his character. On a basic level, experience is expressed as a set of N probability distribution functions,  α α E α = pα α = 1, . . . , N , (2) S , pC , pB , where N is the number of possible activities (playing Go, chess, connecting to a chat room, ...). For every option α the agent records, as illustrated in Fig. 2, the probability pα i for the activity to be satisfying/challenging/boring (i = S/C/B). Defining with qα the likelihood to engage in activity α, the overall experience EA is given as X X EA = qα E α , qα = 1 , (3) α  α  where the E α are defined in (2). The global objective, to align character CA and experience EA , is achieved by minimizing the Kullback-Leibler divergence between CA and EA with respect to the qα . This strategy, which corresponds to a greedy approach, can be supplemented by an explorative component that allows to sample new opportunities [31]. Modulo exploration, an activity α is hence selected with probability qα E. Stationarity principle Our framework is based on aligning two probability distribution functions, EA and CA , an information-theoretical postulate that has been denoted the ‘stationarity principle’ [32] in the context of neuronal learning [33]. It states, that not the activity as such should be optimized, but the distribution of activities. The resulting state is then varying in time, but stationary with respect to its statistical properties. The underlying principle of the here presented framework corresponds to ‘time allocation via emotional stationarity’ (TAES). Within this approach the character of the agent serves as a guiding functional, a stochastic implementation of the principle of guided self-organization [34]. F. Motivational drives Up to now we considered purely stochastic decision making, namely that activities are selected probabilitistically, as determined by the selection probabilities qα . An interesting extension are deterministic components that correspond to emotional drives. Considering finite time spans, we denote with pi (Na ) the relative number of times that emotion i has been experienced over the course of the last Na activities. Ideally, the trailing averages pi (Na ) converge to the desired frequencies Pi . Substantial fluctuations may however occur, for example when the agent is matched repeatedly to opponents with low levels of expertise, which may lead to an extended series of boring games. The resulting temporary discrepancy, Mi = Pi − pi (Na ) , (4) between desired and trailing emotion probabilities can then be regarded as an emotional drive. Stochastically, Mi averages out, as far as possible, when selecting appropriate probabilities qα to select an activity α. On a shorter time scale one may endorse the agent with the option to reduce excessive values of Mk by direct action, viz by selecting an activity β characterized by large/small pβk when Mk is strongly positive/negative. This is however only meaningful if the distribution {pβi } is peaked and not flat. Emotional drives correspond in this context to a additional route for reaching the overall goal, the alignment of experience with character. G. Including utility maximization In addition to having emotional motivations, agents will in general be expected to maximize one or more reward functions, like gaining credits for wining games or answering questions in a chat room. Without emotional constraints, the program would just select the most advantageous option, given that all options have already been explored in sufficient  character / experience S  q(chess)  S  C  C  B  q(chat)  q(Go)  B  chess  S  C  B  Go  S  C  B  chat  Fig. 2. Aligning experience with character. Behavioral options (playing chess, playing Go, joining a chat) are evaluated along emotional criteria, such as being satisfying (S), challenging (C) or boring (B). The corresponding probability distributions are superimposed with weights qα = q(α), where α ∈ {chess, Go, chat}. See Eq. (3). The goal is to align a predefined target distribution of emotional states, the character, with the actual emotional experience. This can be achieved by optimizing the probabilities qα to engage in activity α.  depth, in analogy to the multi-armed bandit problem [35]. An interesting constellation arises when rewards are weighted emotionally, e.g. with the help of the Kullback-Leibler divergence Dα between the character and the emotional experience of a given behavioral option [36],   X Pi Dα = Pi log . (5) α p i i Credits received from behavioral options that conform with the character of the agent, having a small Dα , would be given a higher weight than credits gained when engaging in activities characterized by a large Dα . There are then two conflicting goals, to maximize the weighted utility and to align experience with character, for which a suitable prioritization or Pareto optimality may be established [37]. Instead of treating utility as a separate feature, one may introduce a new emotional trait, the desire to receive rewards, and subsume utility under emotional optimization. Depending on the target frequency PU to generate utility, the agent will select its actions such that the full emotional spectrum is taken into account. A relative weighting of utility gains, as expressed by (5), is then not necessary. IV. D ISCUSSION Computational models of emotions have focused traditionally on the interconnection between emotional stimuli, synthetic emotions and emotional responses [29]. A typical goal is to generate believable behaviors of autonomous social agents [38], in particular in connection with psychological theories of emotions, involving f.i. appraisal, dimensional aspects or hierarchical structures [29]. Closer to the scope of the present investigation are proposals that relate emotions to learning and such to behavioral choices [39]. One possibility is to use homeostatic state variables, encoding f.i. ‘well-being’, for the regulation of reinforcement learning [40]. Other state  variables could be derived from utility optimization, like water and energy uptake, or appraisal concepts [40], with the latter being examples for the abstract evaluation criteria used in the TAES framework. One route to measure well-being consist in grounding it on the relation between short- and long-term trailing reward rates [41]. Well-being can then be used to modulate dynamically the balance between exploitation (when doing well) and exploration (when things are not as they used to be). Alternatively, emotional states may impact the policy [42]. Going beyond the main trust of research in synthetic emotions, to facilitate human-computer interaction and and to use emotions to improve the performance of machine learning algorithms that are applied to dynamic landscapes, the question that has been asked here regards how an ever ongoing sequence of distinct tasks can be generated by optimizing emotional experience, in addition to reward. Formulated as a time allocation problem, the rational of this approach is drawn mainly from affective neuroscience [43], and only to a lesser extend from psychological conceptualizations of human emotional responses. Within this setting, the TAES framework captures the notion that a central role of emotions is to serve as abstract evaluation tools that are to be optimized as a set, and not individually. This premise does not rule out alternative emotional functionalities. V. C ONCLUSION Frameworks for synthetic emotions are especially powerful and functionally close to human emotions if they can be extended with ease along two directions. First, when the protocol for the inclusion of new behavioral options is applicable to a wide range of activity classes. This is the case when emotions do not correspond to specific features, but to abstract evaluation criteria. A given activity could then be evaluated as being boring, challenging, risky, demanding, easy, and so  on. It is also desirable that the framework allows for the straightforward inclusion of new traits of emotions, such as frustration. Two agents equipped with the identical framework can be expected to be able to show distinct behaviors, in analogy to the observation that human decision making is generically dependent on the character of the acting person. For synthetic emotions this implies that there should exist a restricted set of parameters controlling the balancing of emotional states in terms of a preferred distribution, the functional equivalent of character. As realized by the TAES framework, the overarching objective is to adjust the relative frequencies to engage in a specific task, such that the statistics of the experienced emotional states aligns with the character. Choosing between competing reward options can be done using a variety of strategies [44]. An example is the multiarmed bandits problem, for which distinct behavioral options yield different rewards that are initially not known [35]. Human life is characterized in comparison by behavioral options, to study, to visit a friend, to take a swim in the pool, and so on, that have strongly varying properties that come with multivariate reward dimensions. As a consequence we proposed to define utility optimization not in terms of money-like credits, as it is the case for the multi-armed bandits problem, but on an abstract level. For this one needs evaluation criteria that are functionally equivalent to emotions. In this perspective, lifelong success depends not only on the algorithmic capability to handle specific tasks, but also on the character of the agent. R EFERENCES [1] D. Silver, et al., “Mastering the game of go without human knowledge,” Nature, vol. 550, no. 7676, pp. 354–359, 2017. [2] X. W. Gao, R. Hui, and Z. Tian, “Classification of ct brain images based on deep learning networks,” Computer methods and programs in biomedicine, vol. 138, pp. 49–56, 2017. [3] V. Kumar and N. Michael, “Opportunities and challenges with autonomous micro aerial vehicles,” in Robotics Research. Springer, 2017, pp. 41–58. [4] M. Malfaz, Á. Castro-González, R. Barber, and M. A. Salichs, “A biologically inspired architecture for an autonomous and social robot,” IEEE Transactions on Autonomous Mental Development, vol. 3, no. 3, pp. 232–246, 2011. [5] C. Gros, “Emotional control–conditio sine qua non for advanced artificial intelligences?” in Philosophy and Theory of Artificial Intelligence. Springer, 2013, pp. 187–198. [6] K. Deb, “Multi-objective optimization,” in Search methodologies. Springer, 2014, pp. 403–449. [7] J. Panksepp, Affective neuroscience: The foundations of human and animal emotions. Oxford university press, 2004. [8] C. Gros, “Cognition and emotion: perspectives of a closing gap,” Cognitive Computation, vol. 2, no. 2, pp. 78–85, 2010. [9] R. F. Baumeister, K. D. Vohs, C. Nathan DeWall, and L. Zhang, “How emotion shapes behavior: Feedback, anticipation, and reflection, rather than direct causation,” Personality and social psychology review, vol. 11, no. 2, pp. 167–203, 2007. [10] J. A. Beeler, R. Cools, M. Luciana, S. B. Ostlund, and G. Petzinger, “A kinder, gentler dopamine... highlighting dopamine’s role in behavioral flexibility,” Frontiers in neuroscience, vol. 8, 2014. [11] K. N. Ochsner and J. J. Gross, “The cognitive control of emotion,” Trends in cognitive sciences, vol. 9, no. 5, pp. 242–249, 2005. [12] M. Inzlicht, B. D. Bartholow, and J. B. Hirsh, “Emotional foundations of cognitive control,” Trends in cognitive sciences, vol. 19, no. 3, pp. 126–132, 2015.  [13] D. Cutuli, “Cognitive reappraisal and expressive suppression strategies role in the emotion regulation: an overview on their modulatory effects and neural correlates,” Frontiers in Systems Neuroscience, vol. 8, 2014. [14] M. Miller and A. Clark, “Happily entangled: prediction, emotion, and the embodied mind,” Synthese, vol. 195, no. 6, pp. 2559–2575, 2018. [15] L. Pessoa, “On the relationship between emotion and cognition,” Nature reviews neuroscience, vol. 9, no. 2, p. 148, 2008. [16] ——, “Embracing integration and complexity: placing emotion within a science of brain and behaviour,” Cognition and Emotion, vol. 33, no. 1, pp. 55–60, 2019. [17] ——, “Understanding emotion with brain networks,” Current opinion in behavioral sciences, vol. 19, pp. 19–25, 2018. [18] E. A. Phelps, “Emotion and cognition: insights from studies of the human amygdala,” Annu. Rev. Psychol., vol. 57, pp. 27–53, 2006. [19] C. Morawetz, S. Bode, J. Baudewig, A. M. Jacobs, and H. R. Heekeren, “Neural representation of emotion regulation goals,” Human brain mapping, vol. 37, no. 2, pp. 600–620, 2016. [20] J. S. Lerner, Y. Li, P. Valdesolo, and K. S. Kassam, “Emotion and decision making,” Annual review of psychology, vol. 66, pp. 799–823, 2015. [21] M. Reimann and A. Bechara, “The somatic marker framework as a neurological theory of decision-making: Review, conceptual comparisons, and future neuroeconomics research,” Journal of Economic Psychology, vol. 31, no. 5, pp. 767–776, 2010. [22] A. Panno, M. Lauriola, and B. Figner, “Emotion regulation and risk taking: Predicting risky choice in deliberative decision making,” Cognition & emotion, vol. 27, no. 2, pp. 326–334, 2013. [23] R. Gilkey, R. Caceda, and C. Kilts, “When emotional reasoning trumps iq,” harvard business review, vol. 88, no. 9, p. 27, 2010. [24] H.-R. Pfister and G. Böhm, “The multiplicity of emotions: A framework of emotional functions in decision making,” Judgment and decision making, vol. 3, no. 1, p. 5, 2008. [25] T. Schlösser, D. Dunning, and D. Fetchenhauer, “What a feeling: the role of immediate and anticipated emotions in risky decisions,” Journal of Behavioral Decision Making, vol. 26, no. 1, pp. 13–30, 2013. [26] T. Rumbell, J. Barnden, S. Denham, and T. Wennekers, “Emotions in autonomous agents: comparative analysis of mechanisms and functions,” Autonomous Agents and Multi-Agent Systems, vol. 25, no. 1, pp. 1–45, 2012. [27] M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli, T. G. Rudner, C.-M. Hung, P. H. Torr, J. Foerster, and S. Whiteson, “The starcraft multi-agent challenge,” arXiv preprint arXiv:1902.04043, 2019. [28] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel et al., “Mastering chess and shogi by self-play with a general reinforcement learning algorithm,” arXiv preprint arXiv:1712.01815, 2017. [29] L.-F. Rodrı́guez and F. Ramos, “Computational models of emotions for autonomous agents: major challenges,” Artificial Intelligence Review, vol. 43, no. 3, pp. 437–465, 2015. [30] J. Velsquez, “Modeling emotions and other motivations in synthetic agents,” in Proc. 14th Nat. Conf. Artif. Intell, 1997, pp. 10–15. [31] P. Auer, “Using confidence bounds for exploitation-exploration tradeoffs,” Journal of Machine Learning Research, vol. 3, no. Nov, pp. 397– 422, 2002. [32] R. Echeveste, S. Eckmann, and C. Gros, “The fisher information as a neural guiding principle for independent component analysis,” Entropy, vol. 17, no. 6, pp. 3838–3856, 2015. [33] P. Trapp, R. Echeveste, and C. Gros, “Ei balance emerges naturally from continuous hebbian learning in autonomous neural networks,” Scientific reports, vol. 8, no. 1, p. 8939, 2018. [34] C. Gros, “Generating functionals for guided self-organization,” Guided Self-Organization: Inception, pp. 53–66, 2014. [35] J. Vermorel and M. Mohri, “Multi-armed bandit algorithms and empirical evaluation,” in European conference on machine learning. Springer, 2005, pp. 437–448. [36] C. Gros, Complex and adaptive dynamical systems: A primer. Springer, 2015. [37] O. Sener and V. Koltun, “Multi-task learning as multi-objective optimization,” in Advances in Neural Information Processing Systems, 2018, pp. 527–538. [38] K. R. Scherer, “Emotions are emergent processes: they require a dynamic computational architecture,” Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 364, no. 1535, pp. 3459–3474, 2009.  [39] S. C. Gadanho, “Learning behavior-selection by emotions and cognition in a multi-goal robot task,” Journal of Machine Learning Research, vol. 4, no. Jul, pp. 385–412, 2003. [40] T. M. Moerland, J. Broekens, and C. M. Jonker, “Emotion in reinforcement learning agents and robots: a survey,” Machine Learning, vol. 107, no. 2, pp. 443–480, 2018. [41] J. Broekens, W. A. Kosters, and F. J. Verbeek, “On affect and selfadaptation: Potential benefits of valence-controlled action-selection,” in International Work-Conference on the Interplay Between Natural and Artificial Computation. Springer, 2007, pp. 357–366. [42] T. Kuremoto, T. Tsurusaki, K. Kobayashi, S. Mabu, and M. Obayashi, “An improved reinforcement learning system using affective factors,” Robotics, vol. 2, no. 3, pp. 149–164, 2013.  [43] C. Gros, “Emotions, diffusive emotional control and the motivational problem for autonomous cognitive systems,” in Handbook of Research on Synthetic Emotions and Sociable Robotics: New Applications in Affective Computing and Artificial Intelligence. IGI Global, 2009, pp. 119–132. [44] M. I. Jordan and T. M. Mitchell, “Machine learning: Trends, perspectives, and prospects,” Science, vol. 349, no. 6245, pp. 255–260, 2015.  "
"A Multimodal Alerting System for Online Class Quality Assurance  arXiv:1909.11765v1 [cs.HC] 1 Sep 2019  Jiahao Chen, Hang Li, Wenxin Wang, Wenbiao Ding, Gale Yan Huang, and Zitao Liu? TAL AI Lab, TAL Education Group, DanLing SOHO, No.6 DanLing Street, Beijing, China {chenjiahao, lihang4, wangwenxin2, dingwenbiao, galehuang, liuzitao}@100tal.com    1  Introduction  With the recent development of technology such as digital video processing and live streaming, there has been a steady increase in the number of students enrolling online courses worldwide [2]. Online 1 on 1 class is created to offer more personalized education experience. Both students and instructors are able to choose their out-class available time slots and have the class anywhere. To better allocate education resources in China, we create an online learning platform, i.e., Dahai (http://www.dahai.com) with two distinct types of participants representing supply (instructors) and demand (students). On Dahai platform, instructors are senior college students from top Chinese universities and students come to Dahai for online tutoring. Once the study plan agreement is reached, the matched student and instructor start online courses in Dahai’s virtual classroom via live streaming. Dahai provides a wide range of online teaching tools to enable better teaching performance and interactions. Fig. 1(a) shows the 1 on 1 learning environment provided by Dahai. Example industries include accommodation (Airbnb), ride sharing (Uber, Lyft, DiDi), online shops (Etsy, Taobao), ?  Corresponding Author: Zitao Liu  2  J. Chen et al.  etc. Without a doubt, quality assurance for these types of marketplaces need to satisfy both supply and demand sides of the ecosystem in order to grow and prosper [1]. Allowing college students to be tutoring instructors1 is a double-edged sword. On one hand, it greatly alleviates the problem of imbalanced teaching resources in China. However, on the other, part-time instructors may not have enough teaching experience. Some unprofessional behaviors may lead to low class quality and inferior learning performance. Being an online education platform, Dahai is responsible for its class quality. The most common way to alleviate this problem is to allow students to give ratings for the online classes and detect low-quality classes by utilizing ratings. However, such approaches usually fail in online K12 education since K-12 students rarely give responsible ratings. For example, a student may give a 5-star rating to an instructor teaching video games. Therefore, we build a multimodal alerting system to automatically monitor the quality of each class in Dahai. Student Video  Instructor  Class Materials  Student  (a) Dahai online course scenario illustration. The virtual classroom consists of three panels: class materials, student and instructor.  Instructor Video  Video to Audio  Audio Feature Extractor  Banned Word Detector ASR Text Feature Extractor Class Quality Predictor  (b) Overview of our multimodal alerting system. The gray boxes indicate the key components in our system. ASR is short for automatic speech recognition.  Fig. 1: Dahai online course scenario illustration and an overview of our multimodal alerting system. Both student’s and instructor’s faces are hided by gray circles due to the privacy issue.  2  The Multimodal Alerting System  Multimodal information of the entire course is stored in the online learning environment. When a class is finished in Dahai, both the student and the instructor videos are passed to the backend for alerting and monitoring analysis. First, we extract audio tracks from videos. We transcribe the teaching conversations by using an automatic speech recognition (ASR) system. After that, we apply the banned word detector to scan all the contents to ensure there is no misbehavior happened. Second, we extract both the linguistic and prosodic features and build a logistic regression predictor to automatically evaluate the overall online 1 on 1 course quality. The entire workflow of our alerting system is illustrated in Fig. 1 (b). 1  Tutoring instructors have to pass a series of interviews and training exercises before teaching the class.  A Multimodal Alerting System for Online Class Quality Assurance  2.1  Banned Word Detector  Instructors may thoughtlessly speak out swearing or insulting words or phrases. These words are referred to as banned words and are definitely not allowed to appear in the class. However, banned words may happen in many scenarios. For example, instructors may lose their patience when students couldn’t response after given many hints. Another example, instructors may speak their casual mantras during the class accidentally, which contains banned words. Besides, there are also cases that the teaching environment is very noisy and the banned words appear from the background. As an education platform, we must assure a cyberbullyfree and positive learning environment. Therefore, we develop the detector to take charge of banned word monitoring. We build the banned word detector by the following two steps: Step 1. We construct a banned word bank that covers all possible banned words and their variants. In Chinese, the smallest semantic unit is character instead of word. A word is made up of several characters. This leads to more linguistic variants. To tackle this problem, we first pre-define a seed set of banned words and then expand the seed set by finding the nearest neighbor words from the gigantic Chinese Internet corpus. The nearest neighbor search is conducted in the pre-trained Chinese word embedding space. The word embeddings are learned by directional skip-gram, which explicitly distinguishes left and right context in Chinese [4]. After this expansion, we end up with a banned word bank with more than 3000 banned words. Step 2. We detect banned words by applying several fuzzy matching rules and heuristics. The matching procedure is challenging because of the recognition errors in the ASR transcriptions. To address this issue, we first write fuzzy regular expressions to retrieve banned word candidates. Our fuzzy regular expressions match not only the Chinese words but the romanization of the Chinese characters based on their pronunciation, i.e., Pinyin [5]. After that, we conduct Chinese word segmentation on the candidate words and their corresponding contexts. The segmentation process takes account into the semantic meaning of each candidate and eliminate false positive candidates. Here, we list a few classes caught by our detector in Table 1. Table 1: Examples of classes caught by the banned word detector. Examples Instructor speech snippets with banned words Class#1 Read it again. Fuck, can’t you remember these two sentence? Class#2 Damn it. I knew it. You didn’t read the paper. Class#3 (Background noises) Come. Come. There is a group of idiots.  2.2  Class Quality Predictor  Besides catching the class with banned words, we are responsible for the overall quality of the class. The 1 on 1 online class is more like a black box that only happens between the instructor and the student. First, majority of parents have  3  4  J. Chen et al.  no time to watch their kids during the class, which makes no pressure from the demand side in this online marketplace. Second, students wouldn’t tell the truth about the class quality. For example, we caught one class that the instructor spent the entire class talking about a mobile game, which makes the student highly satisfied. Third, one of the largest advantages of 1 on 1 class is that instructors are able to frequently interact with students. Students have many chances to ask questions and talk about their own thoughts. However, due to the lack of teaching experience, some instructors may still keep using the traditional offline teaching paradigm. There are barely any interactions and instructors talk for 60 minutes without stops. Therefore, we build an automated quality predictor to monitor all the online courses on Dahai. We extract linguistic features from the ASR transcriptions and prosodic features from audio tracks. The linguistic features include the number of characters, words, and sentences, the number of class subject related words, etc. The prosodic features include signal energy, loudness, mel-frequency cepstral coefficients (MFCC) [3], etc. We asked our teaching professionals to annotate 972 positive (good) courses and 219 negative (bad) courses. We use 80% of them for training our logistic regression classifier and use the rest for testing purpose. We evaluate the effectiveness of linguistic and prosodic features respectively. We report accuracy, precision, recall and F1 score of the quality prediction performance in Table 2. As we can see, both two types of features are very important to the quality prediction and the combination of both yields to the best results. Table 2: Offline experimental results of class quality prediction. Features Linguistic Only Prosodic Only Linguistic + Prosodic  2.3  Accuracy Precision Recall F1 score 0.897 0.949 0.954  0.899 0.944 0.949  0.986 0.997 0.997  0.940 0.970 0.972  Online System Performance  We deployed our monitoring and alerting system online. We set a few alerts based on the results of banned word detector and class quality predictor. Once the alarms are fired, we have operation staffs to watch the playback videos to conduct the final judgments. After comparing the staffs’ ratings with our system’s altering results, we achieve 74.3% accuracy in system alerting.  3  Conclusion & Future Work  In this paper, we presented our monitoring and alerting system for online 1 on 1 classes. By using the multimodal information, we are able to not only find misbehaviors in the online courses but measure the class quality. With the banned word detector and the class quality predictor, we are able to achieve 74.3% accuracy in our online production system. In the future, we plan to explore information from the class materials panel and improve the alerting performance as well.  A Multimodal Alerting System for Online Class Quality Assurance  References 1. Abdallah, A., Maarof, M.A., Zainal, A.: Fraud detection system: A survey. Journal of Network and Computer Applications 68, 90–113 (2016) 2. ChinaEducationResources: The largest education system in the world is going online. http://www.chinaeducationresources.com/s/OurMarket.asp (2012), [Online; accessed 5-Feb-2019] 3. Rabiner, L.R., Gold, B.: Theory and application of digital signal processing. Englewood Cliffs, NJ, Prentice-Hall, Inc., 1975. 777 p. (1975) 4. Song, Y., Shi, S., Li, J., Zhang, H.: Directional skip-gram: Explicitly distinguishing left and right context for word embeddings. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). vol. 2, pp. 175–180 (2018) 5. Wikipedia: Pinyin. https://en.wikipedia.org/wiki/Pinyin/ (2019), [Online; accessed 7-Feb-2019]  5  "
"2019 AAAI Fall Symposium on Artificial Intelligence for Human-Robot Interaction  An Automated Vehicle (AV) like Me? The Impact of Personality Similarities and Differences between Humans and AVs Qiaoning Zhang1  Connor Esterwood2  MAVRIC University of Michigan qiaoning@umich.edu  MAVRIC University of Michigan cte@umich.edu   Introduction Autonomous vehicles (AVs) are an artificial intelligence (AI)-enabled service robots. AVs are expected to provide more fuel-efficient and safer driving (Chen, Wang, and Meng 2019; Katrakazas et al. 2015; Young and Stanton 2004; Eby et al. 2016; Robert 2019). Yet, there are doubts about whether individuals will adopt AVs (Du et al. 2019). One solution to promoting the acceptance of AVs is to design them to have a similar personality as their human riders. Research on human-to-human interactions has found that humans often prefer interacting with other humans with a similar personality (Byrne and Griffitt 1969). However, the literature on human and robot personalities has found mixed results. Some studies have found that similarities in human and robot personalities led to positive human-robot interactions(Aly and Tapus 2016; Tapus and Mataric 2008). Others have found that dissimilarities in personalities led to positive interactions (Lee et al. 2006). To better understand the impacts of similarities and dissimilarities in human and AV personalities we conducted a study employing a nationwide survey of 443 individuals. This study examined the impacts of similarities and dissimilarities in human and AV personalities as they relate to the Big Five personality traits. Generally, similarities in human and robot personalities increased perceptions when both the human and AV were high in particular personality traits. Dissimilarities in human and robot personalities also yielded increases in perceptions of AV safety, but only when the AV was higher than the human in a personality trait. The positive  X. Jessie Yang  Lionel P. Robert Jr.  MAVRIC University of Michigan xijyang@umich.edu  MAVRIC University of Michigan lprobert@umich.edu  impacts of both similarities and dissimilarities were limited to agreeableness, conscientiousness and emotional stability. No such effects were found for extroversion or openness to experience. Finally, there was a moderation effect involving the experimental condition on the relationship between conscientiousness and AV safety.  Background Personality and the Big Five Personality can be used to predict human attitudes, emotions and behaviors (Robert 2018). Personality is used as a label to describe traits that represent an individual’s predisposition toward a behavior or object. Personality is now a core construct in understanding human-robot interactions (for a review see Robert 2018). The Big Five is the most popular set of personality traits in social science in general and in the study of human-robot interaction specifically (Robert 2018). The Big Five include: (1) extroversion, defined as being sociable, gregarious, and ambitious; (2) agreeableness, defined as being kind, considerate, likeable, and cooperative (Graziano and Eisenberg 1997); (3) conscientiousness, which reflects self-control and a need for achievement and order; (4) emotional stability, characterized by being well-adjusted, emotionally stable and secure; and (5) openness to experience, which is represented by flexibility of thought and tolerance of new ideas (Costa Jr, McCrae, and Dye 1991; Devaraj, Easley, and Crant 2008; Graziano and Eisenberg 1997).  Personality and Human-Robot interaction The literature on human-robot personality can be grouped into three sets. First, several authors have found that similarity in human and robot personality can lead to positive interactions. These studies are based on the underlying logic that birds of a feather flock together(Byrne and Griffitt 1969). For example, several studies have found that humans prefer interacting with robots that have their own personality over robots who have a different personality (Aly and Tapus 2016; Tapus and Mataric 2008). Second, prior research has also found that dissimilarity in human and robot personality can lead to positive interactions. These studies were based on the underlying logic that  opposites attract. This assertion has also been supported by several studies which found that humans preferred interacting with robots that had a different personality from theirs (Celiktutan and Gunes 2015; Lee et al. 2006). Finally, another view is that the impacts of human-robot similarity or dissimilarity depends heavily on a given context (Joosse et al. 2013). For example, Joosse et al. (2013) found that the relationship between similar personalities and human preference for a robot were moderated by task type. Taken as a whole, the literature on the impacts of personality similarity/dissimilarity between humans and robots has not found consistent results. In addition, little effort has been made to examine their impacts as they relate to human and AV interactions.  Figure 1: An AV drives in snowy weather.  Methodology This study employed an experimental design and was approved by the institutional review board.  Respondents and Survey Instruments A total of 443 U.S.-licensed drivers (mean age = 47.2 years, standard deviation [SD] = 15.8 years) participated. The sample was selected to represent the typical U.S. driving population based on statistics provided by the U.S. Department of Transportation and AAA Foundation(Triplett et al. 2016). This was done to ensure our sample represented the range in age, gender, ethnicity, and geographic regions of the United States. Qualtrics was hired to recruit the participants, who were paid.  Procedure Step 1, participants were required to fill out a consent form. Step 2, participants completed a survey asking for their demographic information to determine whether they qualified to participate in the study. Step 3, participants completed a survey that measured their personality. Step 4, each participant was required to watch four videos of an AV driving. Each video placed the participant in the front seat of an AV while it drove (see Figure 1). The four videos manipulated the AV driving behavior (i.e. normal or aggressive driving) and the weather conditions (i.e. sunny or snowy). The experimental study employed a 2 x 2 within-subjects design where each participant was randomly assigned to a particular video order to counterbalance any potential learning effects. Step 5, after each video, participants completed a survey to capture their perceptions of AV safety and the AV personality. Steps 4 and 5 were repeated for all four videos. Step 6, the participants were debriefed and paid.  Personality Similarity and Dissimilarity: The participant and AV personality questionnaires measured the Big Five personality traits: extroversion, agreeableness, conscientiousness, emotional stability, and openness to experience. Respondents were required to rate themselves and the AVs on a Ten-item Personality Inventory (TIPI) using a 7point Likert scale (1: disagree strongly; 7: agree strongly) (Gosling, Rentfrow, and Swann Jr 2003). The Big Five personality traits scores were divided into two groups consisting of high or low scores based on their means. Scores above the mean were classified as high and those below the mean were classified as low. This yielded a set of high and low personality categories for the participant as well as for the AV per treatment condition. The similarity and dissimilarity personality measures were as follows: Low Similarity: Participant and the AV had similar personalities but both personality scores were low. High Similarity: Participant and the AV had similar personalities and both personality scores were high. Low Dissimilarity: Participant and the AV had dissimilar personalities in that the participant had a low personality score while the AV had a high personality score. High Dissimilarity: Participant and the AV had dissimilar personalities in that the participant had a high personality score while the AV had a low personality score. Dependent Variable Safety: Safety was measured using a 10-item questionnaire(Hayes et al. 1998). All the items were rated on 5-point Likert scales (1: strongly disagree; 5: strongly agree).  Measurements Independent Variables Human-AV Interaction Conditions: The ”human-AV interaction variable” had four conditions based on the AV driving behaviors and the weather condition. Each factor had two levels representing four conditions, including: sunny and normal AV driving condition, sunny and aggressive AV driving condition, snowy and normal AV driving condition, and snowy and aggressive AV driving condition.  Results The reliability of safety was 0.97, well above 0.70. A mixed liner model was used to analyze the data, with each personality having four categorical levels: high/low similarities and high/low dissimilarities between the participant and the AV. Safety was the dependent variable. Table 1 shows the results summary. Extroversion  Extroversion was not significant (F=0.632, p=0.595). However, the low dissimilarity had the lowest mean (see Table 1). Agreeableness The main effect of agreeableness similarities/dissimilarities on safety was significant (F=6.264, p<0.001). Post-hoc comparisons indicated that the low dissimilarity produced lower safety compared to low similarity and high dissimilarity (low dissimilarity vs. low similarity, p=0.002; low dissimilarity vs. high dissimilarity, p=0.002). Also, high similarity in agreeableness led to higher safety perception than high dissimilarity (p=0.002). Conscientiousness Safety perception was significantly influenced by conscientiousness similarities/dissimilarities (F=10.040, p<0.001). Post-hoc comparisons revealed that high dissimilarity had the lowest safety rating (high dissimilarity vs. low similarity, p=0.010; high dissimilarity vs. high similarity, p<0.001; high dissimilarity vs. low dissimilarity, p<0.001). Low dissimilarity led to the highest safety perception (low dissimilarity vs. low similarity, p=0.001; low dissimilarity vs. high similarity, p=0.031; low dissimilarity vs. high dissimilarity, p<0.001). Emotional Stability There is a significant effect of emotional stability on perceptions of AV safety (F=4.921, p=0.002). Post-hoc comparisons indicated that participants gave a higher safety rating when there was a high similarity or low dissimilarity than low similarity or high dissimilarity (high similarity vs. low similarity: p=0.045; high similarity vs. high dissimilarity: p=0.002; low dissimilarity vs. low similarity: p=0.004; low dissimilarity vs. high dissimilarity: p=0.044). Openness to Experience Openness to experience was not significant (F=0.897, p=0.442). Moderation Effects There was a significant moderation effect of human-AV interaction condition on the relationship between similar/dissimilar personality in conscientiousness (F=3.70, p<.001). Figure 2 displays the two-way interaction of the relationship. AV safety was generally higher for participants high in conscientiousness when the AV drove non-aggressively in sunny weather.  Figure 2: Interaction Effects non-aggressive driving produced the highest perceptions of AV safety for people who are high conscientiousness.  Discussion This study helps to explain the mixed results in the prior literature. First, this study explains when personality similarity could be beneficial. Our findings indicate that similarity led to higher perceptions of safety when both the AV and human had high scores on personality traits such as agreeableness, conscientiousness, and emotional stability. Our paper extends prior literature by highlighting the importance of high versus low personality scores for the impacts of similarity. Second, this paper highlights when personality dissimilarity can also be good. In our study, dissimilarity was only good when perceptions of an AV’s personality were higher than the human’s personality with regard to agreeableness, conscientiousness, or emotional stability. Alternatively, when an AV’s personality was perceived as lower than the human’s personality in these traits, dissimilarity was likely to lead to lower perceived AV safety. Our paper extends prior literature by showing when dissimilarity is likely to lead to positive outcomes. Finally, this paper provides some evidence of the importance of context. Our results support this conclusion by demonstrating the moderation effect of the experimental condition on the relationship between personality similarities/dissimilarities and AV safety.  References Summary of the Results This paper has three main findings. First, there were no impacts associated with human-AV personality similarities/dissimilarities for extroversion and openness to experience. Second, for agreeableness, conscientiousness, and emotional stability, high similarity and/or low dissimilarity produced the highest perception of AV safety. Finally, there was a moderation effect associated with conscientiousness on safety. Generally, sunny weather and  Aly, A., and Tapus, A. 2016. Towards an intelligent system for generating an adapted verbal and nonverbal combined behavior in human–robot interaction. Autonomous Robots 40(2):193–209. Byrne, D., and Griffitt, W. 1969. Similarity and awareness of similarity of personality characteristics as determinants of attraction. Journal of Experimental Research in Personality. Celiktutan, O., and Gunes, H. 2015. Computational analysis of human-robot interactions through first-person vision: Personality and interaction experience. In 2015 24th IEEE  Personality dimension Extraversion (F=0.632, p=0.595) Agreeableness (F=6.264, p<0.001) Conscientiousness (F=10.040, p<0.001) Emotional Stability (F=4.921, p=0.002) Openness to experience (F=0.897, p=0.442)  Table 1: Safety rating summary Independent variable Safety means Differences High dissimilarity 2.63 None High similarity 2.62 Low dissimilarity 2.54 Low similarity 2.52 Low dissimilarity 2.76 Low similarity vs. Low dissimilarity, p=0.002 High similarity 2.65 High similarity vs. High dissimilarity, p=0.002 Low similarity 2.50 Low dissimilarity vs. High dissimilarity, p=0.002 High dissimilarity 2.40 Low dissimilarity 2.81 Low similarity vs. Low dissimilarity, p<0.001 High similarity 2.64 Low similarity vs. High dissimilarity, p=0.013 Low similarity 2.55 High similarity vs. High dissimilarity, p<0.001 High dissimilarity 2.32 Low dissimilarity vs. High dissimilarity, p<0.001 High similarity 2.70 Low similarity vs. High similarity, p=0.045 Low dissimilarity 2.69 Low similarity vs. low dissimilarity, p=0.004 Low similarity 2.47 High similarity vs. High dissimilarity, p=0.002 High dissimilarity 2.46 Low dissimilarity vs. High dissimilarity, p=0.044 High similarity 2.65 High dissimilarity 2.59 None Low similarity 2.58 Low dissimilarity 2.50  International Symposium on Robot and Human Interactive Communication (RO-MAN), 815–820. IEEE. Chen, S.; Wang, H.; and Meng, Q. 2019. Designing autonomous vehicle incentive program with uncertain vehicle purchase price. Transportation Research Part C: Emerging Technologies 103:226–245. Costa Jr, P. T.; McCrae, R. R.; and Dye, D. A. 1991. Facet scales for agreeableness and conscientiousness: A revision of the neo personality inventory. Personality and individual Differences 12(9):887–898. Devaraj, S.; Easley, R. F.; and Crant, J. M. 2008. Research note-how does personality matter? relating the five-factor model to technology acceptance and use. Information systems research 19(1):93–105. Du, N.; Haspiel, J.; Zhang, Q.; Tilbury, D.; Pradhan, A. K.; Yang, X. J.; and Robert Jr, L. P. 2019. Look who’s talking now: Implications of av’s explanations on driver’s trust, av preference, anxiety and mental workload. Transportation Research Part C: Emerging Technologies 104:428–442. Eby, D. W.; Molnar, L. J.; Zhang, L.; Louis, R. M. S.; Zanier, N.; Kostyniuk, L. P.; and Stanciu, S. 2016. Use, perceptions, and benefits of automotive technologies among aging drivers. Injury epidemiology 3(1):28. Gosling, S. D.; Rentfrow, P. J.; and Swann Jr, W. B. 2003. A very brief measure of the big-five personality domains. Journal of Research in personality 37(6):504–528. Graziano, W. G., and Eisenberg, N. 1997. Agreeableness: A dimension of personality. In Handbook of personality psychology. Elsevier. 795–824. Hayes, B. E.; Perander, J.; Smecko, T.; and Trask, J. 1998. Measuring perceptions of workplace safety: Development  and validation of the work safety scale. Journal of Safety research 29(3):145–161. Joosse, M.; Lohse, M.; Pérez, J. G.; and Evers, V. 2013. What you do is who you are: The role of task context in perceived social robot personality. In 2013 IEEE International Conference on Robotics and Automation, 2134–2139. IEEE. Katrakazas, C.; Quddus, M.; Chen, W.-H.; and Deka, L. 2015. Real-time motion planning methods for autonomous on-road driving: State-of-the-art and future research directions. Transportation Research Part C: Emerging Technologies 60:416–442. Lee, K. M.; Peng, W.; Jin, S.-A.; and Yan, C. 2006. Can robots manifest personality?: An empirical test of personality recognition, social responses, and social presence in human–robot interaction. Journal of communication 56(4):754–772. Robert, L. 2018. Personality in the human robot interac-tion literature: A review and brief critique. In Proceedings of the 24th Americas Conference on Information Systems, Aug, 16–18. Robert, L. P. 2019. Are automated vehicles safer than manually driven cars? AI & SOCIETY 1–2. Tapus, A., and Mataric, M. J. 2008. Socially assistive robots: The link between personality, empathy, physiological signals, and task performance. In AAAI spring symposium: emotion, personality, and social behavior, 133–140. Triplett, T.; Santos, R.; Rosenbloom, S.; and Tefft, B. 2016. American driving survey: 2014–2015. Young, M. S., and Stanton, N. A. 2004. Taking the load off: investigations of how adaptive cruise control affects mental workload. Ergonomics 47(9):1014–1035.  "
"arXiv:1909.11830v1 [cs.LG] 26 Sep 2019  I MPROVING SAT S OLVER H EURISTICS WITH G RAPH N ETWORKS AND R EINFORCEMENT L EARNING Vitaly Kurin ∗ Department of Computer Science University of Oxford Oxford, United Kingdom vitaly.kurin@magd.ox.ac.uk  Saad Godil NVIDIA Santa Clara, California United States sgodil@nvidia.com  Shimon Whiteson Department of Computer Science University of Oxford Oxford, United Kingdom shimon.whiteson@cs.ox.ac.uk  Bryan Catanzaro NVIDIA Santa Clara, California United States bcatanzaro@nvidia.com    I NTRODUCTION  Boolean satisfiability (SAT) is an important problem for both industry and academia impacting various fields, including circuit design, computer security, artificial intelligence, automatic theorem proving, and combinatorial optimization. As a result, modern SAT solvers are well-crafted, sophisticated, reliable pieces of software that can scale to problems with hundreds of thousands of variables (Ohrimenko et al., 2009). SAT is known to be NP-complete (Karp, 1972), and most state-of-the-art open-source and commercial solvers rely on multiple heuristics to speed up the exhaustive search, which is otherwise intractable. These heuristics are usually meticulously crafted using expert domain knowledge and are often iterated on using trial and error. In this paper, we investigate how we can use machine learning to improve upon an existing branching heuristic without leveraging domain expertise. We present Graph-Q-SAT (GQSAT), a branching heuristic in a Conflict Driven Clause Learning (Marques-Silva & Sakallah, 1999; Bayardo Jr & Schrag, 1997, CDCL) SAT solver trained with value-based reinforcement learning (RL), based on DQN (Mnih et al., 2015). GQSAT uses a graph ∗  The work was done when the author was a research intern at NVIDIA.  1  representation of SAT problems similar to Selsam et al. (2018) which provides permutation and variable relabeling invariance. It uses a Graph Neural Network (Gori et al., 2005; Battaglia et al., 2018, GNN) as a function approximator to provide generalization as well as support for a dynamic state-action space. GQSAT uses a simple state representation and a binary reward that requires no feature engineering or problem domain knowledge. GQSAT modifies only part of the CDCL based solver, keeping it complete, i.e. always leading to a correct solution. We demonstrate that GQSAT outperforms Variable State Independent Decaying Sum (Moskewicz et al., 2001, VSIDS), most frequently used CDCL branching heuristic, reducing the number of iterations required to solve SAT problems by 2-3X. GQSAT is trained to examine the structure of the particular problem instance to make better decisions at the beginning of the search whereas the VSIDS heuristic suffers from bad decision during the warm-up period. We show that our method generalizes to problems five times larger than those it was trained on. We also show that our method generalizes across problem types from SAT to unSAT. We also show, to a lesser extent, it generalizes to SAT problems from different domains, such as graph colouring. Finally, we show that some of these improvements are achieved even when training is limited to single SAT problem demonstrating data efficiency of our method. We believe GQSAT is a stepping stone to a new generation of SAT solvers leveraging data to build better heuristics learned from past experience.  2 2.1  BACKGROUND SAT PROBLEM  A SAT problem involves finding variable assignments such that a propositional logic formula is satisfied or showing that such an assignment does not exist. A propositional formula is a Boolean expression, including Boolen variables, ANDs, ORs and negations. ’x’ or ’NOT x’ make up a literal. It is convenient to represent Boolean formulas in conjunctive normal form (CNF), i.e., conjunctions (AND) of clauses, where a clause is a disjunction (OR) of literals. An example of a CNF is (x1 ∨ ¬x2 ) ∧ (x2 ∨ ¬x3 ), where ∧, ∨, ¬ are AND, OR, and negation respectively. This CNF has two clauses: (x1 ∨ ¬x2 ) and (x2 ∨ ¬x3 ). In this work, we use SAT to both denote the Boolean Satisfiability problem and a satisfiable instance, which should be clear from the context. We use unSAT to denote unsatisfiable instances. There are many types of SAT solvers. In this work, we focus on CDCL solvers, MiniSat (Sorensson & Een, 2005) in particular, because it is an open source, minimal, but powerful implementation. A CDCL solver repeats the following steps: every iteration it picks a literal, assigns a variable a binary value. This is called a decision. After deciding, the solver simplifies the formula building an implication graph, and checks whether a conflict emerged. Given a conflict, it can infer (learn) new clauses and backtrack to the variable assignments where the newly learned clause becomes unit (consisting of a single literal) forcing a variable assignment which avoids the previous conflict. Sometimes, CDCL solver undoes all the variable assignments keeping the learned clauses to escape futile regions of the search space. This is called a restart. We focus on the branching heuristic because it is one of the most heavily used during the solution procedure. The branching heuristic is responsible for picking the variable and assigning some value to it. VSIDS (Moskewicz et al., 2001) is one of the most used CDCL branching heuristics. It is a counter-based heuristic which keeps a scalar value for each literal or variable (MiniSat uses the latter). These values are increased every time a variable gets involved in a conflict. The algorithm behaves greedily with respect to these values called activities. Activities are usually initialized with zeroes (Liang et al., 2015). 2.2  R EINFORCEMENT L EARNING  We formulate the RL problem as a Markov decision process (MDP). An MDP is a tuple hS, A, R, T , ρ, γi with a set of states S, a set of actions A, a reward function R = R(s, a, s0 ) and the transition function T = p(s, a, s0 ), where p(s, a, s0 ) is a probability distribution, s, s0 ∈ S, a ∈ A. ρ is the probability distribution over initial states. γ ∈ [0, 1) is the discount factor responsible for trading off the preferences between the current immediate reward and the future reward. To solve an MDP means to find an optimal policy, a mapping which outputs an action or distribu2  tion over given the state, such that we maximize the expected discounted cumulative return Pactions ∞ R = E[ t=0 γ t rt ], where rt = R(st , at , st+1 ) is the reward for the transition from st to st+1 . In Section 3 we apply deep Q-networks (Mnih et al., 2015, DQN), a value-based RL algorithm that approximates an optimal Q-function, an action-value function that estimates the sum of future rewards after taking an action a in state s and following the optimal policy π thereafter: Q∗ (s, a) = Eπ,T ,ρ [R(s, a, s0 ) + γ maxa0 Q∗ (s0 , a0 )]. A mean squared temporal difference (TD) error is used to make an update step: L(θ) = (Qθ (s, a) − r − γ maxa0 Qθ̄ (s0 , a0 ))2 . Qθ̄ is called a target network (Mnih et al., 2015). It is used to stabilise DQN by splitting the decision and evaluation operations. Its weights are copied from the main network Qθ after each k minibatch updates. 2.3  G RAPH N EURAL N ETWORKS  We use Graph Neural Networks (Gori et al., 2005, GNN) to approximate our Q-function due to their input size, structure, and permutation invariance. We use the formalism of Battaglia et al. (2018) which unifies most existing GNN approaches. Under this formalism, GNN is a set of functions that take a labeled graph as input and output a graph with modified labels but the same topology. Here, a graph is a directed graph hV, E, U i, where V is the set of vertices, E is the set of edges with e = (s, r) ∈ E, s, r ∈ V , and U is a global attribute. The global attribute contains information, relevant to the whole graph. We call vertices, edges and the global attribute entities. Each entity has its features vectors. A GNN changes this features as a result of its operations. A graph network can be seen as a set of six functions: three update functions and three aggregation functions. The information propagates between vertices along graph edges. Update functions compute new entity labels. Aggregation functions exist to ensure the GNN’s ability to process graphs of arbitrary topologies, compressing multiple entities features into vectors of fixed size. GNN blocks can be combined such that the output of one becomes input of the other. For example, the EncodeProcess-Decode architecture (Battaglia et al., 2018) processes the graph in a recurrent way, enabling information propagation between remote vertices.  3  GQSAT  We represent the set of all possible SAT problems as an MDP. The state of such an MDP consists of unassigned variables and unsatisfied clauses. The action set includes two actions for each unassigned variable: assigning it to true or false. The initial state distribution is a distribution over all SAT problems. We modify the MiniSat-based environment of Wang & Rompf (2018) which is responsible for the transition function. It takes the actions, modifies its implication graph internally and returns a new state, containing newly learned clauses and without the variables removed after the propagation. Strictly speaking, this state is not fully observable. In the case of a conflict, the solver undoes the assignments for variables that are not in the agent’s observation. However, in practice, this should not inhibit the goal of quickly pruning the search tree: the information in the state is enough to pick a variable that leads to more propagations in the remaining formula. We use a simple reward function: the agent gets a negative reinforcement p for each non-terminal transition and 0 for reaching the terminal state. This reward encourages an agent to finish an episode as quickly as possible and does not require elaborate reward shaping to start using GQSAT. 3.1  S TATE R EPRESENTATION  We represent a SAT problem as a graph similar to Selsam et al. (2018). We make it more compact, using vertices to denote variables instead of literals. We use nodes to encode clauses as well. Our state representation is simple and does not require scrupulous feature engineering. An edge (xi , ci ) means that a clause ci contains literal xi . If a literal contains a negation, a corresponding edge has a [1, 0] label and [0, 1] otherwise. GNN process directed graphs so we create two directed edges with the same labels: from a variable to a clause and vice-versa. Vertex features are two dimensional one-hot vectors, denoting either a variable or a clause. We do not provide any other information to the model. The global attribute input is empty and is only used for message passing. Figure 1a gives an example of the state for (x1 ∨ x2 ) ∧ (¬x2 ∨ x3 ). 3  [1,0]  x1  [1,0]  x2  0,1 c1 0,1 1,0 c2 0,1  [1,0]  [42.0 , 3.14]  x1  [1.62 , 2.70]  x2  [6.02 , 1.67]  x3  [0,1]  c1  [0,1]  c2  x3  (a) Bipartite graph representation of the Boolean formula (x1 ∨ x2 ) ∧ (¬x2 ∨ x3 ). The numbers next to the vertices distinguish variables and clauses. Edge labels encode literal polarities. The global component does not contain any information about the state.  (b) Graph Q-function values for setting variables to true and false respectively. The action is chosen using arg max across all Q values of variable nodes. Since GNN work on directed graphs, we add two directional edges to connect two nodes.  Figure 1: State-action space representation of GQSAT  3.2  Q- FUNCTION REPRESENTATION  We use the encode-process-decode architecture (Battaglia et al., 2018), which we discuss in more detail in Appendix B.1. Similarly to Bapst et al. (2019), our GNN labels variable vertices with Qvalues. Each variable vertex has two actions: pick the variable and set it to true or false as shown on Figure 1b. We choose the action which gives the maximum Q-value across all variable vertices. The graph contains only unassigned variables so all actions are valid. We use common DQN techniques such as memory replay, target network and -greedy exploration. To expose the agent to more episodes and prevent it from getting stuck, we cap the maximum number of actions per episode. This is similar to the episode length parameter in gym (Brockman et al., 2016). 3.3  T RAINING AND E VALUATION P ROTOCOL  We train our agent using Random 3-SAT instances from the SATLIB benchmark (Hoos & Stützle, 2000). To measure generalization, we split these data into train, validation and test sets. The train set includes 800 problems, while the validation and test sets are 100 problems each. We provide more details about dataset in Appendinx B.2. To illustrate the problem complexities, Table 1 provides the number of steps it takes MiniSat to solve the problem. Each random 3-SAT problem is denoted as SAT-X-Y or unSAT-X-Y, where SAT means that all problems are satisfiable, unSAT stands for all problems are unsatisfiable. X and Y stands for the number of variables and clauses in the initial formula. While random 3-SAT problems have relatively small number of variables/clauses, they have an interesting property which makes them more challenging for a solver. For this dataset, the ratio of clauses to variables is close to 4.3:1 which is near the phase transition, when it is hard to say whether the problem is SAT or unSAT (Cheeseman et al., 1991). In 3-SAT problems each clause has exactly 3 variables, however, learned clauses might be of arbitrary size and GQSAT is able to deal with it. We use Median Relative Iteration Reduction (MRIR) w.r.t. MiniSat as our main performance metric which is a number of iterations it takes MiniSat to solve a problem divided by GQSAT’s number of iterations. By one iteration we mean one decision, i.e. choosing a variable and setting it to a value. MRIR is the median across all the problems in the dataset. We compare ourselves with the best MiniSat results having run MiniSat with and without restarts. We cap the number of decisions our method takes at the beginning of the solution procedure and then we give control to MiniSat. When training, we evaluate the model every 1000 batch updates on the validation subsets of the same distribution as the train dataset and pick the one with the best validation results. After that, we evaluate this model on the test dataset and report the results. For each of the model we do 5 training runs and report the average MRIR results, the maximum and the minimum. We implement our models using Pytorch (Paszke et al., 2017) and Pytorch Geometric (Fey & Lenssen, 2019). We provide all the hyperparameters needed to reproduce our results in Appendix B. We will release our experimental code as well as the MiniSat gym environment. 4  Table 2: MRIR for GQSAT trained on SAT50-218. Evaluation for SAT-50-218 is on a separate test data not seen during training.  Table 1: Number of iterations (no restarts) it takes MiniSat to solve random 3-SAT instances. dataset  median  mean  dataset  mean  min  max  SAT 50-218 SAT 100-430 SAT 250-1065  38 232 62 192  42 286 76 120  SAT 50-218 SAT 100-430 SAT 250-1065  2.46 3.94 3.91  2.26 3.53 2.88  2.72 4.41 5.22  unSAT 50-128 unSAT 100-430 unSAT 250-1065  68 587 178 956  68 596 182 799  unSAT 50-128 unSAT 100-430 unSAT 250-1065  2.34 2.24 1.54  2.07 1.85 1.30  2.51 2.66 1.64  Iterations improvement  unSAT 100-430 unSAT 250-1065 unSAT 50-218  SAT 100-430 SAT 250-1065 SAT 50-218  10 Iterations improvement  SAT 100-430 SAT 250-1065 SAT 50-218  5 4 3 2  8  unSAT 100-430 unSAT 250-1065 unSAT 50-218  6 4 2  1 0  10  50 100 300 # model decisions  500  1000  1  Figure 2: GQSAT number of maximum first decisions vs performance. GQSAT shows improvement starting from 10 iterations confirming our hypothesis of VSIDS initialization problem. Best viewed in colour.  4 4.1  10  50  100 300 Training set size  500  800  Figure 3: Dataset size effect on generalization. While GQSAT profits from having more data in most of the cases, it is able to generalize even from one data point. Model is trained on SAT50-218. Best viewed in colour.  E XPERIMENTAL RESULTS I MPROVING UPON VSIDS  In our first experiment, we consider whether it is possible improve upon VSIDS using no domain knowledge, a simple state representation, and a simple reward function. The first row in Table 2 gives us a positive answer to that question. DQN equipped with GNN as a function approximation solves the problems in fewer than half the iterations of MiniSat. GQSAT makes decisions resulting in more propagations, i.e., inferring variable values based on other variable assignments and clauses. This helps GQSAT prune the search tree faster. For SAT50-218, GQSAT does on average 2.44 more propagations than MiniSat (6.62 versus 4.18). We plot the average number of variable assignments for each problem individually in the Appendix A. These results raise the question: Why does GQSAT outperform VSIDS? VSIDS is a counter-based heuristic that takes time to warm up. Our model, on the other hand, perceives the whole problem structure and can make more informed decisions from step one. To check this hypothesis, we vary the number of decisions our model makes at the beginning of the solution procedure before we hand the control back to VSIDS. The results of the experiment in Figure 2 support this hypothesis. Even if our model is used for only the first ten iterations, it still improves performance over VSIDS. One strength of GQSAT is that VSIDS keeps being updated while the decisions are made with GQSAT. We believe that GQSAT complements VSIDS by providing better quality decisions in the initial phase while VSIDS is warming up. Capping the number of model calls can significantly reduce the main bottleneck of our approach – wall clock time spent on model evaluation. Optimizing for speed was not our focus, however even with the current unoptimized implementation, if we use the model for the first 500 iterations and assuming this gives us a 2x reduction in total iterations, our approach is competitive if it takes more than 20 seconds for a base solver to solve the problem. 5  Table 3: SAT-50 models performance on SATLIB flat graph coloring benchmark. Median Relative Iteration Reduction (MRIR) is w.r.t. MiniSat with restarts, since MiniSat performs better in this mode for this benchmark. dataset flat-30-60 flat-50-115 flat-75-80 flat-100-239 flat-125-301 flat-150-360 flat-175-417 flat-200-479  4.2 4.2.1  variables  clauses  MiniSat median iterations  90 150 225 300 375 450 525 600  300 545 840 1117 1403 1680 1951 2237  10 15 29 55 106 179 272 501  GQSAT MRIR average min max 1.51 1.36 1.40 1.44 1.02 0.76 0.67 0.67  1.25 0.47 0.31 0.31 0.32 0.37 0.44 0.54  1.65 1.80 2.06 2.38 1.87 1.40 1.36 0.87  G ENERALIZATION P ROPERTIES OF GQSAT G ENERALIZATION ACROSS PROBLEM SIZES  Table 2 shows that GQSAT has no difficulties generalizing to bigger problems, showing almost 4x improvement in iterations for the dataset 5 times bigger than the training set. GQSAT on average leads to more variable assignments changes per step, e.g., 7.58 vs 5.89 on SAT-100-430. It might seem surprising that the model performs better for larger problems. However, our performance metric is relative. An increase in score for different problem sizes might also mean that the base solver scales worse than our method does for this benchmark. 4.2.2  G ENERALIZATION FROM SAT TO UN SAT  An important characteristic of GQSAT is that the problem formulation and representation makes it possible to solve unSAT problems when training only on SAT, which was problematic for some of the existing approaches (Selsam et al., 2018). The performance is, however, worse than the performance on satisfiable problems. On the one hand, SAT and unSAT problems are different. When the solver finds one satisfying assignment, the problem is solved. For unSAT, the algorithm needs to exhaust all possible options to prove that there is no such assignment. On the other hand, there is one important similarity between these two types of problems – an algorithm has to prune the search tree as fast as possible. Our measurements of average propagations per step demonstrate that GQSAT learns how to prune the tree more efficiently than VSIDS (6.36 vs 4.17 for unSAT-50-218). 4.2.3  G ENERALIZATION ACROSS PROBLEM STRUCTURES  SAT problems have distinct structures. The graph representation of a random 3-SAT problem looks much different than that of a graph coloring problem. To investigate how much our model, trained on SAT-50, can generalize to problems of different structures, we evaluate it on the flat graph coloring benchmark from SATLIB (Hoos & Stützle, 2000). All the problems in the benchmark are satisfiable. Table 3 shows a decrease in GQSAT performance when generalizing to another problem distribution. We believe there are two potential reasons. First, different SAT problem distributions have different graph properties that are not captured during training on another distribution. Second, this might be related to our model selection process which does not favor generalization across problem structures. Table 3 shows that graph coloring problems have more variables. We conducted an experiment investigating GQSAT’s ability to scale to larger problems (more variables, more clauses). We trained GQSAT on flat75-180 with problems of 225 variables and 840 clauses. Graph Coloring benchmarks have only 100 problems each, so we do not split them into train/validation/test dataset using flat-7580 for training and flat-100-239 to do model selection. We use the same hyperparameters as in all previous experiments changing only the gradient clipping parameter to 0.1. The results on Table 4 show that GQSAT can scale to bigger problems on the flat graph coloring benchmark. 6  Table 4: GQSAT Median Relative Iteration Reduction (MRIR) for the model trained on flat-75-80, evaluated on flat-100-239. flat-75-80 and flat-100-239 are separated, because the model was trained using the former and validated on the second. The results are for five training runs. dataset  GQSAT MRIR average min max  flat75-180 flat-100-239  2.44 2.89  2.25 2.77  2.70 2.98  flat-30-60 flat-50-115 flat-125-301 flat-150-360 flat-175-417 flat-200-479  1.74 2.08 2.43 2.07 1.98 1.70  1.33 2.00 2.20 2.00 1.69 1.38  2.00 2.13 2.66 2.11 2.21 1.98  Apart from scaling to bigger graphs, we could test scaling for longer episodes. Table 1 shows exponential growth in the number of iterations it takes MiniSat to solve larger problems. Our preliminary experiments show that generalizing is easier than learning. Learning on SAT-100-430 requires more resources, does not generalize as well, and is generally less stable than training on SAT-50-218. This is most likely related to higher variance in the returns caused by longer episodes, challenges for temporal credit assignment, and difficulties with exploration, motivating further research. It also motivates curriculum learning as the next step of GQSAT development. Bapst et al. (2019) shows a positive effect of curriculum learning on RL with GNN. 4.3  DATA E FFICIENCY  We design our next experiment to understand how many different SAT problems GQSAT needs to learn from. We varied the SAT-50-218 train set from a single problem to 800 problems. Figure 3 demonstrates that GQSAT is extremely data efficient. Having more data helps in most cases but, even with a single problem, GQSAT generalizes across problem sizes and to unSAT instances. This should allow GQSAT to generalize to new benchmarks without access to many problems from it.  5  R ELATED W ORK  Using machine learning for the SAT problem is not a new idea (Haim & Walsh, 2009; Grozea & Popescu, 2014; Flint & Blaschko, 2012; Singh et al., 2009). Xu et al. (2008) propose a portfoliobased approach which yielded strong results in 2007 SAT competition. Liang et al. (2016) treat each SAT problem as a multi-armed bandit problem capturing variables’ ability to generate learnt clauses. Recently, SAT has attracted interest in the deep learning community. There are two main approaches: solving a problem end-to-end or learning heuristics while keeping the algorithm backbone the same. Selsam et al. (2018, NeuroSAT) take an end-to-end supervised learning approach demonstrating that GNN can generalize to SAT problems bigger than those used for training. NeuroSAT finds satisfying assignments for the SAT formulae and thus cannot generalize from SAT to unSAT problems. Moreover, the method is incomplete and might generate incorrect results, which is extremely important especially for unSAT problems. Selsam & Bjørner (2019) modify NeuroSAT and integrate it into popular SAT solvers to improve timing on SATCOMP-2018 benchmark. While the approach shows its potential to scale to large problems, it requires an extensive training set including over 150,000 data points. Amizadeh et al. (2018) propose an end-to-end GNN architecture to solve circuit-SAT problems. While their model never produces false positives, it cannot solve unSAT problems. The following methods take the second approach learning a branching heuristic instead of learning an algorithm end-to-end. Jaszczur et al. (2019) take the supervised-learning approach using the same graph representation as in Selsam et al. (2018). The authors show a positive effect of combining DPLL/CDCL solver with the learnt model. As in Selsam et al. (2018), their approach requires a diligent process of the test set crafting. Also, the authors do not compare their approach to the VSIDS heuristic, which is known to be crucial component of CDCL (Katebi et al., 2011). 7  Wang & Rompf (2018), whose environment we took as a starting point, show that DQN does not generalize for 20-91 3-SAT problems, whereas Alpha(GO) Zero does. Our results show that the issue is related to the state representation. They use CNNs, which are not invariant to variable renaming or permutations. Moreover, CNNs require a fixed input size which makes it infeasible when applying to problems with different number of variables or clauses. The work of Lederman et al. (2018) is closest to ours. They train a REINFORCE (Williams, 1992) agent to replace the branching heuristic for Quantified Boolean Formulas using GNNs for function approximation. They note positive generalization properties across the problem size for problems from similar distributions. Besides the base RL algorithm and some minor differences, our approaches differ mainly in the state representation. They use 30 variables for the global state encoding and seven variables for vertex feature vectors. GQSAT does not require feature engineering to construct the state. We use only two bits to distinguish variables from clauses and encode literal polarities. Also, Lederman et al. (2018) use separate vertices for x and ¬x in the graph representation. Vinyals et al. (2015) introduce a recurrent architecture for approximately solving complex geometric problems, such as the Traveling Salesman Problem (TSP), approaching it in a supervised way. Bello et al. (2016) consider combinatorial optimization problems with RL, showing results on TSP and the Knapsack Problem. Khalil et al. (2017) approach combinatorial optimization using GNNs and DQN, learning a heuristic that is later used greedily. It is slightly different from the approach we take since their heuristic is effectively the algorithm itself. We augment only a part of the algorithm – the branching heuristic. Paliwal et al. (2019) use GNN with imitation learning for theorem proving. Carbune et al. (2018) propose a general framework of injecting an RL agent into existing algorithms. Cai et al. (2019) use RL to find a suboptimal solution that is further refined by another optimization algorithm, simulated annealing (Kirkpatrick et al., 1983, SA) in their case. The method is not limited with SA, and this modularity is valuable. However, there is one important drawback of the approach. The second optimization algorithm might benefit more from the first algorithm if they are interleaved. For instance, GQSAT can guide search before VSIDS overcomes its initialization bias. Recently, GNN received a lot of attention in the RL community, enabling the study of RL agents in state/action spaces of dynamic size, which is crucial for generalization beyond the given task. Wang et al. (2018) and Sanchez-Gonzalez et al. (2018) consider GNN for the generalization of the control problem. Bapst et al. (2019) investigate graph-based representation for the construction task and notice high generalization capabilities of their agents. Jiang et al. (2018); Malysheva et al. (2018); Agarwal et al. (2019) study generalization of the behaviour in multi-agent systems, noting the GNN benefits due to their invariance to the number of agents in the team or other environmental entities.  6  C ONCLUSION AND F UTURE W ORK  In this paper, we introduced GQSAT, a branching heuristic of a SAT solver that causes more variable propagations per step solving the SAT problem in fewer iterations comparing to VSIDS. GQSAT uses a simple state representation and does not require elaborate reward shaping. We demonstrated its generalization abilities, showing more than 2-3X reduction in iterations for the problems up to 5X larger and 1.5-2X from SAT to unSAT. We showed how GQSAT improves VSIDS and showed that our method is data efficient. While our method generalizes across problem structures to a lesser extent, we showed that training on data from other distributions might lead to further performance improvements. Our findings lay the groundwork for future research that we outline below. Scaling GQSAT to larger problems. Industrial-sized benchmarks have millions of variables. Our experiments training on SAT-100 and graph coloring show that increases in problem complexity makes our method less stable due to typical RL challenges: longer credit assignment spans, reward shaping, etc. Further research will focus on scaling GQSAT using latest stabilizing techniques (Hessel et al., 2018), more sophisticated exploration methods and curriculum learning. From reducing iterations to speeding up. SAT heuristics are good because they are fast. It takes constant time to make a decision with VSIDS. GNN inference takes much longer. However, our experiments show that GQSAT can show an improvement using only the first k steps. An efficient C++ implementation of our method should also help. 8  Interpretation of the results. Newsham et al. (2014) show that the community structure of SAT problems is related to the problem complexity. We are interested in understanding how graph structure influences the performance of GQSAT and how we can exploit this knowledge to improve GQSAT. Although we showed the powerful generalization properties of graph-based RL, we believe the problem is still far from solved and our work is just one stepping stone towards a new generation of solvers that can discover and exploit heuristics that are too difficult for a human to design. ACKNOWLEDGMENTS The authors would like to thank Rajarshi Roy, Robert Kirby, Yogesh Mahajan, Alex Aiken, Mohammad Shoeybi, Rafael Valle, Sungwon Kim and the rest of the Applied Deep Learning Research team at NVIDIA for useful discussions and feedback. The authors would also like to thank Andrew Tao and Guy Peled for providing computing support.  R EFERENCES Akshat Agarwal, Sumit Kumar, and Katia Sycara. Learning transferable cooperative behavior in multi-agent teams, 2019. Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. Learning to solve circuit-sat: An unsupervised differentiable approach. 2018. Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly L. Stachenfeld, Pushmeet Kohli, Peter W. Battaglia, and Jessica B. Hamrick. Structured agents for physical construction, 2019. Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. Roberto J Bayardo Jr and Robert Schrag. Using csp look-back techniques to solve real-world sat instances. In Aaai/iaai, pp. 203–208. Providence, RI, 1997. Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym, 2016. Qingpeng Cai, Will Hang, Azalia Mirhoseini, George Tucker, Jingtao Wang, and Wei Wei. Reinforcement learning driven heuristic optimization, 2019. Victor Carbune, Thierr"
"Efficient Large-Scale Multi-Drone Delivery Using Transit Networks  arXiv:1909.11840v1 [cs.RO] 26 Sep 2019  Shushman Choudhury, Kiril Solovey, Mykel J. Kochenderfer, and Marco Pavone    I. I NTRODUCTION Rapidly growing e-commerce demands have greatly strained dense urban communities by increasing delivery truck traffic and slowing operations, and impacting travel times for public and private vehicles [1, 2]. Further congestion is being induced by newer services (e.g., Instacart and Uber Eats) relying on ride-sharing vehicles. There is a clear need to redesign the current method of package distribution in cities [3]. The agility and aerial reach of drones, the flexibility and ease of establishing drone networks, and recent advances in drone capabilities make them highly promising for modern logistics networks [4]. However, drones have limited travel range and carrying capacity [5, 6]. On the other hand, ground-based transit networks have less flexibility but far greater coverage and throughput. By combining the strengths of both, we can achieve significant commercial benefits and social impact (reducing ground congestion; delivering medicine and essentials). We address the problem of operating a large number of drones to deliver multiple packages simultaneously in an area. The drones can use one or more vehicles in a publictransit network as modes of transportation, thereby saving their limited battery energy stored on-board and increasing their effective travel range. We are required to decide which deliveries each drone should make and in what order, which modes of transit to use, and for what duration. Our approach must contend with the multiple significant challenges of our problem. It must plan over large timedependent transit networks, while accounting for energy The authors are with Stanford University, CA, USA: S.C. is with the Department of Computer Science and K.S., M.K., and M.P. are with the Department of Aeronautics and Astronautics.  DELIVERY  DEPOT  TRANSIT RIDE TRANSFER FLIGHT  constraints that limit the drones’ flight ranges. It must avoid inter-drone conflicts, such as where more than one drone attempts to board the same vehicle at the same time, or when the maximum carrying capacity of a vehicle is exceeded. We seek not just feasible multi-agent plans but high-quality solutions in terms of a cumulative objective over all drones, the makespan, i.e., the maximum individual delivery time for any drone. Additionally, our approach must also solve the task allocation problem of determining which drones deliver which packages, and from which distribution centers. A. Related work Some individual aspects of our problem have been studied. The single-agent setting of controlling an autonomous drone to use multiple modes of transit en route to its destination was investigated in [7]. Recent work has considered pairing a drone with a delivery truck, which does not exploit public transit [8–10]. The multi-agent issues of task allocation and inter-agent conflicts also not addressed either. Our problem is closely related to routing a fleet of autonomous vehicles providing mobility-on-demand services [11–13]. Specifically, the task is to compute routes for the vehicles (both customer-carrying and empty) so that travel demand is fulfilled and operational cost is minimized. In particular, recent works [14, 15] study the combination of such service with public transit, where passengers can use several modes of transportation in the same trip. However, such works abstract away inter-agent constraints or dynamics and are not suited for autonomous pathfinding. The taskallocation setting we consider in our problem can be viewed as an instance of the vehicle routing problem [16–18], variants of which are typically solved by mixed integer linear programming (MILP) formulations that scale poorly, or by heuristics that do not provide optimality guarantees. We must contend with the challenges of planning for multiple agents. Accordingly, the second layer of our approach is a multi-agent path finding (MAPF) problem [19, 20]. Since the drones are on the same team, we have a centralized or cooperative pathfinding setting [21]. The MAPF  problem is NP-hard to solve optimally [22]. A number of efficient solvers have been developed that work well in practice [23]. The MAPF formulation and algorithms have been extended to several relevant scenarios such as lifelong pickup-and-delivery [24] and joint task assignment and pathfinding [25, 26], though for different task settings and constraints than ours. Also, a MAPF formulation was applied for UAV traffic management in cities [27]. However, none of the approaches considered pathfinding over large time-dependent transit networks. We use models, algorithms and techniques from transportation planning [28–30]. B. Statement of contributions We present a comprehensive algorithmic framework for large-scale multi-drone delivery in synergy with a ground transit network. Our approach strives to minimize the maximum time to complete any delivery. We decompose the highly challenging problem and solve it stage-wise with a two-layer approach. First, the upper layer assigns drones to package-delivery sequences with a task allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network. Algorithmically, we develop a new delivery sequence allocation method for the upper layer that obtains a provably near-optimal solution in polynomial runtime. For the lower layer, we extend techniques for multi-agent path finding that account for time-dependent public transit networks and agent energy constraints to perform multi-drone routing. Experimentally, we present extensive results supporting the efficiency of our approach on settings with up to 200 drones, 5000 packages, and transit networks of up to 8000 stops in San Francisco and the Washington DC area. We demonstrate that our framework can compute solutions within a few seconds (up to 2 minutes for the largest settings) on our commodity hardware, and that in our problem scenarios, drones travel up to 450% of their flight range by using transit. The following is the paper structure. We present an overall description of the two-layer approach in Section II, and then elaborate on each layer in Sections III and IV. We present experimental results on simulations in Section V, and conclude the paper with Section VI. We will also refer to the appendices, which have additional details, illustrations, results, and discussions. II. M ETHODOLOGY We provide a high-level description of our formulation and approach to illustrate the various interacting components. A. Problem Formulation We are operating a centralized homogeneous fleet of m drones within a city-scale domain. There are ` product depots with known geographic locations, denoted by VD := {d1 , . . . , d` } ⊂ R2 . The depots are both product dispatch centers and drone-charging stations. At the start of a large time interval (e.g., a day), a batch of delivery requests for k different packages, denoted VP := {p1 , . . . , pk } ⊂ R2 , is received (we assume that k  m). Each pi has a corresponding location. We assume that any package can be dispatched from  any depot; our framework exploits this property to optimize the solution quality in terms of makespan, i.e., the maximum execution time for any delivery. In Section III, we mention how our approach can accommodate dispatch constraints. The drones carry packages from depots to delivery locations. They can extend their effective travel range by using public-transit vehicles operating in the area (e.g., a public bus network), which remain unaffected by the drones’ actions. Our problem is to route drones to deliver all packages while minimizing makespan. A drone route consists of its current location and the sequence of depot and package locations to visit with a combination of flying and riding on transit. A central constraint is the drones’ limited energy, which we characterize as a maximum flight distance. A feasible solution must satisfy inter-drone constraints such as collision avoidance and transit vehicle capacity limits. Finally, we make some assumptions for our setting: a drone carries one package at a time, which is reasonable given state-of-the-art drone payloads [6]; drones are recharged upon visiting a depot in negligible time (e.g., a battery replacement); depots have unlimited drone capacity; the transit network is deterministic with respect to locations and vehicle travel times (we mention uncertainty in Section VI). We do account for the time-varying nature of the transit. B. Approach overview In principle, we could frame the entire problem as a mixed integer linear program (MILP). However, for realworld problems (hundreds of drones; thousands of packages; large transit networks), even state-of-the-art MILP approaches are unlikely to scale. Moreover, even a simpler problem that ignores the interaction constraints is an instance of the (notoriously challenging) multi-depot vehicle routing problem [17]. Thus, we decouple the problem into two distinct subproblems that we solve stage-wise in layers. The upper layer performs task allocation to determine which packages are delivered by which drone and in what order. It takes as input the known depot and package locations, and an estimate of the drone travel time between every pair of locations. It then solves a threefold allocation to minimize delivery makespan and assigns to each package (i) the dispatch depot and (ii) the delivery drone, and to each drone (iii) the order of package deliveries. To this end, we develop an efficient polynomial-time task-allocation algorithm that achieves a near-optimal makespan. The lower layer performs route planning for the drone fleet to execute the allocated delivery tasks. It generates detailed routes of drone locations in time and space and the transit vehicles used, while accounting for the timevarying transit network. It also ensures that (i) simultaneous transit boarding by multiple drones is avoided, (ii) no transit vehicle exceeds its drone-carrying capacity, and (iii) drone (battery) energy constraints are respected. We efficiently handle individual and inter-drone constraints by framing the routing problem as an extension of multi-agent path finding (MAPF) to transit networks. We adapt a scalable, bounded sub-optimal variant of a highly effective MAPF solver called Conflict-Based Search (CBS) [31] to solve the one-deliveryper-drone problem. Finally, we obtain routes for the sequence  of deliveries in a receding-horizon fashion by replanning for the next task once a drone completes its current one. Decomposition-based stage-wise optimization approaches typically have an approximation gap compared to solutions of the full problem. In our case, this gap manifests in the surrogate cost estimate we use for the drone’s travel time between locations in the task-allocation layer (instead of jointly solving for allocation and multi-agent routing over transit networks, which is not feasible at scale). The better the surrogate, the more coupled the layers are, i.e., the better is the solution of the first stage for the second one. Such surrogates have a tradeoff between efficiency and approximation quality. An easy-to-compute travel time surrogate, for instance, is the drone’s direct flight time between two locations (ignoring the transit network). However, that can be of very poor quality when the drone requires transit to reach an outof-range target. We use a surrogate that actually accounts for the transit network, at the expense of some modest preprocessing. We defer details to Appendix III, but the basic idea is to precompute the pairwise shortest travel times between locations spread around the city, over a representative snapshot of the transit network.  Algorithm 1: M ERGE S PLIT T OURS(GA ) Solve MCT(GA ) to get t tours T := {T1 , . . . , Tt }; while |T| > 1 do Pick distinct tours T, T 0 ∈ T and depots d ∈ T, d0 ∈ T 0 that minimize cdd0 + cd0 d ; Merge T, T 0 by adding (d, d0 ), (d0 , d) edges ; Split final tour T into m sub-paths P1 , . . . , Pm , where LENGTH (Pi ) is proportional to LENGTH (T )/m for each i (similar to [34]); Extend each Pi to ensure it begins and ends at a depot; return P1 , . . . , Pm ; TABLE I: An integer programming formulation of the MCT problem. · VP , Given allocation graph GA = (VA , EA ), with VA = VD ∪ X minimize xuv · cuv subject to xuv ∈ {0, 1},  We now discuss how we leverage our problem’s structure to design a new algorithm called M ERGE S PLIT T OURS for the task-allocation layer, which guarantees a near-optimal solution in polynomial time. The goal of this layer is to (i) distribute the set of packages VP among m agents, (ii) assign each package destination p ∈ VP to a depot d ∈ VD , and (iii) assign drones to a sequence of depot pickups and package deliveries. The objective is to minimize the maximum travel time among all agents over all three of the above components. Our problem can be cast as a special version of the m traveling salesman problem [32], which we call the m minimal visiting paths problem (m-MVP). We seek a set of m paths such that the makespan, i.e., the maximum travel time for any path, is minimized. We only need paths that start and end at (the same or different) depots, not tours. Our formulation is the asymmetric variant, for a directed underlying graph, which is NP-hard even for m = 1. Moreover, the current best polynomial-time approximation [33] yields the fairly large approximation factor O(log n/ log log n), for a graph with n vertices. An additional challenge is the inability to assume the triangle inequality on our objective of travel times. A key element of m-MVP is the allocation graph GA = (VA , EA ), with vertex set VA = VD ∪ VP . Each directed edge (u, v) ∈ EA is weighted according to an estimated travel time cuv from the location of u to that of v in the city. As we flagged in Section II-B, any dispatch constraints are modeled by excluding edges from the corresponding depot. We are now ready for the full definition of m-MVP: Definition 1. Given the allocation graph GA , the m minimal visiting paths problem (m-MVP) consists of finding m-paths ∗ P1∗ , . . . , Pm on GA , such that (1) each path Pi∗ starts at some depot d ∈ VD and terminates at the same or different  ∀(u, v) ∈ EA , u ∈ VP ∨ u ∈ VP , 0  0  xuv ∈ N>0 , ∀(d, d ) ∈ EA , d, d ∈ VD , X X xpd = 1, ∀p ∈ VP , xdp =  (2) (3) (4)  d∈N− (p)  d∈N+ (p)  X  III. TASK A SSIGNMENT AND PACKAGE A LLOCATION  (1)  (uv)∈EA  xvd −  v∈N+ (d)  X  xdv = 0,  ∀d ∈ VD .  (5)  v∈N− (d)  where N+ (v), N− (v) denote the in and out going neighbors of v ∈ VA .  depot d0 ∈ VD , (2) exactly one path visits each package p ∈ VP , and (3) the maximum travel time of any of the paths is minimized. Let OPT be the optimal makespan, i.e., OPT := maxi∈[m] LENGTH(Pi∗ ), where LENGTH(·) denotes the total travel time along a given path or tour. We make three observations. First, if a path contains the sub-path (d, p), (p, d0 ), for some d, d0 ∈ VD , p ∈ VP , then p should be dispatched from depot d and the drone delivering p will return to d0 after delivery. Second, a package p being found in Pi∗ indicates that drone i ∈ [m] should deliver it. Third, Pi∗ fully characterizes the order of packages delivered by drone i. A. Algorithm Overview We present our M ERGE S PLIT T OURS algorithm for solving m-MVP (Algorithm 1); see a detailed description in Appendix I. A key step is generating an initial set of tours T by solving the minimal-connecting tours (MCT) problem (see Table I), which attempts to connect packages to depots within tours to minimize the total edge weight in eq. (1). The constraint in eq. (4) is that each package is connected to precisely one incoming and one outgoing edge from and to depots respectively. The final constraint in eq. (5) enforces inflow and outflow equality for every depot. Edges connecting packages can be used at most once, whereas edges connecting depots can be used multiple times. The solution to MCT is the assignment {xuv }(u,v)∈E , i.e., which edges of GA are used and how many times. This assignment implicitly represents the desired collection of aforementioned tours T1 , . . . , Tt ; see Appendix I.  CAPACITY CONFLICT CONFLICT RESOLVED  BOARDING CONFLICT CAPACITY =2  CAPACITY =2  CONFLICT RESOLVED  (a)  CAPACITY =1  (b)  (c)  CAPACITY =1  (d)  Fig. 1: In our formulation of multi-agent path finding with transit networks, conflicts arise from the violation of shared inter-drone constraints: (a) boarding conflicts between two or more drones and (c) capacity conflicts between more drones than the transit vehicle can accommodate. The modified paths after resolving the corresponding conflicts are depicted in (b) and (d), respectively.  B. Theoretical Guarantees All proofs from this secion are in Appendix I. The following theorem states that M ERGE S PLIT T OURS is correct and that its makespan is close to optimal. Theorem 1. Suppose GA is strongly connected and the subgraph GA (VD ) induced by the vertices VD is a directed clique. Let P1 , . . . , Pm be the output of M ERGE S PLITT OURS. Then, every package p ∈ VP is contained in exactly one path Pi , and every Pj starts and ends at a depot. Moreover, it holds that maxi∈[m] LENGTH(Pi ) 6 OPT + α + β, where α := max cdd0 +cd0 d , β := 0 d,d ∈VD  max  d,d0 ∈VD ,p∈VP  cdp +cpd0 .  The key idea is that the total cost of the tours induced by the solution to MCT cannot exceed the total length of ∗ }. The MCT solution is then adapted to m {P1∗ , . . . , Pm paths with an additional overhead of α + β per path. When m  |VP | (typically the case), α, β are small compared to OPT , making the bound tight. For instance, in our randomlygenerated scenarios in Section V-A, for m = 5, k = 200, the approximation ratio maxi∈[m] LENGTH(Pi )/OPT = 1.09, and for m = 10, k = 500, the factor is 1.06. The main computational bottleneck of the algorithm is MCT, while the other components can clearly be implemented polynomially in the input size. However, it suffices to solve a relaxed version of MCT to obtain the same integral solution. Lemma 1. The optimal solution to the fractional relaxation of MCT, in which xuv ∈ [0, 1] for all u ∈ VP ∨ v ∈ VP , and xuv ∈ R+ otherwise, yields the integer optimal solution. The lemma follows from casting MCT as the minimumcost circulation problem, for which the constraint matrix is totally unimodular [35]. Therefore, M ERGE S PLIT T OURS can be implemented in polynomial time. IV. M ULTI -AGENT PATH F INDING For each drone i ∈ [m], the allocation layer yields a sequence of delivery tasks d1 p1 . . . pl dl . Each delivery sequence has one or more subsequences of dpd0 . The routeplanning layer treats each dpd0 subsequence as an individual drone task, i.e., leaving with the package from depot d, carrying it to package location p and returning to the (same or different) depot d0 , without exceeding the energy capacity. We seek an efficient and scalable method to obtain highquality (with respect to travel time) feasible paths, while using transit options to extend range, for m different drone  dpd0 tasks simultaneously. The full set of delivery sequences can be satisfied by replanning when a drone finishes its current task and begins a new one; we discuss and compare two replanning strategies in Appendix IV. Thus, we formulate the problem of multi-drone routing to satisfy a set of delivery sequences as receding-horizon multi-agent path finding (MAPF) over transit networks. In this section, we describe the graph representation of our problem and present an efficient bounded sub-optimal algorithm. A. MAPF with Transit Networks (MAPF-TN) The problem of Multi-Agent Path Finding with Transit Networks (MAPF-TN) is the extension of standard MAPF to where agents can use one or more modes of transit in addition to moving. The incorporation of transit networks introduces additional challenges and underlying structure. The input to MAPF-TN is the set of m tasks (di , pi , d0i )i=1:m and the directed operation graph GO = (VO , EO ). In Section III, the allocation graph GA only considered depots and packages, and edges between S them. Here, GO also includes transit vertices, VT N = τ ∈T Rτ , where T is the set of trips, and each trip Rτ = {(s1 , t1 ) . . .} is a sequence of time-stamped stop locations (a given stop location may appear as several different nodes with distinct time-stamps). Similarly, we also use time-expanded [28] versions of VD and VP . The edges are defined as follows: An edge e = (u → v) ∈ E is a transit edge if u, v ∈ VT N and are consecutive stops on the same trip Rt . Any other edge is a flight edge. An edge is time-constrained if v ∈ VT N and time-unconstrained otherwise. Every edge has three attributes: traversal time T , energy expended N , and capacity C. Since each vertex is associated with a location, kv − uk denotes the distance between them for a suitable metric. MAPF typically abstracts away agent dynamics; we have a simple model where drones move at constant speed σ, and distance flown represents energy expended. Due to high graph density (drones can fly point-to-point between many stops), we do not explicitly enumerate edges but generate them on-the-fly during search. We now define the three attributes for EO . For timeconstrained edges, T (e) = v.t−u.t is the difference between corresponding time-stamps (if u ∈ VD ∪ VP , u.t is the chosen departure time), and for time-unconstrained edges, T (e) = kv − uk/σ is the time of direct flight. For flight edges, N (e) = kv−uk (flight distance), and for transit edges, N (e) = 0. For transit edges, C(e) is bounded by the capacity  of the vehicle, while for flight edges, C(e) = ∞. Here, we assume that time-unconstrained flight in open space can be accommodated (thorougly examined in [27]). We now describe the remaining relevant details of the MAPF-TN problem. An individual path πi for drone i from 0 d i through pi to di is feasible if the energy constraint P N (e) 6 N̄ is satisfied, where N̄ is the drone’s e∈πi maximum flight distance. In addition, the drone should be able to traverse the distance of a time-constrained flight edge in time, i.e., σ × (v.t − u.t) > kv − uk. For simplicity, we abstract away energy expenditure due to hovering in place by flying the drone at reduced speed to reach the transit just in time. Thus, the constraint N̄ is only on the traversed distance. The cost of P an individual path is the total traversal time, T (π ) = i e∈πi T (e). A feasible S solution Π = i=1:m πi is a set of m individually feasible paths that does not violate any of the following two shared constraints (see Figure 1): (i) Boarding constraint, i.e., no two drones may board the same vehicle at the same stop; (ii) Capacity constraint, i.e., a transit edge e may not be used by more than C(e) drones. As with the allocation layer, the global objective for MAPF-TN is to minimize the solution makespan, argminΠ maxπ∈Π T (π), i.e., minimize the worst individual completion time. B. Conflict-Based Search for MAPF-TN To tackle MAPF-TN, we modify the Conflict-Based Search (CBS) algorithm [31]. The multi-agent level of CBS identifies shared constraints and imposes corresponding path constraints on the single-agent level. The single-agent level computes optimal individual paths that respect all constraints. If individual paths conflict (i.e., violate a shared constraint), the multi-agent level adds further constraints to resolve the conflict, and invokes the single-agent level again, for the conflicting agents. In MAPF-TN, conflicts arise from boarding and capacity constraints. CBS obtains optimal multi-agent solutions without having to run (potentially significantly expensive) multi-agent searches. However, its performance can degrade heavily with many conflicts in which constraints are violated. Figure 1 illustrates the generation and resolution of conflicts in our MAPF-TN problem. For scalability, we use a bounded sub-optimal variant of CBS called Enhanced CBS (ECBS), which achieves orders of magnitude speedups over CBS [36]. ECBS uses bounded sub-optimal Focal Search [37] at both levels, instead of bestfirst A* [38]. Focal search allows using an inadmissible heuristic that prioritizes efficiency. We now describe a crucial modification to ECBS required for MAPF-TN. Focal Weight-constrained Search: Unlike typical MAPF, the low-level graph search in MAPF-TN has a path-wide constraint (traversal distance) in addition to the objective function of traversal time. For the shortest path problems on graphs, adding a path-wide constraint makes it NP-hard [39]. Several algorithms for constrained search [40, 41] require an explicit enumeration of the edges. We extend the A* for MultiConstraint Shortest Path (A*-MCSP) algorithm [42] (suitable for our implicit graph) to focal search (called FocalMCSP). Focal-MCSP uses admissible heuristics on both objective and constraint and maintains only non-dominated  TABLE II: The mean computation time for M ERGE S PLIT T OURS in seconds, over 100 different trials for each setting. M ERGE S PLIT T OURS is polynomial in input size and highly scalable. Here, k = |VP | is the number of package deliveries and ` = |VD | is the number of depots. The Out-of-Memory cases are due to the linear programming step of MCT and could be resolved in practice with a larger machine or a distributed implementation. k  `=2  `=5  ` = 10  ` = 20  ` = 30  50 100 200 500 1000 5000  0.006 0.016 0.053 0.311 1.483 38.05  0.022 0.070 0.272 1.731 6.811 OutOfMem  0.074 0.268 1.171 6.532 31.08 OutOfMem  0.326 1.274 4.323 31.15 OutOfMem OutOfMem  0.981 2.823 11.09 83.31 OutOfMem OutOfMem  paths to intermediate nodes. This extensive book-keeping requires a careful implementation for efficiency. Focal-MCSP inherits the properties of A*-MCSP and Focal Search; therefore, it yields a bounded-suboptimal feasible path to the target node. Accordingly, ECBS with FocalMCSP yields a bounded sub-optimal solution to MAPFTN. The result follows from the analysis of ECBS [36]. Also, note that a dpd0 path requires a bounded sub-optimal path from d to p and another from p to d0 , such that their concatenation is feasible. Since this is even more complicated, in practice, we run Focal-MCSP twice (from d to p and p to d0 ) with half the energy constraint each time and concatenate the two paths, thus guaranteeing feasibility. In Appendix II-B we discuss other required modifications to standard MAPF and important speedup techniques that nonetheless retain the bounded sub-optimality of Enhanced CBS for our MAPF-TN formulation. V. E XPERIMENTS AND R ESULTS We implemented our approach using the Julia language and tested it on a machine with a 6-core 3.7 GHz 16 GiB RAM CPU 1 . For very large combinatorial optimization problems, solution quality and algorithm efficiency are of interest. We have already shown that the upper and lower layers are near-optimal and bounded-suboptimal respectively in terms of solution quality, i.e., makespan. Therefore, for evaluation we focus on their efficiency and scalability to large real-world settings. We do not attempt to baseline against a MILP approach for the full joint problem; we estimate that a typical setting of interest to us will have O(107 ) variables in a MILP formulation, besides exponential constraints. We ran simulations with two large-scale public transit networks in San Francisco (SFMTA) and the Washington Metropolitan Area (WMATA). We used the open-source General Transit Feed Specification data [43] for each network. We considered only the bus network (by far the most extensive), but our formulation can accommodate multiple modes. We defined a geographical bounding box in each case, of area 150 km2 for SFMTA and 400 km2 for WMATA (illustrated in Appendix IV), within which depots and package locations were randomly generated. For the transit network, we considered all bus trips that operate within the bounding box. The size of the time-expanded network, |VT N |, is the total number of stops made by all 1 The code for our work is available at https://github.com/sisl/ MultiAgentAllocationTransit.jl  TABLE III: (All times are in seconds) An extensive analysis of the MAPF-TN layer, on 20 trials for each {`, m} setting, with different randomly generated depots and delivery locations for each trial. The integer carrying capacity of any transit edge C(e) was randomly chosen from {3, 4, 5} (single and double-buses). The sub-optimality factor for ECBS was 1.1. For settings with m/` = 10, a number of trials timed out (over 180 s) and were discarded. San Francisco |VT N | = 4192 ; Area 150 km2 {Depots, Agents} {`, m} {5, 10} {5, 20} {5, 50} {10, 20} {10, 50} {10, 100} {20, 50} {20, 100} {20, 200}  Washington DC |VT N | = 7608; Area 400 km2      {Median, Avg} Plan Time  {Avg, Max} Range Ext.  {Avg, Max} Transit Used  Avg Soln. Makespan  {Median, Avg} Plan Time  {Avg, Max} Range Ext.  {Avg, Max} Transit Used  Avg Soln. Makespan  {0.53, 1.07} {1.11, 2.79} {3.18, 8.29} {0.64, 0.68} {1.73, 2.96} {2.09, 4.17} {0.19, 0.26} {0.41, 0.66} {0.73, 1.70}  {1.5, 3.0} {1.6, 2.5} {1.7, 2.4} {1.2, 1.8} {1.6, 3.6} {1.4, 1.8} {0.9, 1.2} {1.1, 1.3} {1.1, 2.3}  {2.9, 6} {3.1, 6} {4.5, 6} {2.3, 4} {3.2, 5} {3.7, 7} {0.8, 4} {1.4, 4} {2.2, 5}  2464.8 2555.5 3485.8 2082.7 2671.1 3084.5 1054.8 1457.6 1824.4  {4.73, 7.17} {11.5, 12.4} {50.5, 57.6} {7.31, 8.78} {15.6, 19.8} {35.9, 39.7} {8.14, 11.2} {17.5, 19.2} {22.9, 26.2}  {2.0, 3.5} {2.3, 3.8} {2.9, 3.6} {2.1, 3.2} {2.5, 4.5} {2.5, 3.7} {1.9, 4.1} {2.2, 3.7} {2.2, 2.9}  {3.3, 8} {4.5, 7} {5.1, 7} {3.7, 7} {3.9, 6} {4.9, 8} {3.6, 7} {4.1, 6} {4.4, 6}  5006.5 5819.2 6861.7 5195.6 6316.7 6889.8 5086.9 5400.9 6050.1  trips; |VT N | = 4192 for SFMTA and |VT N | = 7608 for WMATA (recall that edges are implicit, so |ET N | varies between queries, but the full graph GO can be dense). The drone’s flight range constraint is set (conservatively) to 7 km and average speed to 25 kph, based on the DJI Mavic 2 specifications [6]. In this section, we evaluate the performance of the two primary components — the task allocation and multi-agent path finding layers. In Appendix IV we compare two replanning strategies for when a drone finishes its current delivery, and compare two surrogate travel time estimates for layer coupling. A. Task Allocation The scale of the allocation problem is determined by the number of depots and packages, i.e., ` + k. The runtimes for M ERGE S PLIT T OURS with varying `, k over SFMTA are displayed in Table II. The roughly-quadratic increase in runtimes along a specific row or column demonstrate that our provably near-optimal M ERGE S PLIT T OURS algorithm is indeed polynomial in the size of the input. Even for up to 5000 deliveries, the absolute runtimes are quite reasonable. We do not compare with naive MILP even for allocation, as the number of variables would exceed ` · k, in addition to the expensive subtour elimination constraints [44]. B. MAPF with Transit Networks (MAPF-TN) Solving multi-agent path finding optimally is NPhard [22]. Previous research has benchmarked CBS variants and shown that Enhanced CBS is most effective [36, 45]. Therefore, we focus on extensively evaluating our own approach rather than redundant baselining. Table III quantifies several aspects of the MAPF-TN layer with varying numbers of depots (`) and agents (m), the two most tunable parameters. Before each trial, we run the allocation layer and collect m dpd0 tasks, one for each agent. We then run the MAPF-TN solver on this set of tasks to compute a solution. We discuss broad observations here and provide a detailed analysis in Appendix IV. The results are very promising; our approach scales to large numbers of agents (200) and large transit networks (nearly 8000 vertices); the highest average makespan for the true delivery time is less than an hour (3485.8 s) for SFMTA and 2 hours (6889.8 s) for WMATA; drones are using up to 8 transit options per route to extend their range by up to 4.5x. As we anticipated, conflict resolution is a major bottleneck of MAPF-TN. A higher ratio  of agents to depots increases conflicts due to shared transit, thereby increasing plan time (compare {5, 20} to {10, 20}). A higher number of depots puts more deliveries within flight range of a depot, reducing conflicts, makespan, and the need for transit usage and range extension (compare {10, 50} to {20, 50}). Plan times are much higher for WMATA due to a larger area and a larger and less uniformly distributed bus network, leading to higher single-agent search times and more multi-agent conflicts. Trial"
"Mathematical Reasoning in Latent Space  arXiv:1909.11851v1 [cs.LG] 26 Sep 2019  Dennis Lee, Christian Szegedy, Markus N. Rabe, Sarah M. Loos and Kshitij Bansal Google Research Mountain View, CA, USA {ldennis,szegedy,mrabe,smloos,kbk}@google.com September 2019   Introduction  Automated reasoning has long been considered to require development of logics and “hard” algorithms, such as backtracking search. Recently, approaches that employ deep learning have also been applied, but these have focused on predicting the next step of a proof, which is again executed with a hard algorithm [Loos et al., 2017, Gauthier et al., 2017, Lederman et al., 2018, Bansal et al., 2019b]. We raise the question whether hard algorithms could be omitted from this process and mathematical reasoning performed entirely in the latent space. To this end, we investigate whether we can predict useful latent representations of the mathematical formulas that result from proof steps. Ideally, we could rely entirely on predicted latent representations to sketch out proofs and only go back to the concrete mathematical formulas to check if our intuitive reasoning was correct. This would allow for more flexible and robust system designs for automated reasoning. In this work, we present a first experiment indicating directly that theorem proving in the latent space might be possible. We build on HOList, an environment and benchmark for automated theorem provers based on deep learning [Bansal et al., 2019b] which is makes use of the interactive theorem prover HOL Light [Harrison, 1996], an interactive proof assistant. The HOList theorem database comprises of over 19 thousand theorems and lemmas from a variety of mathematical domains, including topology, multivariate calculus, real and complex analysis, geometric algebra, and measure theory. Concrete examples include basic properties of real and complex numbers such as (x−1 = y) ⇔ (x = y −1 ), and also well-known theorems, such as Pythagoras’ theorem, Skolem’s theorem, the fundamental theorem of calculus, Abel’s theorem for complex power series and that the eigenvalues of a complex matrix are the roots of its characteristic polynomial. We focus on rewrite rules (or rewrites in short). Rewrites are only one of several proof tactics in HOL Light, but they enable powerful transformations on mathematical formulas, as they can be given arbitrary theorems  1  as parameters. For example, the formula 32 = z can be rewritten to 3 · 3 = z by performing a rewrite with the parameter x2 = x · x. Alternatively, a rewrite may diverge (as it operates recursively) or it may return the same formula – in both these cases we consider the rewrite to fail. For instance, in the example above, the rewrite would fail if we used equation x + y = y + x as a rewrite parameter instead, since the expression 32 = z does not contain any + operators to match with. In our experiments, we first train a neural network to map mathematical formulas into a latent space of fixed dimension. This network is trained by predicting – based on the latent representation being trained – whether a given rewrite is going to succeed (i.e. returns with a new formula). For successful rewrites we also predict the latent representation of the resulting formula. To evaluate the feasibility of reasoning in latent space over two steps, we first predict the latent representation of the result of a rewrite, then we evaluate whether the predicted latent representation still allows for accurate predictions of the rewrite success of the resulting formula. For multi-step reasoning beyond two steps, we predict the future latent representations based on the previous latent representation only - without seeing the intermediate formula. Our experiments suggest that even after 4 steps of reasoning purely in latent space, neural networks show non-trivial reasoning capabilities, despite not being trained on this task directly.  2  Related Work  Our work is motivated by deep learning based automated theorem proving, but is also closely connected to model based reinforcement learning and approaches that learn to predict the future as part of reinforcement learning. Model based reinforcement learning is concerned with creating a model of the environment while maximizing the expected reward (e.g. [Ha and Schmidhuber, 2018]). Already early works have shown that predicting the latent representations of reinforcement learning environments with deep learning is sometimes feasible - even over many steps [Oh et al., 2015, Chiappa et al., 2017]. This can enable faster training, since it can preempt the need for performing expensive simulations of the environment. Predicting latent representation was also proposed in [Brunner et al., 2018] as a regularization method for reinforcement learning. One recent successful example of model based reinforcement learning is [Kaiser et al., 2019], where the system learns to predict the pixel-wise output of the Atari machine. However this approach is based on actually simulating the environment directly in the “pixel space” as opposed to performing predictions in a low dimensional semantic embedding space. More related to our work is [Piotrowski et al., 2019], which attempts to learn to rewrite simple formulas. The goal is there again is to predict the actual outcome of the rewrite rather than a latent representation of it. In [Dosovitskiy and Koltun, 2017], they predict “expected measurements” as an extra supervision in addition to the reward signal. Graph neural networks have been used for premise selection in higher order logic [Wang et al., 2017] and more recently by [Paliwal et al., 2019] as the core component of DeepHOL, a neural theorem prover for higher order logic. In this work, we build upon their neural network architecture, but utilize it for a different task.  3  HOL Light  HOL Light [Harrison, 1996] is an interactive proof assistant (or interactive theorem prover) for higher-order logic reasoning. Traditionally, proof assistants have been used by human users for creating formalized proofs of mathematical statements manually. Although they come with limited forms of automation, it is still a cumbersome process to formalize proofs, even when it is already available in natural language. Some large scale formalization efforts were conducted successfully in HOL Light and Coq [Coq, ], for example the formal proofs of the Kepler conjecture [Hales et al., 2017] and that of the four color theorem [Gonthier, 2008]. They required significant meticulous manual work and expert knowledge of the system. Lately, there have been several attempts to improve the automation of the proof assistants significantly by so called “hammers” [Kaliszyk and Urban, 2015]. Still, traditional proof automation lacks the mathemat-  2  ical intuition of human mathematicians who can perform complicated intuitive arguments. The quest for modelling and automating fuzzy, “human style” reasoning is one of the main motivation for this work.  3.1  Rewrite Tactic in HOL Light  The HOL Light system allows the user to specify a goal to prove, and then offers a number of tactics to apply to the goal. A tactic application consumes the goal and returns a list of subgoals. Proving all of the subgoals is equivalent to proving the goal itself. Accordingly, if a tactic application returns the empty list of subgoals, the parent goal is proven. In this work, we focus on the rewrite tactic (REWRITE TAC) of HOL Light, which is a particularly common and versatile tactic. It takes a list of theorems as parameters (though in this work we only consider applications of rewrite with a single parameter). Parameters must be an equation or a conjunction of equations; possibly guarded by a condition. Given a goal statement T and parameter Pi , the rewrite tactic searches for subexpressions in T that match the left side of one of the equations in Pi and replaces it with the right side of the equation. The matching process takes care of variable names and types, such that minor differences can be bridged. The rewrite procedure is recursive, and hence tries to rewrite the formula until no opportunities for rewrites are left. The rewrite tactic also has a set of built-in rewrite rules, representing trivial simplifications, such as FST(x, y) = x. Note that REWRITE TAC uses “big step semantics”, meaning that the application of each individual operation can perform multiple elementary rewrite steps recursively. For more details on REWRITE TAC, refer to the manual [HOL Light Rewrite Tactic Reference, ].  4  Reasoning in Latent Space  We embed higher-order logic statements into a fixed dimensional embedding space by applying a graph neural network to a suitably chosen graph representation of the corresponding formula. The embedding is trained on predicting the outcome (success or failure) of a large number of possible formula rewrite operations. Note that formulas can be quite complex as they are arbitrary typed lambda expressions in higher order logic. For technical reasons, we will distinguish between two latent embedding spaces L = Rk and L0 = Rk (k = 1024) corresponding to two distinct embeddings for each formula, learned by two different networks. We have trained three different models. S denotes the set of syntactically correct higher-order logic formulas in HOL Light. 1. Rewrite success prediction σ : S × S −→ [0, 1], 2. Rewrite outcome prediction ω : S × S −→ [0, 1] × L, 3. Embedding alignment prediction α : L −→ L0 . These networks and their purposes are described in detail in later subsections. Alternatively, we could use a single fixed embedding space with a single model predicting its own future embedding on the rewritten statement. That network could be trained end-to-end and reach better performance without the need of aligning the embedding spaces, removing the need for α. Here, we opted for a more controlled setup that relies on a fixed embedding network σ, trained for the sole task of predicting whether rewriting statement T by P is successful. This way we can rely on a fixed embedding method and run more detailed ablation analyses. Merging σ and ω, is left for future work.  4.1  Training Data  We start with the theorem database of the HOList environment [Bansal et al., 2019b], which contains 19591 theorems in its theorem database, approximately ordered by increasing complexity. This is split into 11655 training, 3668 validation, and 3620 testing theorems. To generate our training data, we generate all pairs T, P of theorems from the training set, where P must occur before T in the database (to avoid circular  3  dependencies). We then interpret theorem T as a goal to prove and try to rewrite T with P using the REWRITE TAC of HOL Light. This can result in three different outcomes: 1. T is rewritten by theorem P successfully, and the result differs from T . 2. The rewrite operation terminates, but failed to change the input theorem T . 3. The rewrite operation times out or diverges (becomes too big). In our experiments, we consider only the first outcome as successful, i.e. when the application finishes within the specified time limit and changes the target, as a successful rewrite attempt. Each training example, therefore, consists of the pair (T, P ), the success/fail-bit of the rewrite (1 for successful rewrites, 0 for failed rewrites), and, for successful tactic applications, the new formula that results from the rewrite, which we denote with R(T, P ).  4.2  Base Model Architecture and Training Methodology  The rewrite success prediction model σ(T, P ) is trained on the training set of theorems in the HOList benchmark. The training task is to predict the success or failure of the REWRITE TAC application. We used a two-tower network (without weight sharing) with embedding towers γ : S −→ L and π, one for each of the two formulas T and P . Both towers are graph neural networks as described in [Paliwal et al., 2019]. Both of them embed the supplied formula in a fixed dimensional embedding space Rk . The concatenated embeddings are then processed by a three-layer perceptron c with rectified linear activation, which is followed by p, a single output linear function predicting the logit and is trained by logistic regression on the success/fail-bit of the rewrite. Formally: σ(T, P ) = p(c(γ(T ), π(P ))).  4.3  σ  ω p  p'  e'  c  c'  α π  γ  γ'  T  π'  P  Figure 1: Depiction of network ar-  Outcome Prediction Model  chitecture  In addition to our base model, we train ω(T, P ). This model has an identical two-tower embedding architecture as σ, but with a larger combiner network and an extra prediction layer e0 to predict embedding vector of the outcome of the rewritten formulas. Here the embedding towers are denoted by γ 0 and π 0 , the combiner network is c0 and the two linear prediction layers are p0 and e0 . That is: ω(T, P ) = (p0 (c0 (γ 0 (T ), π 0 (P ))), e0 (c0 (γ 0 (T ), π 0 (P ))). This model predicts both the success or failure of applying REWRITE TAC and for successful rewrites, the latent representation γ(R(T, P )) of the result. While p0 is trained to predict the success of rewriting T by P by logistic regression, e0 is trained to predict γ(R(T, P )) by minimizing the squared error.  4.4  Embedding Alignment Model  Since ω and σ produce latent vectors γ(T ) ∈ L and γ 0 (T ) ∈ L0 in different spaces, we need to align those spaces enable deduction purely in the embedding space. (Merging σ and ω will remove the need for the α, however in our current setup we keep the embedding and deduction components separate). Given an initial statement T , we predict the approximate value of γ(R(T, P )), but in order to reason multiple steps in the embedding space alone, without explicitly computing R(T, P ), we need γ 0 (R(T, P )) to compute the outcome prediction of ω. For that, we train a translation model α : L −→ L0 which predicts γ 0 (T ) given an approximation of γ(T ). Note that α does not see T ∈ S as an input, it makes its prediction based on the latent space representation γ(T ) of T alone. This allows us for reasoning multiple steps ahead in the latent space without constructing any of the intermediate formulas explicitly.  4  4.5  Reasoning  After we have trained our three models on the training set theorems (and theorem pairs) of the HOList benchmark, we can use them to perform rewrites in the latent space alone. We use σ : S×S −→ [0, 1] as a quality metric for the propagated embedding vector. σ(T, P ) = p(c(γ(T ), π(P ))) was trained for predicting whether theorem P rewrites T . Given an approximation ṽT of the latent representation γ(T ), we can evaluate σ̃T (P ) defined by σ̃T (P ) = p(ṽT , π(P )) for a large number of tactic parameters P . This is compared with true rewrite successes of T by P to assess the quality of the approximation. To evaluate multiple steps of reasoning in the latent space, start with formula T1 and rewrite by theorems P1 , · · · Pk in that order. For reasoning in the latent space, we only use approximate embeddings vectors of the resulting formulas. To assess the quality of the overall reasoning, the same sequence of rewrites is performed with the actual formulas and the final approximate embedding is evaluated with respect to the formula resulting from the sequence of formal rewrites. In latent space L = Rk we from some initial theorem T1 ∈ S. The following schema indicates the sequence of predictions performed in latent spaces L and L0 : γ0  e0 (•,P1 )  e0 (•,P2 )  α  α  e0 (•,P3 )  T1 −→ l10 ∈ L0 −→ l2 ∈ L −→ l20 ∈ L0 −→ l3 ∈ L −→ l30 ∈ L0 −→ · · · This way, we approximate the following sequence of deductions in the latent space L alone, without producing any intermediate formulas. This is compared with the following formal sequence of rewrites: R(•,P1 )  R(•,P1 )  R(•,P3 )  T1 −→ T2 −→ T3 −→ · · · That is, l2 approximates the latent vector of T2 , that is γ(R(T1 , P1 )) and l3 approximates the latent vector of T3 , that is γ(R(R(T1 , P1 ), P2 ), by construction. By a slight abuse of notation we will refer to the operation of one step of approximate deduction in the latent space by ω ◦ α, as it is composition α and a subnetwork of ω.  5  Experiments  This section provides an experimental evaluation that aims to answer the following question: Is this setup capable of predicting embedding vectors multiple steps ahead? We explore the prediction quality of the embedding vectors (for rewrite success) and see how the quality of predicted embedding vectors degrades after ω ◦ α is used for predicting multiple steps in the latent space alone.  5.1  Neural Network Architecture Details  Our networks σ and ω both have two towers, which are 16-hop graph neural networks with internal node representations of 128 dimensions. The output of each of the two towers is fed into a layer that expands the dimension of the node representation to 1024 with a fully connected layer with shared weights for each node. This is followed by maximum pooling over all nodes of the network. The two resulting embedding vectors are concatenated along with their element-wise multiplication, and are processed by a three layer perceptron with rectified linear activation between the layers. The same architecture is used for both σ and ω, but the two networks do not share weights. Also, ω has larger layers in its combiner network c0 than σ in c, (1024 units each in c0 vs. 128 units in the layers of c). This was necessary for producing good quality predictions of the embedding vector of the outcome of the rewrite, but unnecessary for predicting the rewrite success alone. σ and ω are trained with g = 16 groups of l + 1 = 16 instances in each batch: one successful example in each group and l random negatives. However all other instances in other groups are used as negative instances for for each goal as well and they are considered negative regardless of whether they would rewrite it – this is justified by the fact that only a few theorems rewrite any given Ti so this introduces only a small 5  10 5  Occurrence  10 4 10 3 10 2 10 1 10 0  0  h  5000  10000  15000  Parameter Sorted By Usefulness  20000  Figure 2: Graph showing the distribution of successful rewrites for each parameter P , computed over all (T, P ) pairs in the database. Note that a small portion of parameters can be used for many theorems. This allows the Random and Usage baselines to achieve above-chance performance by predicting success based on parameter alone. amount of uncorrelated label noise. This training methodology is motivated by the fact that evaluating the combiner network is much cheaper then computing the embedding using the graph neural network. Based on [Alemi et al., 2016], we expect that hard negative mining would improve our results significantly, but it is left for future work.  5.2  Evaluation Dataset  In order to measure the performance of our models after multiple deduction steps are performed, we generate datasets D0 , D1 , . . . , Dr successively by applying rewrites to a randomly selected set of examples. We start with all theorems from the validation set of HOList, denoted by D0 . We create Di from Di−1 by selecting a random subset of 2000 statements from the previous step and 200 random tactic parameters PT to rewrite by for each statement. Formally, Di is defined by {R(T, P ) | P ∈ PT }.  5.3  Evaluation of Rewrite Prediction Model  In order to evaluate the performance of σ in isolation we need to compare it with carefully selected baselines: 1. As Figure 2 shows, a few theorems are much more frequently applicable than others. We want to see how the prediction of rewrite success performs based on the rewrite parameter alone if we ignore the theorem to be rewritten. One way to establish such a baseline we just feed a randomly selected theorem T 0 to σ instead of T to predict its rewrite success. 2. A stronger “baseline” is achieved by utilizing the ground-truth to make the best prediction possible based on knowing P but still being independent of T (the theorem to be rewritten). This is the best achievable prediction that does not depend on T . 3. As we have trained σ and ω only on pairs of theorems from the original database, the models exhibit increasing generalization error as we evaluate them on formulas that with increasing number of rewrites. First, we measure the errors these models make if the theorem for the last step is evaluated directly. This gives an upper bound on the rewrite success prediction by ω, since noisier embedding vectors end up with worse results on average. 4. Finally, we want to measure how rate at which the latent vectors degrade as we propagate them in embedding space as described in Subsection 4.5.  6  0.4 0.3 0.2 0.1 0.0 0.4 0.3 0.2 0.1 0.0 0.4 0.3 0.2 0.1 0.0  pos neg  1.0 0.8  True Positive Rate  Ratio of Scores  In order to measure the performance of our models after performing a given number of rewrite steps starting from the theorem database, we measure the tactic success prediction quality of σ using predicted embeddings. To do so, we compute the ROC curve of the predictions and use the area under the curve (AUC) as our main metric. Higher curves and higher AUC values represent more accurate predictions of rewrite success. We measure how ROC curves change as we use different approximations of γ(T ).  pos neg  0.6 0.4 0.2  pos neg  Pred Rewrite 1 Pred Rewrite 4 Random Baseline  0.0 25  20  15  10  5  Score Logit  0.0  0  0.2  0.4  0.6  False Negative Rate  0.8  1.0  Figure 3: Left: Histograms of scores for first step of rewrite predictions (top), fourth step of rewrite predictions (middle), and random baseline (bottom). Right: Corresponding ROC curves.  1.0  0.95  True Positive Rate  Area under RoC  1.00  0.90 0.85 0.80  0.8 0.6 0.4 0.2 0.0  0  True  1  2  Rewrite Steps  Pred (One step)  3  4 Pred (Multi step)  0.0  0.2  0.4  0.6  0.8  False Negative Rate  Usage Baseline  1.0  Random Baseline  Figure 4: Left: Comparison of Area under ROC curves for each embedding method. Right: Comparison of ROC curves for each embedding method after the fourth rewrite.  6  Analysis  Figure 3 shows the distribution of the theorem pair prediction score logits of p(c(γ(T ), π(P ))), for the those “positive” pairs that rewrite and the “negative” pairs that do not rewrite. Note that the ratio is normalized separately for the positive and negative pairs as negative pairs occur much more frequently than positive pairs. One can see that the quality of the rewrite success prediction degrades significantly after four steps of reasoning purely in the latent space, but it is still much better than the random baseline. This gives clear evidence that the embedding prediction manages to propagate much useful information over multiple steps in the latent space alone.  7  0.10 0.08 0.06 0.04 0.02 0.00  45 40 35  l2 Distance  Ratio of Distances Ratio of Distances  0.10 0.08 0.06 0.04 0.02 0.00  0 10 20 30 40 50 60 70 80  30 25 20 15  0 10 20 30 40 50 60 70 80  l2 Distance  Pred (One step)  10  0  Pred (Multi step)  1  2  Rewrite Steps  3  4  Random Baseline  Figure 5: Left: Histogram of l2 distances between each embedding method and the true embedding for rewrite steps 1 and 4. Right: Plot showing the mean l2 distances across rewrite steps. Pred (One step): expected error kγ(R(T, P )) − α(e(γ 0 (T ), π 0 (P )))k2 between the true embedding of the rewritten statement R(T, P ) and that of the prediction network ω ◦ α one step ahead in Dk . Pred (Multi step): expected error for γ(R(...R(T, P1 ), ..., Pk )) in Dk after k approximate rewrites. Random baseline: expected l2 distance kγ(T1 ) − γ(T2 )k2 between the “true” embeddings of two randomly chosen theorems from Dk .  In Figure 4 we make further measurements and comparisons on the quality of reasoning in the embedding space. On the left hand side we measure five different metrics: The “True” curve assess the embedding γ(T ) computed directly from the target theorem T . The “Pred (One step)” curve uses the approximate embedding α(e(γ 0 (Ti−1 ), π 0 (Pi−1 ))) for Ti = R(Ti−1 , Pi−1 ). That is, we measure the degradation when performing a single step of embedding prediction. The “Pred (Multi-step)” curve uses multiple steps of predictions completely in the latent space as described in Subsection 4.5. The “Random Baseline” predicts the rewrite success based on the latent vector of a random statement instead of the correct one. “Usage Baseline” is based on the constant prediction that ranks the parameters by how probably they rewrite any statement in the theorem database. This prediction is also independent of T . One can see that our model could perform reasoning for 4 steps in the embedding space and still retain a lot of the predictive power of the original model. In order to appreciate the above results one should keep in mind that none one of our models α, σ and ω were trained on statements that were already rewritten. All the training was done only on the theorems present in the initial database. The reduction of prediction performance is apparent from downward trajectory of the “Pred (Multi step)” curve, which isolates this effect from that of the error accumulated by the embedding predictions, the effect of which is measured indirectly by the “True” curve. In Figure 5 we have measure the l2 distance of the predicted embedding vectors versus that of the true embedding vectors γ(T ) of formulas after multiple rewrite steps in the latent space. These results are consistent with our earlier findings on success of rewrite predication after rewrite steps in the latent space: while there is some divergence of the predicted embedding vectors from the true embedding vectors (as computed from the rewritten statements directly), the predicted embedding vectors are significantly closer to the true embedding vectors than randomly selected embeddings.  7  Conclusion  In this paper we studied the feasibility of performing complex reasoning for mathematical formulas in a fixed (1024) dimensional embedding space. We proposed a new evaluation metric that measures the preservation semantic information under multiple reasoning steps in the embedding space. Although our models were not trained for performing rewrites on rewritten statements, nor were they trained for being able to deduce multiple steps in the embedding space, our approximate rewrite prediction model ω ◦ α has demonstrated  8  1  2  pair.ml arith.ml lists.ml calc_num.ml topology.ml metis.ml sets.ml nums.ml ind_types.ml class.ml canon.ml ints.ml convex.ml measure.ml floor.ml reals.ml theorems.ml calc_int.ml polytope.ml integration.ml trivia.ml iterate.ml vectors.ml metric.ml realax.ml permutations.ml realarith.ml degree.ml  40 35 30  3  4  l2 distance  25 20  15  10  (a) Visualization of the embedding spaces produced by embedding prediction e0 in the latent space after 1, 2, 3 and 4 rewrite steps. The points are colored by their l2 -distance to the true embedding γ(T ).  (b) Visualization of the embedding space γ(T ) evolving over four rewrite steps. Theorems are colored by the area of mathematics they originate from, sourced from the theorem database. The brightness of the embeddings corresponds to the rewrite step in which it was generated, with more recent embeddings being darker.  significant prediction power as far as 4 approximate rewrite steps performed in the latent space. Although it seems likely that these results can be significantly improved by better neural network architectures, hard negative mining and training on rewritten formulas, our methods showcases a simple and efficient general methodology for reasoning in the latent space. In addition, it proposes an easy to use, fast to train and crisp evaluation methodology for representing mathematical statements by neural networks. It is likely that such representations prove helpful for faster learning to prove without imitating human proofs like that in DeepHOL-Zero [Bansal et al., 2019a], given that premise selection is a closely related task to predicting the rewrite success of statements. Self-supervised pre-training or even co-training such models with premise selection could prove useful as a way of learning more semantic feature representations of mathematical formulas.  References [Alemi et al., 2016] Alemi, A. A., Chollet, F., Irving, G., Eén, N., Szegedy, C., and Urban, J. (2016). Deepmath-deep sequence models for premise selection. In Advances in Neural Information Processing Systems, pages 2235–2243. [Bansal et al., 2019a] Bansal, K., Loos, S. M., Rabe, M. N., and Szegedy, C. (2019a). Learning to reason in large theories without imitation. arXiv preprint arXiv:1905.10501. [Bansal et al., 2019b] Bansal, K., Loos, S. M., Rabe, M. N., Szegedy, C., and Wilcox, S. (2019b). HOList: An environment for machine learning of higher-order theorem proving. ICML 2019. International Conference on Machine Learning. [Brunner et al., 2018] Brunner, G., Fritsche, M., Richter, O., and Wattenhofer, R. (2018). Using state predictions for value regularization in curiosity driven deep reinforcement learning. In 2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI), pages 25–29. IEEE. [Chiappa et al., 2017] Chiappa, S., Racanière, S., Wierstra, D., and Mohamed, S. (2017). Recurrent environment simulators. CoRR, abs/1704.02254. [Coq, ] Coq. The Coq Proof Assistant. http://coq.inria.fr. [Dosovitskiy and Koltun, 2017] Dosovitskiy, A. and Koltun, V. (2017). Learning to act by predicting the future. ICLR 2017. 9  [Gauthier et al., 2017] Gauthier, T., Kaliszyk, C., and Urban, J. (2017). Tactictoe: Learning to reason with HOL4 tactics. In LPAR-21. 21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning, volume 46, pages 125–143. [Gonthier, 2008] Gonthier, G. (2008). Formal proof–the four-color theorem. Notices of the AMS, 55(11):1382– 1393. [Ha and Schmidhuber, 2018] Ha, D. and Schmidhuber, J. (2018). Recurrent world models facilitate policy evolution. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural Information Processing Systems 31, pages 2450–2462. Curran Associates, Inc. [Hales et al., 2017] Hales, T., Adams, M., Bauer, G., Dang, T. D., Harrison, J., Le Truong, H., Kaliszyk, C., Magron, V., McLaughlin, S., Nguyen, T. T., et al. (2017). A formal proof of the Kepler conjecture. In Forum of Mathematics, Pi, volume 5. Cambridge University Press. [Harrison, 1996] Harrison, J. (1996). HOL Light: A tutorial introduction. In FMCAD, pages 265–269. [HOL Light Rewrite Tactic Reference, ] HOL Light Rewrite Tactic Reference. Accessed: 2019/09/23. [Kaiser et al., 2019] Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et al. (2019). Model-based reinforcement learning for Atari. arXiv preprint arXiv:1903.00374. [Kaliszyk and Urban, 2015] Kaliszyk, C. and Urban, J. (2015). Hol (y) hammer: Online atp service for hol light. Mathematics in Computer Science, 9(1):5–22. [Lederman et al., 2018] Lederman, G., Rabe, M. N., and Seshia, S. A. (2018). Learning heuristics for automated reasoning through deep reinforcement learning. CoRR, abs/1807.08058. [Loos et al., 2017] Loos, S., Irving, G., Szegedy, C., and Kaliszyk, C. (2017). Deep network guided proof search. LPAR-21. 21st International Conference on Logic for Programming, Artificial Intelligence and Reasoning. [Oh et al., 2015] Oh, J., Guo, X., Lee, H., Lewis, R. L., and Singh, S. (2015). Action-conditional video prediction using deep networks in Atari games. In Advances in neural information processing systems, pages 2863–2871. [Paliwal et al., 2019] Paliwal, A., Loos, S., Rabe, M., Bansal, K., and Szegedy, C. (2019). Graph representations for higher-order logic and theorem proving. arXiv preprint arXiv:1905.10006. [Piotrowski et al., 2019] Piotrowski, B., Brown, C., Urban, J., and Kaliszyk, C. (2019). Can neural networks learn symbolic rewriting? [Wang et al., 2017] Wang, M., Tang, Y., Wang, J., and Deng, J. (2017). Premise selection for theorem proving by deep graph embedding. In Advances in Neural Information Processing Systems, pages 2786– 2796.  10  "
"Military Dog Based Optimizer and its Application to Fake Review Detection Ashish Kumar Tripathi Jaypee Institute of Information Technology, Noida  arXiv:1909.11890v1 [cs.NE] 26 Sep 2019  mail2ashish07@gmail.com  Kapil Sharma Delhi Technological University, New Delhi kapil@ieee.org  Manju Bala IP College of Women, New Delhi manjugpm@gmail.com    September 27, 2019  proposed algorithm outperforms the other considered algorithms on the majority of performance parameters. Keywords: Optimization, benchmark, clustering, fake reviews.  1. Introduction Meta-heuristic algorithm are gaining more and more popularity in the engineering domain due to their ability to bypass local optima and applicability across different disciplines, whereas the classical optimization algorithms are not able to provide a suitable solution for solving the optimization problems of high dimensionality. Since, the search space increases exponentially with the problem size, therefore solving these problems with the techniques like exhaustive search is impractical. Various heuristic approaches have been developed by the researchers to solve the global optimization problems such as Genetic algorithm (GA) [1], Particle swarm Optimization (PSO) [2], Gravitational search algorithm (GSA) [3], central force optimization (CFO) [4], Colliding Bodies optimization (CBO) [5], Magnetic charged system search (MCS) [5], Ray optimization [6], Cuckoo optimization (CO) [7], Firefly algorithm (FA) [8], etc. Meta-heuristics are the population based algorithms inspired from the nature. Each algorithms starts with the random set of solution called population. What makes the difference is the way of movements of population towards the global optima during the optimization process. These algorithms are tested and analyzed in the different domains of engineering. As No Free Lunch theorem clearly obviates the claim of an optimization algorithm for all optimization problems [9]. Thus, the urge of new meta-heuristic algorithm is standstill. Therefore, in this paper, a new meta-heuristic algorithm is proposed which leverages the searching ability of the trained military dogs. Dogs are trained by the humans for object detecting and tracking purposes. They train them especially as military dogs, sniffer dogs, hunting dogs, police dogs, search dogs, and detector dogs. Military dogs are the category of dogs, especially trained for detecting substances like bombs, illegal drugs, wildlife scats, currency, or blood [10]. Mostly, military dogs work in groups called military dog squad to detect the object. They use the barking sound to locate or signal other dogs. Coren and Hodgson [11] studied that each sound of the military dog have some meaning associated with it. For example, loud sound of dog indicates insecurity. Baying sound indicates a call from the military dog to assure that his mates are alerted [11]. Generally, the smelling power of the dog is 1,000 to 10,000 times more than the humans or other species [11]. Table 1 shows the number of scent receptors in the various species. Moreover, 2  Table 1: Number of Scent Receptors for different Species Species  Number of Scent Receptors  Humans  5 million  Dachshund  125 million  Fox Terrier  147 million  Beagle  225 million  German Shepherd  225 million  Bloodhound  300 million  the military dogs have the capability of deducing the direction of smell by moving their nostrils. Also, they have ability of storing meaningful information about the object in the form of scent while searching, which helps them in reaching to the desired object. Furthermore, united states war dogs association studies stated that the smelling power of dogs is effected by the wind. A dog may detect the suspected object up to 200 meters by the smell power if there is no wind. However, with the greater wind factor, the same can detect up to 1000 meters. Moreover, the factors like smoke and heavy vegetation are the confusing factors for a dog, as it confuse them in sensing the direction of actual smell or sound. This paper mimics the searching process of trained military dog squad to introduce a novel military dog based optimizer for finding the global optima. The overall contribution of this paper has three folds. First, a new military dog based optimization has been presented. Second, the mathematical model of the proposed algorithm has been detailed. The validation of the proposed algorithm has been done against 17 benchmark functions and performance is measured in term of 4 parameters namely: fitness value, standard deviation, convergence behavior, and consistency in the results. The efficiency of the algorithm is compared with 5 existing meta-heuristics. Third, the real-world problem of fake review detection has been unfolded using the proposed algorithm. Rest of the paper is organized as follows. Section II discusses the related work. Section III presents the mathematical model of the military dog based optimizer. Section IV provides the experimental results. Section V details the fake review detection problem and how it can be solved using MDBO. Conclusion and future work is elucidated in section VI. 2. Related Work Nature-inspired meta-heuristic algorithms mimics the optimization behavior of the nature. Generally, these algorithms are population-based and start with a population of random solutions to 3  obtain the global best solution. In contrast to this, there exists single-solution based algorithms like hill climbing [12] and simulated annealing [13], which initiates the optimization process with a single solution. However, these algorithms suffer with the problem of local trap and premature convergence as they do not share any kind of information. On the contrary, population-based algorithms improve the solution over the iterations by information sharing. Two common aspects of the population-based algorithms are exploration and exploitation. Exploration represents the diversification in the search space, while exploitation corresponds to the intensification of the current solution. All population based algorithm tries to attain an equilibrium between exploration and exploitation to achieve the global best solution. Every agent of the meta-heuristic tries to improve its performance by sharing its fitness value with other agents at each iteration. The meta-heuristic can be broadly classified into three categories namely; physics-based, swarm-behavior based and evolutionary-based. The physics-based algorithm optimizes the problem by imitating the physics based phenomenon. Gravitational search algorithm, proposed by Rashedi et al. [3], is one such algorithm which is based on Newtonian laws of gravity and motion. Hosseini [14] proposed an intelligent water drop algorithm which was inspired from the flow of rivers, as rivers often follow shortest path while flowing from source to destination. Further, Birbil [15] proposed an algorithm based on the concept of electromagnetism in which the properties of attraction and repulsion is used to attain a balanced trade-off between exploration and exploitation. Moreover, Mirjalili et al. [16] proposed multi-verse optimizer (MVO) in 2015, which is based on the notion of cosmology i.e white hole, black hole and wormhole. Some other physics based algorithms are Galaxy-based Search Algorithm (GbSA) [17], Black Hole (BH) [18] algorithm, Small-World Optimization Algorithm (SWOA) [19], Ray Optimization (RO) [20], Curved Space Optimization (CSO) [21].  Swarm-based algorithms behave like the swarm of agents such as fishes or birds to achieve optimization results. Eberhart et al. [22] proposed the particle swarm optimization (PSO) which was inspired from the swarming behavior of fish or birds in search of food. Gandomi [23] presented an algorithm based on the simulation of the krill individuals. Mirjalili [24] proposed an ant-lion based optimizer that mimics the hunting mechanism of ant-lions. Moreover, Mirjalili [25]also introduced the moth-flame optimization, which simulates the death behavior of moths, in which the movement of agent is based on the transverse orientation based navigation of moths. Further, Wang et al. [26]  4  proposed the hybrid krill heard algorithm to overcome the problem of poor exploitation capability of the krill herd algorithm. Ant colony optimization is another swarm based algorithm, which imitates the path finding behavior of ants [27]. Some other swarm based algorithm proposed in the literature are Cuckko search, Bat algorithm, Firefly optimization, Spider monkey optimization and Artificial bee colony optimization [28]. Evolution based algorithm are inspired from the biological evolution phenomena such as Darvin evolutionary theory. The evolutionary algorithms work on the principle of generating better individuals with the course of iterations by combining best individuals of the current generation. The popular genetic algorithm (GA) is an evolutionary algorithm based on the evolution of natural species. It maintains the balance between exploration and exploitation through the mutation and crossover operators. Another biological process based evolutionary algorithm is ES which gives almost equal importance to recombination and mutation, and it uses more than two parents to accord to an offspring. Baluja [29] proposed the probability-based incremental learning algorithm (PBIL) which manages only statics of the population rather than managing the complete population. Simon presented bio-geography based optimizer which is based on the immigration and emigration of the species between the islands of natural bio-geography. Differential evolution is another popular evolutionary algorithmic introduced by storm et al. [30].  3. Military Dog Optimizer In this section, a new optimization algorithm based on the behavior of military dog’s squad is introduced. Military dogs are the special trained dogs who go through a special training to search any specific type object, where they learn to identify thousands of scents. Moreover, military dogs undergo intense one on one training where they learn to work as a team to find the particular suspicious object for which they are trained. All these military dogs can communicate with each other by passing their message via barking. Hence dogs can cooperate with each other directly by passing message via the way of barking and its loudness. The loudness of barking indicates its closeness with the target object. When, a group of military dogs are left out for searching of a target object hidden in an open ground. The military dogs randomly start searching the area. With smelling sensation, the military dogs analyze a particular location and they define the fitness of the location in terms of loudness. The highest loudness indicates the best location among them. Military dog takes small step based on the scent smell in a particular location to exploit the local 5  search area and it moves to explore the search space based on the loudness of barking. The smelling sensation analysis of the military dogs help them to take a move closer towards the target object and exploit the current location. Military dogs diverge from each other to search for the target object and converge to indicate that the target object is close. 3.1. MDBO Definitions and Algorithm In this subsection the behavior of military dogs is mathematically simulated and explained. First, some definitions for formalizing the MDBO are explained. Thereafter, the whole procedure of the MDBO is outlined. In the given definitions R is used to refer the set of real numbers, φ is used to refer an empty set, while Z is used to denote the set of integers. Def inition1 : A military dog squad M DS m is a set of m trained military dogs. The size m of the military dog squad remains constant. Future work could allow variable size military dog squad. Def inition2 : The feasible solution vector F SV d , represents the position of a military dog in M DS. F SV ∈ Rd is a set of all real numbers that represents the urine marking of a M DS. Def inition3 : A military dog smell index M DSI : M D → R is a measure of goodness of the solution that is represented by a M D. In most of the population based algorithm, this M DSI is called fitness of the individual. Def inition4 : Sniffing movement δ(p, Pm , ) : M D → M D is a probabilistic operator that randomly modifies the military dogs F SV d based on the fitness of the loudest barking M D and movement probability Pm . Sniffing movement takes place using the following equation.   F SV j lodest , p ≤ Pm j F SVi (t + 1) =  F SV j + R(0, 1) × step(i), p > Pm i  (1)  where,  j step(i) = w × K(0, 1) × (F SVij − F SVloudest )  K(0, 1) is any randomly chosen number between 0 and 1. w is wind constant and p is any random number between 0,1. Def inition5 : Barking movement ω(p, q, α) : M Dn → MD is a probabilistic operator that adjusts position of a military dog based on F SV d of loudest barking and any randomly chosen Military Dog. The probability p, that position of M D is modified is constant and q ∈ (1, 2, 3..., d) is the randomly chosen index. alpha is the smog or vegetation constant that effects the sound coming from the other military dog. 6  The feasible solution vector (F SV d ) modification of a military dog is defined by:   F SV j (t), p ≤ α i F SVij (t + 1) =  F SV j (t) + Bm × R(0, 1), p > α i  (2)  where Bm = (F SVloudest − F SVq ) and R(0, 1) is any random number between (0,1). Def inition6 : The M DS transition function φ = (m, d, δ, ω, Pm ) : M Dm → M Dm is a 5tuple that modifies the M DS from one iteration to the next iteration. The M DS transition function begins by computing the feasible solution vector F SV d and military dog smell index M DSI. Further, the M DS modification is performed on each military dog M D followed by M DSI recalculation for each military dog. Def inition7 : A MDBO algorithm M DBO = (H, φ, T ) is a three tuple that finds the solution for an optimization problem. H :→ {M Dn , M DSI n } is a function that creates an initial M DS and computes the corresponding M DSI. φ is a M DS transition function defined earlier. H is implemented using the random number generators inside the urine marking area of the military dog. T : M Dn → {true, f alse} is a termination criterion. The MDBO algorithm can be informally described as follows: 1. The MDBO algorithm starts with the initialization of the MDBO parameters. In this step the method is derived for mapping the problem solution to F SV d and M DS as described in definition 1 and 2, which are problem dependent. Also the maximum number of military dogs, sniff movement probability Pm , smoke or vegetation constant α, wind factor K are initialized according to the nature of the optimization problem. 2. Initialize the position of each military dog in the search space corresponding to the potential solution given in the problem. This is defined by the H operator described in definition 7. 3. Sniffing around current area (exploitation step): In this step, each M D modifies its F SV based on the information got from the loudest barking dog. While searching, the dogs take a random walk and steer around the new location. M D searches around the target object and may either move directly towards the military dog at best position with movement probability Pm or they may take random movements according to its own position and the position of the M D nearest to the target object as described in definition 4. The pseducode of the sniffing movement is described as follows  7  for (i = 1 to m) do IF(K < Pm ) j F SVij (t + 1) = F SVloudest j step(i) = w × K(0, 1) × (F SVij − F SVloudest );  F SVij (t + 1) = F SVij + R(0, 1) × step(i) end for Table 2: Parameter values of algorithm of proposed and other algorithms S.  Parameter  PSO  PBIL  GA  ES  MVO  MDBO  No. 1.  Population Size (N )  50  50  50  50  50  50  2.  Number of Iterations (itr)  500  500  500  500  500  500  3.  Number of Dimensions (dim)  30  30  30  30  30  30  4.  Elite Size (keep)  2  2  2  2  2  2  5.  Inertial Constant (w)  1  0.3  −−  −−  −−  −−  6.  Congnitive Constant (c1 )  1  –  −−  −−  −−  −−  7.  Social Constant (c2 )  1  –  −−  −−  −−  −−  8.  Mutation Probability (Pmutate )  −−  −−  .1  0.1  9.  Smog or Vegetation factor (α)  –  –  –  –  –  .25  10.  Wind constant (w)  –  –  –  –  –  .25  4. Movement due to barking of other dogs(exploration step): It is the general nature of the military dogs that they bark loudly where they smell the suspected object. This creates a global movement of the military dogs. After a certain threshold of barking military dogs try to explore the search region with respect to the most loudly barking military dog. Each military dog takes a random move by considering the loudest barking military dog as global best and any randomly chosen barking military dog. The updated position is defined as per definition 5. The psedo-code of the barking movement is described as follows: for (i = 1 to m) do K = rand(0, 1) K=IF(K < α) Bm = rand ∗ (F SVloudest − F SVq ); (i+1)  F SVi  = F SVi (t) + Bm × K  end for  8  5. Go to step three for the next iteration. This loop continues till the predefined number of iterations, or the desired solution has been found . This is the implementation of the T operator described in definition 6. Fig. ??a and Fig. ??b demonstrate the barking and sniffing movement of the M Ds. It can be depicted from Fig. ??a that the barking movement represents the exploration step of the MDBO. However, the barking movement corresponds to the exploration step, as its movement is influenced any randomly chosen M D.  4. Fake Review Detection Nowadays, reviews play an important role in the sales of the products and services, thus ascertaining their authenticity is a challenging problem. For the same, fake review detection is one of the fundamental approach used to detect the fake reviews. In literature, the majority of the contemporary work is based on supervised learning models [31]. However, the supervised models require labeled datasets. Therefore, the applicability of the supervised models is limited, as labeled datasets of fake reviews are rarely available. On the contrary, unsupervised learning models work on unlabeled datasets to induce the learning model. Generally, these models explore hidden structures of the dataset with N data objects into K clusters such that the data objects within a cluster have maximum resemblance [32]. The traditional clustering methods, such as K-means and FCM, generally produce local optima in the presence of noise [33][34] [35] [18]. To alleviate this, the meta-heuristic algorithms have been proved to be efficient in performing clustering [36] [37]. Therefore, this paper leverages the strengths of MDBO to produce optimal cluster centroids for untangling the fake review detection problem. 4.1. MDBO based clustering for fake review detection In the MDBO based clustering, the F SV of each military dog represents a set of cluster centroids, C = {C1 , C2 , · · · , Ck } for K clusters. The M DSI value of each military dog corresponds to the sum of squired Euclidean distance as defined in Eq. (3). M inD(Z, C) =  N X k X i=1 j=1  9  wij | zi − cj |  (3)  Table 3: Benchmark Functions  Sr.  Function  Equation  Range  No. Name  Optimal Optimal value  Category  position values  1  Ackley  F1 (X) ed  2  3  Alpine  Dixon  =  − 1 Pd i=1 cos(2πxi )  Pd  F2 (X) =  and  q P −0.02 d− 1 d x2 i=1 i −20e  i=1  |xi sin(xi ) + 0.1xi |  Price Griewank  F4 (X) = 1 +  5  Levy  F5 (X) Pd−1  7  8  Pathological  Perm  Powell  Pd  i=2  x2 i i=1 4000  Pd  i(2x2i − xi−1 )2  −  i=1  cos( √ii )  Pd  i=1  sin2  +  -  0  (0, · · · , 0)  Multi-Model  0  (0, · · · , 0)  Unimodal  -20,+20  0  (0, · · · , 0)  Multi-model  -50,+50  0  (0, · · · , 0)  Multi-Model  -  0  (0, · · · , 0)  Multi-model  0  (1, 1/2, · · · , 1/d)Multi-Model  -10,10  0  (0, · · · , 0)  Uni-model  -  0  (0, · · · , 0)  Uni-Model  0  (0, · · · , 0)  Uni-Model  + +  P d  j=1  i=1  q  100x2 +x2 −0.5 i i+1 1+0.001(x2 −2xi xi+1 +x2 )2 i i+1  0.5 +  Pd/4  2  Multi-Model  = 1, · · · , d  i=1  F8 (X) =  2    Pd−1  F7 (X) =  (0, · · · , 0)  -  x  Qd  sin2 (πω1 )  1 + 10 sin (πωi + 1) i=1 (ωi − 1)   2 2 (ωd − 1) 1 + sin (2πωd ) + , ωi = 1 F6 (X) =  0  100,+100  =  2  xi −1 4 , f oralli  6  -32,+32  100,+100  F3 (X) = (x1 − 1)2 +  4  −  + 20 + e   (j + β) xji −  1 ji  !  2  [(x4i−3 + 10x4i−2 )2 + 5(x4i−1 −  100,+100 100,+100  x4i ) + (x4i−3 + 2x4i−2 )4 + 10(x4i−3 + x4i )4 ] 9  10  PowellSum  Rastrigin  F9 (X) =  Pd  i=1  |xi |i+1  100,+100  F10 (X) = 10d +  11  Rosenbrock’s  F11 (X) =  12  Rotated  F12 (X) =  Hyper-  Pd−1  Pd  2 i=1 (xi  − 10cos(2πxi ))  5.12,+5.12  [100(xi+1 − x2i )2 + (xi − 1)2 ]  -30,+30  0  (0, · · · , 0)  Multi-Model  Pd  Pi  -  0  (0, · · · , 0)  Uni-Model  Pd  x4i  0  (0, · · · , 0)  Uni-model  0  (0, · · · , 0)  Multi-Model  0  (0, · · · , 0)  Uni-Model  0  (0, · · · , 0)  Uni-model  0  (0, · · · , 0)  Uni-Model  i=1  i=1  j=1  x2j  65.536,+65.536  Ellipsoid 13  Schumer Stei-  F13 (X) =  glitz 14  15  16  17  Schwefel  Sphere  Step  Trigonometric  F14 = −  i=1  Pd  F15 (X) =  F16 (X) =  F17 (X) = sin(xi ))]2  i=1  Pd  100,+100  xi sin  i=1  p |xi |  500,+500  x2i  -  10  Pd  i=1 (⌊|xi |⌋)  Pd  i=1 [d  100,+100 100,+100  −  Pd  j=1  cos xj + i(1 − cos(xi ) −  0,3.14  Where N represents the number of data objects, | zi − cj | is the Ecludian distance of I th data object from the J th centroid. Further, wij represents the association weight of ith review vector in the j th cluster, i.e. the value of wij is 1 if the data object i is allocated to the cluster j otherwise 0. For, the M DS of size N , the clustering process starts with N candidate solution and these solutions are optimized with the course of iterations to improve M DSI. Finally, the F SI of the M D with best M DSI value are returned as the final cluster centroids. 4.2. Datasets For performing the experiments, the real life dataset is collected from the Yelp [38], which has 142 million unique visitors from 31 countries. Yelp itself filters the reviews which are considered as highly reliable and accurate [39]. In this work, total of 6000 reviews are compiled from the Yelp recommended and non recommended section for genuine and fake reviews. The beautiful soup library of python for web scraping is used for crawling the data. The reviews are extracted pin code wise, starting from zip code 10000 and 10050, which corresponds to the restaurant pages of New York city. The complete block of the review is extracted consisting of review text, star rating by the reviewer to the restaurant, no of cool votes to the particular review, no of funny votes to the particular review, no of useful votes to the particular review, no of check-ins of reviewer in the hotel, no of photos uploaded by reviewer of the review on Yelp, no of friends of reviewer of the review on yelp, no of reviews till date of the reviewer of the review. Furthermore, the content analysis of the reviews has been performed using natural language tool kit (NLTK) to extract review centric features. NLTK provides an easy to use interface with rich set of lexical resources such as WordNet for the tokenization, parsing, semantic reasoning and tagging. Moreover, the feature extracted from the NLTK were also validated manually by randomly picking 30 reviews. Total 11 verbal and non verbal features has been used in the experiments based on the previous studies [40] [31] [39]. Table 4 contains the summary of the features used in the experiments. Each review represent a feature vector of length 11 and the numerical value of each feature is normalized between [0,1].  5. Experimental results The performance of the proposed algorithm is evaluated in two folds, first the MDBO is validated on benchmark functions and results are are detailed in section 5.1. Second, the the effectiveness of the MDBO is vindicated on fake review detection problem and the results are presented in section 11  Table 4: Features taken for the clustering using MDBO  Sr.  Feature  Category  Definition  No. Name 1  Nonverbal  Review count  It defines the total number of reviews posted by the reviewer.  2  Nonverbal  check-ins:  it represents the number of check-ins by the reviewer for the hotel  3  Nonverbal  Friend count:  It denotes the total number of friends of the person making review  4  Nonverbal  Vote count  it represents the count of votes on the review  5  Nonverbal  useful, cool, funny votes  count of useful, cool and funny votes  6  Nonverbal  Followers  it is the total number of followers of the reviewers  7  Nonverbal  Elite reviewer  it is the count of years for which reviewer has been a permanent yelp member  8  Nonverbal  Average posting rate:  it represents the total number of reviews posted per day  9  verbal  Review length  it is the total number of words per review  10  verbal  Average content similarity  It is defines as the average similarity in the reviews given by a single reviewer  11  Verbal  Average content similarity  It corresponds the average similarity in the text of the reviews posted by a particular reviewer  12  5.2. For fair comparison, each algorithms is rum on a computer with 2.8 Ghz Intel ( R) Pentiam (R) core i3 processor and 8 GB RAM using Matlab 2015a. 5.1. Benchmark Function Results In this section, the performance and uniqueness of the proposed MDBO is analyzed and compared with five recent population based algorithms. Seventeen standard benchmark functions given in Table 3 are used for comparison of algorithm based on mean and standard deviation. Convergence behavior of MDBO is analyzed and compared with other algorithms by plotting the convergence graph for each benchmark function. Box plots are employed to visualize and establish the consistency of the proposed MDBO algorithm. Box plots are non parametric methods to display variations in results of proposed MDBO algorithm, which are further compared with five other algorithms on seventeen benchmark functions. Moreover, Wilcoxon rank sum test is performed, which shows the dissimilarity of MDBO with other algorithms. 5.1.1. Comparison with existing algorithms The proposed MDBO was tested on the minimization functions and the results were compared with five other algorithms namely MVO, ES, Pbil, GA as well as PSO. Table 2 contains the values of population size, number of dimensions, number of iterations, social constant, cognitive constant, mutation probability, wind factor and smog constant used in simulation. Table 3 contains the details of the seventeen benchmark functions including range values, optimal position values and categories upon which the proposed algorithm has been tested and compared. Each function either belongs to uni-model or multi-model class. Nine uni-model functions are used to test the convergence rate and eight multi-model functions are used to test the local optima avoidance capability of the algorithm. Further, each algorithm was run fifteen times on each benchmark function to get the mean and standard deviation. Table 5 shows the values of mean and the standard deviation of fitness values computed in fifteen rounds by each algorithm. From the comparison of mean and standard deviation of seventeen benchmark functions for six algorithms as given in Table 5, it is observed that proposed MDBO outperformed all five algorithms under comparison on sixteen benchmark functions in terms of mean fitness values. However, ES performed better than MDBO for only one benchmark functions i.e F6 with mean value 4.48 as compared to 4.68 mean value of proposed MDBO. Further, standard deviation of the proposed MDBO is minimum for sixteen benchmark functions while ES has given minimum value of the standard deviation for one function 13  i.e., F1. It can be observed that in all the nine uni-model functions proposed MDBO algorithm has beat all other algorithms showing stronger local search ability. However, proposed algorithm outperformed other algorithms in seven multi-model functions out of eight which confirms stronger exploration capability of the proposed algorithm. 5.1.2. Wilcoxon Test The uniqueness of the proposed algorithms have been statistically validated using Wilcoxon rank sum test. NULL hypothesis assumes that the two algorithms are similar at the five percent significance level α for benchmark functions. p values has been computed for all the benchmark functions using the fitness values of compared and proposed algorithms. If the value of p<0.05 then null hypothesis is rejected and symbolized by ‘+’ or ‘-’, otherwise it is rejected and represented by symbol ‘=’. However ‘+’ indicates better result and ‘-’ represents poor results of the proposed MDBO algorithm. Table 6 shows the results of Wilcoxon rank sum test for the NULL hypothesis over seventeen benchmark functions explained Table 3. The proposed MDBO algorithm is compared with ES, PSO, MVO, Pbil, and GA on the basis of p values value. The p value is computed by running fifteen iterations of each algorithm on all functions. A pair wise comparison of MDBO with other algorithms shows significant levels on the basis of p value, mean and standard deviation. Significant level is positive if p value is less than 0.05 and the value of mean and standard deviation are less than the compared algorithm. It is observed from the Table 6, that MDBO has outperformed ES on all the benchmark functions except F6 where ES has given competitive result. Further, MDBO has surpassed PSO for all the benchmark functions. When MDBO is compared with MVO it has beaten on sixteen benchmark function out of seventeen. However for one function i.e., F6, GA performed well. Moreover, MDBO has given positive significance on all the benchmark functions when compared with PBIL and GA except for F6 function. Hence is can be concluded that the proposed algorithm is significantly different and outperforms five existing algorithms i.e., MVO, ES, PBIL, GA as well as PSO on each benchmark function. 5.1.3. Convergence rate The convergence behavior of the proposed MDBO algorithm is analyzed and compared with other five existing algorithms by plotting the convergence graph for each bench-mark function. Vertical axis of the graph represents the best of fitness value and the horizontal axis represents corresponding iteration number as depicted in Fig. ??. Further, Fig. ??a, ??b...??q, shows 14  Table 5: Comparison of mean fitness and standard deviation values for 15 runs on benchmark functions for existing and proposed algorithms Fun  PBIL  PSO  GA  MVO  ES  MDBO  Mean  STD  Mean  STD  Mean  STD  Mean  STD  Mean  STD  Mean  STD  F1  4.51E+00  0.19E+00  3.55E+00  0.30E+00  2.18E+00  4.32E-01  1.63E+00  4.50E-01  6.31E+00  1.88E-01  9.71E-01  7.39E-01  F2  4.89E+02  3.54E+01  1.02E+00  5.01E+00  5.29E+01  1.15E+01  1.27E+02  3.42E+01  4.91E+02  3.67E+01  2.34E+00  1.91E+00  F3  8.56E+09  1.84E+09  3.06E+08  3.03E+08  1.05E+06  1.06E+06  5.71E+03  5.32E+03  8.26E+09  2.54E+09  1.90E+02  7.28E+02  F4  1.46E+00  0.06E+00  1.11E+00  0.04E+00  1.07E+00  2.57E-02  1.50E-02  1.25E-02  1.54E+00  4.98E-02  2.79E-03  4.26E-03  F5  3.69E+03  5.89E+02  5.21E+02  2.13E+02  5.32E+01  1.86E+01  4.83E+02  3.14E+02  3.70E+03  3.60E+02  2.22E+01  2.35E+01  F6  9.37E+00  0.66E+00  7.80E+00  0.52E+00  5.65E+00  4.94E-01  9.93E+00  3.62E-01  4.48E+00  3.19E-01  4.68E+00  8.03E-01  F7  2.00E+117  3.60E+117  3.30E+98  1.20E+99  5.59E+58  2.15E+59  1.05E+17  4.05E+17  4.50E+117  6.50E+117  1.00E+10  0.00E+00  F8  4.62E+04  93.54E+02  1.82E+04  7.40E+03  1.86E+02  1.22E+02  7.78E+00  5.49E+00  1.24E+05  2.91E+04  9.65E-03  7.42E-03  F9  4.46E+41  4.99E+41  1.32E+54  4.80E+54  3.33E+16  1.20E+17  2.96E+11  4.10E+11  8.41E+43  1.70E+44  1.00E+10  0.00E+00  F10  1.54E+02  1.17E+01  1.43E+00  3.94E+01  1.40E+01  6.49E+00  1.23E+02  3.56E+01  4.14E+02  1.96E+01  2.07E+01  5.61E+00  F11  1.39E+08  3.50E+07  3.67E+06  3.10E+06  9.47E+03  1.03E+04  4.58E+02  5.30E+02  1.50E+08  2.87E+07  1.02E+02  4.02E+01  F12  2.46E+01  1.99E+04  9.56E+04  7.77E+04  4.88E+03  2.50E+03  1.83E+01  1.31E+01  2.79E+05  2.70E+04  8.89E-12  5.32E-12  F13  1.87E+08  3.56E+08  7.40E+06  6.77E+06  4.25E+04  3.49E+04  1.01E-01  5.52E-02  1.55E+08  4.13E+07  3.55E-16  5.74E-16  F14  8.65E+00  3.32E+02  6.57E+03  1.07E+03  3.22E+03  6.44E+02  4.53E+03  7.70E+02  8.41E+03  3.79E+02  5.48E+02  2.20E+02  F15  4.52E+00  4.14E+03  1.02E+04  2.70E+03  1.05E+03  4.98E+02  6.76E-01  1.91E-01  4.62E+04  4.21E+03  2.07E-12  1.58E-12  F16  9.32E+02  6.70E+00  4.82E+02  7.82E+01  1.82E+02  5.65E+01  5.60E+00  4.73E+00  9.12E+02  5.87E+01  2.67E-01  7.04E-01  F17  0.89E+00  2.36E+00  7.67E+03  6.86E+00  5.05E+00  1.9"
"Improving the Intelligibility of Electric and Acoustic Stimulation Speech Using Fully Convolutional Networks Based Speech Enhancement Natalie Yu-Hsien Wang, Hsiao-Lan Sharon Wang, Tao-Wei Wang, Szu-Wei Fu, Xugan Lu, Yu Tsao, and Hsin-Min Wang   I.  INTRODUCTION  Cochlear implant (CI) is a surgically implanted electronic medical device that stimulates nerves to provide a sense of sound for people with profound-to-severe hearing loss. Despite technological and surgical advances since the 1960s, improving the speech perception and intelligibility of CI users in real-world scenarios remains challenging [1–5]. One promising direction is the combined electric and acoustic stimulation (EAS) technology. For EAS, an electrode array is implanted only partially into the cochlea because many people with hearing loss still have the residual acoustic hearing (20-60 dB hearing loss up to 750 Hz) at the low frequencies. That is, the device is a combination of hearing aid, which acoustically amplifies the low frequency signals, and a CI, which stimulates the regions responsible for the mid and high frequency sounds (for reviews on EAS fitting and signal proManuscript submitted May 1, 2019; revised xxxxx. (Corresponding author: Yu Tsao). N. Y. Wang, T.-W. Wang, S.-W. Fu, and Y. Tsao are with the Research Centre for Information Technology Innovation, Academia Sinica, Taipei 11529, Taiwan (email: nataliewang@citi.sinica.edu.tw; dati1020@citi.sinica.edu.tw; jasonfu@citi.sinica.edu.tw; yu.tsao@citi.sinica.edu.tw). X. Lu is with the National Institute of Information and Communications Technology, Tokyo 184-0015, Japan (email: xugang.lu@nict.go.jp) S. S. Wang is with the Department of Special Education, National Taiwan Normal University, Taipei 10610, Taiwan (email: hlw36@ntnu.edu.tw). H. –M. Wang is with the Institute of Information Science, Academia Sinica, Taipei 11529, Taiwan (email: whm@iis.sinica.edu.tw).  cessing, see [6, 7]). Dorman and Gifford [8] revealed that, compared to acoustic-only hearing aid and conventional CI users, EAS users achieved better speech recognition at both word and sentence levels. Although the benefits of EAS have been documented [9, 10], there is room for improvement in the performance of EAS in noisy environments. However, there is very little work on speech enhancement (SE) for EAS. Motivated by the advantage of EAS and the need for an effective SE approach for EAS devices, this study therefore explores whether deep-learning-based SE models are suitable for EAS, in comparison to a conventional SE approach. Moreover, this study examines whether SE approaches that have been found effective for CI are equally effective for EAS. Currently, various SE models have been developed to cope with different noisy conditions [2,11–17]. These SE models are primarily used for conditions with a single microphone or multiple microphones. Compared to single-microphone approaches, multi-microphone approaches deal with spatially separated target and noise more efficiently [18–21]. Beamforming is constantly used to improve the recognition accuracy of multi-microphone speech data. For instance, Buechner et al. [22], compared omnidirectional microphone setting with two types of beamforming (adaptive monaural and binaural). The results demonstrated that both beamforming types yielded better speech perception scores than the omnidirectional approach. Despite satisfactory speech intelligibility can be achieved, multi-microphone approaches have some limitations. On the one hand, these approaches involve more hardware, such as a secondary microphone and headphone combination, and are therefore more expensive than the single-microphone methods. On the other hand, the applicability of multi-microphone approaches is restricted to the acoustic situation where the target and noise are spatially separated, and its efficacy degrades in reverberant environments [23]. Moreover, the speech signals acquired by multiple microphones are eventually fused to form a single-channel speech signal before being sent to EAS or CI users. Therefore, the effectiveness of a single-microphone SE approach plays an important role in the performance of EAS or CI devices. Various single-microphone SE approaches have been proposed, which can be roughly divided into unsupervised and supervised approaches. A class of unsupervised SE approaches are derived based on the spectral and statistical properties of noise and speech signals; well-known approaches include spectral subtraction [24], minimum-mean square-error (MMSE) [25], logMMSE [26], Wiener-filter-based [27, 28], and signal-to-noise-ratio (SNR)-based [29, 30] methods. Another class of unsupervised SE approaches are the subspace-based methods, which construct two subspaces, one for clean speech and the other for noise signals, and use the information in the clean-speech subspace to restore the clean speech. Notable subspace techniques include singular value decomposition (SVD) [31], Karhunen–Loeve transform (KLT) [32, 33], and principal component analysis (PCA) [34]. Although many of these unsupervised single-microphone ap-  proaches can produce satisfactory SE results for CI processors, they are more effective for stationary noise than for nonstationary noise, which does not always satisfy the unpredictable reality of acoustic conditions [35]. In addition to unsupervised SE approaches, numerous machine-learning-based algorithms have been popularly used in the single-channel SE field. For these approaches, a denoising model is usually prepared in a data-driven manner without imposing strong statistical assumptions on the clean speech and noise signals, and the noisy speech signal is processed by the denoising model to extract the clean speech signal. Notable examples include nonnegative matrix factorization [36], compressive sensing [37], and sparse coding [38]. More recently, deep learning models have been applied to the single-channel SE field. With deep structures, the complex correlation of noise and clean speech signals can be characterized. Deep-learning-based methods have demonstrated notable improvements over traditional methods [39]. Well-known deep-learning-based models include deep denoising autoencoder (DDAE) [15, 40], deep fully connected networks [35, 41, 42], recurrent neural networks (RNNs) [43– 45], and convolutional neural networks (CNNs) [46, 47]. Recently, two research directions of deep-learning-based SE have attracted great attention. The first intends to develop a more appropriate input-output, and the second aims to derive a task-oriented objective function to train the denoising model. Aiming to identify suitable input-output, most conventional single-microphone approaches, such as DDAE-based SE, use power spectrum (PS) or its logarithmic form [14, 15, 40] as the acoustic features as the input of the denoising model. The denoising model aims to transform noisy PS features to generate enhanced features that are as close as possible to the clean references. To restore the speech waveform, the phase information from the original noisy speech is typically used. This is because there is no clear structure in a phase spectrogram, and it is difficult to accurately estimate the clean phase information from its noisy counterpart [46, 48]. It is clear that directly using the phase of the noisy speech is not optimal and may degrade the enhanced speech quality. Many approaches have been proposed to overcome this imperfect phase estimation issue and can be roughly divided into two categories. The first category adopts complex spectra as the acoustic features. The deep learning model learns the mapping or masking function to retrieve the clean complex spectrum from the noisy one, and thus simultaneously estimate the phase and amplitude information of the speech signal. Many studies have confirmed that complex spectral features lead to better performance than (log) PS features [49, 50]. The second category suggests that enhancement can be performed directly on a raw speech waveform without transforming it into spectral features [51–55]. For instance, Fu et al. [51] proposed using a fully convolutional neural network (FCN) model for SE in the time domain because FCN can preserve neighbouring information of a speech waveform to generate high and low frequency components using the same denoising model. Their experimental results show that, compared to CNN and deep neural networks (DNNs), the FCN model yields better speech intelligibility in terms of the short-time objective intelligibility (STOI) with fewer parameters. Later on, an utterance-based SE approach based on the FCN model was proposed in [52]. This utterance-based FCN model is capable of handling different kinds of objective functions from a local time scale (frame) to a global time scale (utterance) and achieves higher perceptual scores (STOI and perceptual estimation of speech quality, PESQ) than the frame-based FCN model.  The second focus of recent deep-learning-based SE research is to derive appropriate objective functions in consideration of human auditory perception. Conventional deep-learning-based SE employs engineering-defined distances, such as the mean square error (MSE) based on the L2 norm Euclidian distance and the L1 norm, to measure the error of the enhanced speech signal and the reference clean speech signal [15, 35, 40]. More recently, other evaluation metrics, such as PESQ [56, 57] and automatic speech recognition accuracy [58–61], have been adopted to form the objective functions for training the deep-learning models. Fu et al. [52] proposed to train the FCN model using the STOI-based objective function (termed FCN(S) in short) and observed that the enhanced speech signal is superior to speech signal generated by an MSE-trained FCN model in terms of STOI. Moreover, the results of subjective recognition tests performed on people with normal hearing (NH) also confirmed that the STOI-based objective function enables the FCN model to generate speech signals with higher intelligibility. A recent study has evaluated the efficacy of the DDAE model for CI using a noise-vocoded speech simulation [14]. Experimental results show that the DDAE-based method outperforms three commonly used single-microphone SE approaches (logMMSE, KLT, and Wiener filter), in terms of intelligibility, evaluated with STOI, and speech recognition, evaluated with listening tests. The results have confirmed the potential of applying deep learning models to improve CI devices. This study aims to evaluate the performance of FCN(S) with vocoded speech, which simulates speech signal processing used in EAS under various noisy conditions. The SE performance of FCN(S) is compared with a traditional SE approach, MMSE, and a DDAE model [14]. We tested the performance on both stationary and nonstationary noise types at two different SNR levels. Experimental results have confirmed that FCN(S) can achieve better performance in both objective evaluation and subjective listening tests compared to MMSE and DDAE. The reminder of this paper is organized as follows: Section II introduces the architecture of the FCN(S) model. Section III presents the vocoder that is used to generate the vocoded speech signals. Section IV reports experimental setup and results as well as provides discussions about the results. Section V provides the concluding remarks of this study. II. THE FCN(S) MODEL The FCN(S) model used in this study is proposed by Fu et al. [52]. Fig.1 shows the structure of the utterance-based FCN SE model. The FCN model can accommodate speech input with any arbitrary length. In the FCN model of Fig.1, each (1D CNN) ﬁlter is convolved with all the generated waveforms from the previous layer and produces ﬁltered waveforms. Therefore, the ﬁlters have another dimension in the channel axis. Since the goal of the single-channel SE approaches is to generate one clean utterance, there is only one ﬁlter in the last layer. Note that the FCN SE model presented in Fig.1 is a complete end-to-end (noisy waveform in and clean waveform out) framework that does not require pre- and post-processing (feature extraction and speech restoration).  Fig.1. Structure of an utterance-based FCN SE model.  ... yt −1  yt  yt +1 ...  F Rt  G It Input  ... xt −1  xt  xt +1 ...  Fig. 2. The relation between the output layer and the last hidden layer in a fully convolutional framework.  The FCN model does not contain fully connected layers, as shown in Fig. 2. It is similar to the conventional CNN but all the fully connected layers are removed. Thus, the total number of parameters in FCN is considerably reduced. More importantly, in CNN with fully connected layers, the local information and the spatial arrangement of the previous layer could not be well preserved. By changing this design, the FCN model is capable of dealing with the high and low frequency components of the raw waveform at the same time. The relation between the output sample 𝑦𝑦𝑡𝑡 and the connected hidden nodes 𝐑𝐑 𝑡𝑡 can be represented by the following equation. 𝑦𝑦𝑡𝑡 = 𝐅𝐅 T 𝐑𝐑 𝑡𝑡  (1)  where 𝐅𝐅 ∈ ℝ𝑓𝑓×1 denotes one of the learned filters, and 𝑓𝑓 is the size of the filter. Note that F is shared in the convolution operation and is fixed for each output sample. Therefore, if 𝑦𝑦𝑡𝑡 is in the high frequency region, 𝐑𝐑 𝑡𝑡 and (𝐑𝐑 𝑡𝑡−1 , 𝐑𝐑 𝑡𝑡+1 ) should be different. The similarity between 𝐑𝐑 𝑡𝑡 and its neighbors depends on the filtered outputs of previous locally connected nodes 𝐈𝐈𝑡𝑡 . For the details on the structure of the FCN model for waveform enhancement, please refer to the previous works [46, 47]. When we use the L2 norm, the objective function is defined as: ℒ(𝜃𝜃) = ∑𝑇𝑇𝑡𝑡=1‖𝑦𝑦𝑡𝑡 − 𝑞𝑞𝑡𝑡 ‖2  (2)  where 𝜃𝜃 denotes the model parameters of FCN, 𝑦𝑦𝑡𝑡 and 𝑞𝑞𝑡𝑡 are the t-th samples of the estimated and reference clean waveforms, respectively, and T denotes the number of samples in the waveform. When using STOI in the objective function, we have 1  ℒ(𝜃𝜃) = − ∑𝑢𝑢 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(𝒘𝒘𝒚𝒚 (𝑢𝑢), 𝒘𝒘𝒒𝒒 (𝑢𝑢)) 𝑈𝑈  (3)  where 𝒘𝒘𝒚𝒚 (𝑢𝑢) and 𝒘𝒘𝒒𝒒 (𝑢𝑢) are the u-th estimated utterance and clean reference, respectively, and U is the total number of training utterances. 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(. ) is the function that calculates the STOI value of the noisy/processed utterance given the clean reference.  There are five steps to calculate the STOI value [62]: 1) Removing silent frames: Silent regions do not contribute to speech intelligibility and are therefore removed prior to evaluation. 2) Short-time Fourier transform (STFT): STFT is applied on both clean and noisy/processed speech utterances to obtain a representation that is similar to the speech properties in the auditory system. 3) One-third octave band analysis: 15 one-third octave bands are used to transform the clean and noisy/processed speech spectra with the lowest center frequency set to 150 Hz and the center-frequency of the highest one-third octave band set to 4.3 kHz. 4) Normalization and clipping: The respective goal of the normalization and clipping procedures is to compensate for global level differences and to ensure that the sensitivity of the STOI assessment to a severely degraded TF-unit is upper bounded. 5) Intelligibility measure: The intermediate intelligibility measure is defined as the correlation coefficient between the temporal envelopes of clean and noisy/processed speech signals. Finally, the STOI score is calculated as the average of the intermediate intelligibility measures on all bands and frames. Notably, short segments (e.g., 30 frames) of temporal envelopes of the clean and the noisy/processed speech are used to compute the correlation coefficient. Therefore, the objective function of Eq. (3) cannot be directly optimized by a traditional frame-wise enhancement scheme. On the contrary, the FCN model that can take input of any arbitrary length can be combined appropriately with the STOI-based objective function. The FCN model optimized with the STOI-based objective function is termed FCN(S). In [52], a disadvantage of using Eq. (3) as the objective function has been noted: the enhanced speech signals still involve clear noise components. To improve the noise suppression capability of FCN(S), we have derived a modified objective function that combines the MSE and STOI terms, which is represented as, ℒ(𝜃𝜃) =  2 1 𝛼𝛼 �( �𝒘𝒘𝒚𝒚 (𝑢𝑢) − 𝒘𝒘𝒒𝒒 (𝑢𝑢)� 𝑈𝑈 𝐿𝐿𝑢𝑢 2 𝑢𝑢  (4)  − 𝑠𝑠𝑠𝑠𝑠𝑠𝑠𝑠(𝒘𝒘𝒚𝒚 (𝑢𝑢), 𝒘𝒘𝒒𝒒 (𝑢𝑢)))  where 𝐿𝐿𝑢𝑢 is the length of the u-th utterances 𝒘𝒘𝒚𝒚 (𝑢𝑢) and 𝒘𝒘𝒒𝒒 (𝑢𝑢), and 𝛼𝛼 is the weighting factor of the two optimization targets. The first term (  1  𝐿𝐿𝑢𝑢  2  �𝒘𝒘𝒚𝒚 (𝑢𝑢) − 𝒘𝒘𝒒𝒒 (𝑢𝑢)� ) in Eq. (4) de2  notes the sample-wise MSE. The combined objective function is to minimize the reconstruction errors while maximizing the STOI score.  Bandpass filter  High Frequency  BPF 1  Envelope detection RECT.  LPF  Preemphasis  X  Noise reduction  Y  X  Band-limiting BPF 1 +  White noise BPF N  Normal speech  Modulation  V1  RECT.  LPF  Divide subband  VN  X  BPF N +  White noise  Low Frequency  Energy normalize  Z EAS-vocoded speech  Energy normalize  Fig. 3. Structure of the vocoder to simulate EAS speech. The system can be divided into electric and acoustic path. The electric path (the upper side) is formed by an N-channel noise-vocoder, and the acoustic path (the lower path) is the low-pass filtered speech signals.  III.  VOCODED SPEECH  Vocoder (Voice Operated reCOrdER) is a voice processing system for analysing and resynthesizing human voice signals [1, 58]. Vocoder has been widely used for audio data compression, voice encryption and transmission, and voice modification. In addition, vocoder has a profound impact on CI research. Using vocoder, speech signals are processed to simulate the sound heard by CI users, and the simulations are presented to NH participants for listening tests for various purposes, such as predicting the general pattern of speech recognition performance of CI users [14, 63–65]. Research using vocoder simulations may solve patient-recruitment issues as well as avoid patient-specific confounding factors, such as neural surviving patterns [64, 66]. Noise-vocoded speech, in particular, has been used in many studies to simulate CI speech processing [14, 67, 68] and produce reliable results. With some modifications to a CI vocoder, an EAS vocoder can be realized. In previous studies, the EAS vocoder has been used to simulate EAS speech in order to evaluate various perspectives of performance. One is to compare the performance between EAS and conventional CI devices [8, 69]. Another is to examine the influence of different coding parameters of EAS on speech intelligibility and recognition performance [70]. The other, the same as this study, adopts the EAS vocoder to evaluate speech recognition under noisy conditions [71, 72]. In this study, we adopted an EAS vocoder with the structured shown in Fig. 3. The normal speech signals, 𝐗𝐗, first passes through an SE stage to obtain an enhanced speech signal, 𝐘𝐘. The enhanced signal is then processed through two paths, namely the acoustic and electric paths. For the acoustic path, the speech signal is directly processed by a low-pass filter (LPF). In this study, we used a Butterworth low-pass filter with a cut-off frequency of 500 Hz. The electric path is based on a standard CI noise vocoder that consists of pre-emphasis, band-pass filtering, envelope detection, modulation, and band-limiting stages. In the pre-emphasis stage, a 3dB/octave roll-off filter with a cut-off frequency of 2000 Hz is applied. In this study, four band-pass filters were used, i.e., N=4. With 4 band-pass filters, the emphasized signal is then divided into 4 frequency with cut-off frequencies of 500, 1017, 1901, and 3414 Hz. A full-wave rectifier is used to extract the 4-bands of temporal envelopes Vn (n=1,..,4) before the signals undergo a low-pass filter with a cut-off frequency of 400 Hz. The envelopes for all bands are then modulated with a set of white noise before further filtered by the same set of band-pass filters. Finally, the modulated sinewaves of the four bands and the filtered acoustic signal are summed, and the level of the combined signal is adjusted to produce a  root-mean-square value equal to the original input wideband signal. Finally, we obtained the EAS-vocoded speech (Z in Fig. 3). IV.  EXPERIMENTAL SETUP AND RESULTS  In this study, the evaluation focused on the ability of FCN(S) to improve speech intelligibility under training-testing mismatched conditions. More specifically, the training and testing utterances were prepared using different scripts, recorded by different speakers, and contaminated by different noise types. The speech corpus for evaluation consisted of 2,560 Mandarin utterances recorded by 8 native speakers (4 males and 4 females), each of whom recorded 320 utterances. The recording scripts were the Taiwan Mandarin hearing in noise test (TMHINT) [73]. Each sentence contained 10 Chinese characters, and the corresponding speech length was about 3-4 seconds. All utterances were recorded in a quiet environment with a sampling rate of 16 kHz. We selected the first 200 utterances of 6 speakers (3 males and 3 females) as the clean training data. The last 120 utterances of the remaining 2 speakers (1 male and 1 female) were used to prepare the testing set. One hundred noise types from a database of 100 nonspeech environmental sounds [74] were adopted to corrupt clean training utterances to produce signals with SNR from -10 to 20dB. We randomly selected 30,000 utterances for the training set. The engine and street noises (different from those used in the training set) were used to generate -3 and 1dB signals for the testing set. The performance of traditional MMSE and DDAE was tested for comparison. Two sets of experiments were conducted, including the evaluation of normal speech and the evaluation of vocoded speech. For normal speech, the STOI scores for normal wideband speech processed by the three SE approaches were reported. In many SE studies, the STOI score has been used as a standardized evaluation metric to measure the speech intelligibility. The range of the STOI score is from 0 to 1; the higher the STOI value, the better the speech intelligibility. As mentioned earlier, we also performed a listening test using an EAS vocoder. In the test, the utterances processed by the three SE approaches and the EAS vocoder were presented to the normal hearing participants. The speech recognition results for each SE approach were then measured based on the listening test.  A. Evaluation on Normal Speech The spectrograms of the two noise types, namely engine and street, are presented in Fig. 4. The spectrogram plot can show how the frequency patterns present in a sequential signal changes over time [75]. From the spectrograms, different characteristics of the engine (as stationary) and street (as nonstationary) noises can be easily observed. In the following, we first provide qualitative comparisons of different SE approaches using waveform and spectrogram plots. Then, we present the results of quantitative evaluation using the STOI scores.  Waveform  Spectrogram  (a)  (b)  (2) Fig. 4. Spectrogram plots of noise signals: (a) engine noise and (b) street noise. The two noise types were used to synthesize the noisy speech of the testing data. In both plots, the horizontal axis is the time in second, and the vertical axis is the frequency in kHz.  1) Waveform and Spectrogram Analyses on Normal Speech Waveform and spectrogram plots are commonly used to visually observe the characteristics of a time-varying signal series. The waveform plot can directly display the sample values along the time index and provide complimentary information to the spectrogram plot. Fig. 5 (c), (d), and (e) show the waveform and spectrogram plots of enhanced speech by the MMSE, DDAE, and FCN(S) SE methods under street noise at -3dB SNR, respectively. For comparison, the spectrogram and waveform plots for the clean and noisy speech signals are shown in Fig. 5(a) and (b), respectively. For a fair comparison, all the speech signals in Fig. 5 are normalized to zero-mean and unit-variance. By comparing the spectrogram plots, we observe that the three SE approaches exhibit different denoising characteristics. As shown in the spectrogram of Fig. 5(c), MMSE removed some of the high frequency components of the street noise signal but the mid- to low- frequency speech signal was considerably distorted. More specifically, when the speech and noise fell in the mid-low frequency region (please see the dashed box (1) in Fig. 5(c)), the MMSE showed limited noise reduction capability. Moreover, some speech components were removed (please see the dashed box (2) in Fig. 5(c)). On the other hand, although DDAE (cf. Fig. 5(d)) effectively removed the high frequency noise components, it overly-removed speech structures in the mid-low frequency region. More specifically, the mid-low frequency speech components that overlapped with the same frequency band of the noise (shown in the dashed box in Fig. 5(d)) were misjudged as noise and were removed. As for the FCN(S) method (cf. Fig. 5(e)), although some noise components remained, it maintained a clearer speech structure in the mid-low frequency region than MMSE and DDAE. The weakness of FCN(S) seemed to be dealing with the high frequency regions where the noise component was more notable than the DDAE result (please see the dashed box in Fig. 5(e)). Yet, it has been pointed out that the mid-low frequency region carries more speech intelligibility information [76], so it is more important to accurately restore this region when the goal is to maximize speech intelligibility. This argument has been discussed and confirmed experimentally in our previous study [52].  (c)  (1)  (d)  (e)  Fig. 5. Waveform and spectrogram plots of a testing utterance: (a) clean speech, (b) noisy speech (street noise; SNR at -3dB), enhanced speech by (c) MMSE, (d) DDAE, and (e) FCN(S). In the waveform and spectrogram plots, the horizontal axis is the time in second. In the waveform plots, the vertical axis is the normalized value; while in the spectrogram plots, the vertical axis is the frequency in kHz.  2) Objective Evaluation on Normal Speech To objectively evaluate the SE performance, we tested the STOI scores for the three SE approaches. The average STOI scores at six different SNR levels for engine and street noise types are demonstrated in Fig. 6(a) and (b), respectively. The results of unprocessed speech are denoted as “Noisy” in the following presentation. For the results of the engine noise, as shown in Fig. 6(a), the average STOI scores for {Noisy, MMSE, DDAE, FCN(S)} are {0.15, 0.17, 0.30, 0.31} at -11dB, {0.25, 0.28, 0.44, 0.49} at -7dB, {0.38, 0.41, 0.59, 0.64} at -3dB, {0.51, 0.54, 0.71, 0.73} at 1dB, {0.64, 0.67, 0.78, 0.79} at 5dB, and {0.73, 0.80, 0.81, 0.83} at 9dB. For the results of the street noise, as shown in Fig. 6(b), the average STOI scores for {Noisy, MMSE, DDAE, FCN(S)} are {0.20, 0.22, 0.37, 0.42} at -11dB, {0.32, 0.33, 0.54, 0.60} at -7dB, {0.44, 0.45, 0.66, 0.71} at -3dB, {0.55, 0.55, 0.75, 0.77} at 1dB, {0.65, 0.67, 0.80, 0.81} at 5dB, and {0.74, 0.77, 0.82, 0.85} at 9dB. The results in Fig. 6 demonstrate that FCN(S) achieved better STOI scores across different SNR levels than the other two SE approaches, regardless of the noise type. As shown in Fig. 6(a) and (b), although the conventional SE approach,  MMSE, achieved slightly better STOI scores than unprocessed noisy speech for stationary noise, it is generally ineffective for nonstationary noise. Compared to MMSE, both DDAE and the FCN(S) demonstrated notable improvement margins (with very similar patterns) across the six different SNR levels. However, it is evident that in lower SNR levels (-7dB, -3dB, and 1dB SNRs) FCN(S) outperformed DDAE. The results in Fig. 6 therefore suggest that FCN(S) can provide better speech intelligibility than DDAE under more challenging SNR conditions for both stationary and non-stationary noise types.  Amplitude Envelop  Spectrogram  (a)  (b)  (c)  (d)  (e)  Fig. 6. Average STOI scores at seven SNR levels for (a) engine and (b) street noises.  B. Evaluation on Vocoded Speech Next, we investigate the performance of the three SE approaches on EAS vocoded speech. Similarly, we first visually compare the amplitude envelop and spectrogram plots of the three SE approaches. Then, we present the recognition results of the subjective listening test. 1) Amplitude Envelop and Spectrogram Analyses on Vocoded Speech In addition to waveform and spectrogram plots, amplitude envelop plots are another useful tool for analyzing time-varying signals [77, 78]. Previous studies [63, 77, 78] have reported a positive correlation of modulation depth (of amplitude envelop) and speech intelligibility. It has also been shown that the middle frequency band is more important for speech intelligibility than the low- and high-frequency bands. Therefore, we examined the amplitude envelop of the second channel of the speech signal processed by the three different SE approaches. Fig. 7(c), (d), and (e), respectively, show the amplitude envelop and spectrogram plots of enhanced speech by MMSE, DDAE, and FCN(S) under street noise at -3dB SNR. For comparison, the amplitude envelop and spectrogram plots of the clean and noisy speech signals are shown in Fig. 7(a) and (b), respectively. The same as Fig. 5, we have nor-  Fig. 7. Spectrograms of a vocoded speech utterance: (a) clean speech, (b) noisy speech (street noise; SNR at -3dB), (c) MMSE enhancement, (d) DDAE enhancement, and (e) FCN(S) enhancement. In the waveform and spectrogram plots, the horizontal axis is for time in second. In the amplitude envelop plots, the vertical axis is for normalized value; in the spectrogram plots, the vertical axis is for frequency in kHz.  malized all the speech signals reported in Fig. 7 to be zero-mean and unit-variance for a clear comparison. The amplitude envelop plots provide another qualitative comparison of the three SE approaches. The modulation depth is associated with speech perception accuracy. As shown in Fig. 7(c), MMSE caused the loss of amplitude information, resulting in poor speech intelligibility. DDAE showed good SE performance; the envelop profile was clear but the modulation depth was slightly sacrificed. FCN(S) outperformed both MMSE and DDAE, showing a higher modulation depth, indicating better speech intelligibility. When comparing the spectrogram plots in Fig. 7, we note that the main trends in vocoded speech simulation are largely in line with those in normal speech (in Fig. 5). MMSE could not effectively reduce the noise components, as highlighted in the dotted box in Fig. 7(c). DDAE, as shown in Fig. 7(d), retained most of the speech structure, although some speech components were clearly overly-removed. Finally, FCN(S) preserved the speech structure well while effectively reducing  Fig. 8. Average speech recognition scores in terms of character correct rate (%) in 1 and -3dB SNR condition for (a) engine and (b) street noise masks. The error bars indicate one standard error of the mean (SEM).  the noise components, comparing the spectrogram plot in Fig. 7(e) with the spectrogram plots of the clean and noisy speech signals in Fig. 7(a) and (b). In the previous normal speech evaluation, FCN(S) showed weakness in dealing with high frequency noise. However, as shown in the dotted box in Fig. 7(e), the speech features in the high frequency region were less affected when using FCN(S), compared to the other two SE models. Based on the spectrograms in Fig. 7, we can predict that FCN(S) will provide better speech intelligibility, which is discussed in the next section. 2) Subjective Listening Test on Vocoded Speech The performances of the SE approaches (MMSE, DDAE, and FCN(S)) on EAS vocoded speech at two SNR levels (-3dB and 1dB) were evaluated by native Mandarin Chinese speakers, aged 18-39, with NH. To avoid listening fatigue, each participant experienced only one SNR level. Two groups of thirty participants, fifteen males and fifteen females, were recruited for the test. As with the objective evaluation, the test adopted the utterances in the TMHINT dataset; engine and street noise types were used to corrupt the utterances. That is, in the listening test, a participant was presented with eight test conditions: 1 SNR level (-1 or 3dB)  2 noise types (street and engine)  4 SE models (Noisy, MMSE, DDAE, and FCN(S)). Each condition contained ten utterances, consisted of ten Chinese characters. The original noisy speech (termed Noisy) was used as baseline for evaluation. Although increasing evidence tha"
"ALBERT: A L ITE BERT FOR S ELF - SUPERVISED L EARNING OF L ANGUAGE R EPRESENTATIONS Zhenzhong Lan1  Mingda Chen2∗  Sebastian Goodman1  Piyush Sharma1 1  Google Research  2  Kevin Gimpel2  Radu Soricut1  Toyota Technological Institute at Chicago  arXiv:1909.11942v1 [cs.CL] 26 Sep 2019  {lanzhzh, seabass, piyushsharma, rsoricut}@google.com {mchen, kgimpel}@ttic.edu   1  I NTRODUCTION  Full network pre-training (Radford et al., 2018; Devlin et al., 2019) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model. We also observe that simply growing the hidden size of a model such as BERT-large (Devlin et al., 2019) can lead to worse performance. Table 1 and Fig. 1 show a typical example, where we simply increase the hidden size of BERT-large to be 2x larger and get worse results with this BERT-xlarge model. ∗  Work done as an intern at Google Research, driving data processing and downstream task evaluations.  1  BERT-large BERT-xlarge  0  2  4  6 8 Steps (1e4)  10  Dev accuracy (MLM)  Training loss  9 8 7 6 5 4 3 2 1 0  0.6 0.5 0.4 0.3 0.2  0.0  12  BERT-large BERT-xlarge  0.1 0  2  4  6 8 Steps (1e4)  10  12  Figure 1: Training loss (left) and dev masked LM accuracy (right) of BERT-large and BERT-xlarge (2x larger than BERT-large in terms of hidden size). The larger model has lower masked LM accuracy while showing no obvious sign of over-fitting. Model BERT-large (Devlin et al., 2019) BERT-large (ours) BERT-xlarge (ours)  Hidden Size 1024 1024 2048  Parameters 334M 334M 1270M  RACE (Accuracy) 72.0% 73.9% 54.3%  Table 1: Increasing hidden size of BERT-large leads to worse performance on RACE. Existing solutions to the aforementioned problems include model parallelization (Shoeybi et al., 2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017). These solutions address the memory limitation problem, but not the communication overhead and model degradation problem. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture. ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT. As a result of these design decisions, we are able to scale up to much larger ALBERT configurations that still have fewer parameters than BERT-large but achieve significantly better performance. We establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, we push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2.  2 2.1  R ELATED WORK S CALING U P R EPRESENTATION L EARNING FOR NATURAL L ANGUAGE  Learning representations of natural language has been shown to be useful for a wide range of NLP tasks and has been widely adopted (Mikolov et al., 2013; Le & Mikolov, 2014; Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; 2019). One of the most significant changes in the last 2  two years is the shift from pre-training word embeddings, whether standard (Mikolov et al., 2013; Pennington et al., 2014) or contextualized (McCann et al., 2017; Peters et al., 2018), to full-network pre-training followed by task-specific fine-tuning (Radford et al., 2018; Devlin et al., 2019). In this line of work, it is often shown that larger model size improves performance. For example, Devlin et al. (2019) show that across three selected natural language understanding tasks, using larger hidden size, more hidden layers, and more attention heads always leads to better performance. However, they stop at a hidden size of 1024. We show that, under the same setting, increasing the hidden size to 2048 leads to model degradation and hence worse performance. Therefore, scaling up representation learning for natural language is not as easy as simply increasing model size. In addition, it is difficult to experiment with large models due to computational constraints, especially in terms of GPU/TPU memory limitations. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, we can easily hit memory limits. To address this issue, Chen et al. (2016) propose a method called gradient checkpointing to reduce the memory requirement to be sublinear at the cost of an extra forward pass. Gomez et al. (2017) propose a way to reconstruct each layer’s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed. In contrast, our parameter-reduction techniques reduce memory consumption and increase training speed.  2.2  C ROSS - LAYER PARAMETER SHARING  The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard transformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard transformer.  2.3  S ENTENCE O RDERING O BJECTIVES  ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday & Hasan, 1976; Grosz et al., 1995). Most objectives found effective in practice are quite simple. Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two. 3  3  T HE E LEMENTS OF ALBERT  In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture (Devlin et al., 2019). 3.1  M ODEL ARCHITECTURE CHOICES  The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H. Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64. There are three main contributions that ALBERT makes over the design choices of BERT. Factorized embedding parameterization. In BERT, as well as subsequent modeling improvements such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding size E is tied with the hidden layer size H, i.e., E ≡ H. This decision appears suboptimal for both modeling and practical reasons, as follows. From a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations. As experiments with context length indicate (Liu et al., 2019), the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent representations. As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs, which dictate that H  E. From a practical perspective, natural language processing usually require the vocabulary size V to be large.1 If E ≡ H, then increasing H increases the size of the embedding matrix, which has size V × E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training. Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H, we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from O(V × H) to O(V × E + E × H). This parameter reduction is significant when H  E. Cross-layer parameter sharing. For ALBERT, we propose cross-layer parameter sharing as another way to improve parameter efficiency. There are multiple ways to share parameters, e.g., only sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters. The default decision for ALBERT is to share all parameters across layers. We compare this design decision against other strategies in our experiments in Sec. 4.5. Similar strategies have been explored by Dehghani et al. (2018) (Universal Transformer, UT) and Bai et al. (2019) (Deep Equilibrium Models, DQE) for Transformer networks. Different from our observations, Dehghani et al. (2018) show that UT outperforms a vanilla Transformer. Bai et al. (2019) show that their DQEs reach an equilibrium point for which the input and output embedding of a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show that our embeddings are oscillating rather than converging. Figure 2 shows the L2 distances and cosine similarity of the input and output embeddings for each layer, using BERT-large and ALBERT-large configurations (see Table 2). We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT. These results show that weight-sharing has an effect on stabilizing network parameters. Although there is a drop for both metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows that the solution space for ALBERT parameters is very different from the one found by DQE. 1  Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000.  4  BERT-large ALBERT-large  0  5  10 15 Layer ID  Cosine Similarity (Degree)  L2 distance  18 16 14 12 10 8 6 4 2 0  20  25  45 40 35 30 25 20 15 10 5 0  BERT-large ALBERT-large  0  5  10 15 Layer ID  20  25  Figure 2: The L2 distances and cosine similarity (in terms of degree) of the input and output embedding of each layer for BERT-large and ALBERT-large. Inter-sentence coherence loss. In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classification loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning about the relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al., 2019) found NSP’s impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks. We conjecture that the main reason behind NSP’s ineffectiveness is its lack of difficulty as a task, as compared to MLM. As formulated, NSP conflates topic prediction and coherence prediction in a single task2 . However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss. We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about discourse-level coherence properties. As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at randombaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks. 3.2  M ODEL SETUP  We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 2. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models. For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. If we set BERT to have an extra-large size with H = 2048, we end up with a model that has 1.27 billion parameters and under-performs (Fig. 1). In contrast, an ALBERT-xlarge configuration with H = 2048 has only 59M parameters, while an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERT-large’s parameters. Note that for ALBERTxxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive. 2 Since a negative example is constructed using material from a different document, the negative-example segment is misaligned both from a topic and from a coherence perspective.  5  Model base large BERT xlarge base large ALBERT xlarge xxlarge  Parameters 108M 334M 1270M 12M 18M 59M 233M  Layers 12 24 24 12 24 24 12  Hidden 768 1024 2048 768 1024 2048 4096  Embedding 768 1024 2048 128 128 128 128  Parameter-sharing False False False True True True True  Table 2: The configurations of the main BERT and ALBERT models analyzed in this paper. This improvement in parameter efficiency is the most important advantage of ALBERT’s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail.  4  E XPERIMENTAL R ESULTS  4.1  E XPERIMENTAL S ETUP  To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the B OOK C ORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. We format our inputs as “[CLS] x1 [SEP] x2 [SEP]”, where x1 = x1,1 , x1,2 · · · and x2 = x1,1 , x1,2 · · · are two segments.3 We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%. Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019). We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly. The probability for the length n is given by 1/n p(n) = PN k=1 1/k We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as “White House correspondents”). All the model updates use a batch size of 4096 and a L AMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 1024, depending on model size. The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified. 4.2 4.2.1  E VALUATION B ENCHMARKS I NTRINSIC E VALUATION  To monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection. 4.2.2  D OWNSTREAM E VALUATION  Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), 3 A segment is usually comprised of more than one natural sentence, which has been shown to benefit performance by Liu et al. (2019).  6  two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.1. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. 4.3  OVERALL C OMPARISON BETWEEN BERT AND ALBERT  We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT’s design choices, as shown in Table 3: with only around 70% of BERT-large’s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.7%), SQuAD v2.0 (+4.2%), MNLI (+2.2%), SST-2 (+3.0%), and RACE (+8.5%). We also observe that BERT-xlarge gets significantly worse results than BERT-base on all metrics. This indicates that a model like BERT-xlarge is more difficult to train than those that have smaller parameter sizes. Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. The slowest one is the BERT-xlarge model, which we use as a baseline. As the models get larger, the differences between BERT and ALBERT models become bigger, e.g., ALBERT-xlarge can be trained 2.4x faster than BERT-xlarge. Model base BERT large xlarge base large ALBERT xlarge xxlarge  Parameters 108M 334M 1270M 12M 18M 59M 233M  SQuAD1.1 90.5/83.3 92.4/85.8 86.3/77.9 89.3/82.1 90.9/84.1 93.0/86.5 94.1/88.3  SQuAD2.0 80.3/77.3 83.9/80.8 73.8/70.5 79.1/76.1 82.1/79.0 85.9/83.1 88.1/85.1  MNLI 84.1 85.8 80.5 81.9 83.8 85.4 88.0  SST-2 91.7 92.2 87.8 89.4 90.6 91.9 95.2  RACE 68.3 73.8 39.7 63.5 68.4 73.9 82.3  Avg 82.1 85.1 76.7 80.1 82.4 85.5 88.7  Speedup 17.7x 3.8x 1.0 21.1x 6.5x 2.4x 1.2x  Table 3: Dev set results for models pretrained over B OOK C ORPUS and Wikipedia for 125k steps. Here and everywhere else, the Avg column is computed by averaging the scores of the downstream tasks to its left (the two numbers of F1 and EM for each SQuAD are first averaged). Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT. 4.4  FACTORIZED E MBEDDING PARAMETERIZATION  Table 4 shows the effect of changing the vocabulary embedding size E using an ALBERT-base configuration setting (see Table 2), using the same set of representative downstream tasks. Under the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by much. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the best. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary step to do further scaling. Model ALBERT base not-shared ALBERT base all-shared  E 64 128 256 768 64 128 256 768  Parameters 87M 89M 93M 108M 10M 12M 16M 31M  SQuAD1.1 89.9/82.9 89.9/82.8 90.2/83.2 90.4/83.2 88.7/81.4 89.3/82.3 88.8/81.5 88.6/81.5  SQuAD2.0 80.1/77.8 80.3/77.3 80.3/77.4 80.4/77.6 77.5/74.8 80.0/77.1 79.1/76.3 79.2/76.6  MNLI 82.9 83.7 84.1 84.5 80.8 81.6 81.5 82.0  SST-2 91.5 91.5 91.9 92.8 89.4 90.3 90.3 90.6  RACE 66.7 67.9 67.3 68.2 63.5 64.0 63.4 63.3  Avg 81.3 81.7 81.8 82.3 79.0 80.1 79.6 79.8  Table 4: The effect of vocabulary embedding size on the performance of ALBERT-base.  7  4.5  C ROSS - LAYER PARAMETER SHARING  Table 5 presents experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table 2) with two embedding sizes (E = 768 and E = 128). We compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and intermediate strategies in which only the attention parameters are shared (but not the FNN ones) or only the FFN parameters are shared (but not the attention ones). The all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears to come from sharing the FFN-layer parameters, while sharing the attention parameters results in no drop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg). Model all-shared shared-attention shared-FFN not-shared all-shared ALBERT shared-attention base shared-FFN E=128 not-shared ALBERT base E=768  Parameters 31M 83M 57M 108M 12M 64M 38M 89M  SQuAD1.1 88.6/81.5 89.9/82.7 89.2/82.1 90.4/83.2 89.3/82.3 89.9/82.8 88.9/81.6 89.9/82.8  SQuAD2.0 79.2/76.6 80.0/77.2 78.2/75.4 80.4/77.6 80.0/77.1 80.7/77.9 78.6/75.6 80.3/77.3  MNLI 82.0 84.0 81.5 84.5 82.0 83.4 82.3 83.2  SST-2 90.6 91.4 90.8 92.8 90.3 91.9 91.7 91.5  RACE 63.3 67.7 62.6 68.2 64.0 67.6 64.4 67.9  Avg 79.8 81.6 79.5 82.3 80.1 81.7 80.2 81.6  Table 5: The effect of cross-layer parameter-sharing strategies, ALBERT-base configuration. 4.6  S ENTENCE ORDER PREDICTION (SOP)  We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 6, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. SP tasks None NSP SOP  Intrinsic Tasks MLM NSP SOP 54.9 52.4 53.3 54.5 90.5 52.0 54.0 78.9 86.5  Downstream Tasks SQuAD2.0 MNLI SST-2 78.1/75.3 81.5 89.9 77.2/74.6 81.6 91.1 80.0/77.1 82.0 90.3  SQuAD1.1 88.6/81.5 88.4/81.5 89.3/82.3  RACE 61.7 62.3 64.0  Avg 79.0 79.2 80.1  Table 6: The effect of sentence-prediction loss, NSP vs. SOP, on intrinsic and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the “None” condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7  E FFECT OF NETWORK DEPTH AND WIDTH  In this section, we check how depth (number of layers) and width (hidden size) affect the performance of ALBERT. Table 7 shows the performance of an ALBERT-large configuration (see Table 2) using different numbers of layers. Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).4 If we compare a 3-layer ALBERT model with 4 If we compare the performance of ALBERT-large here to the performance in Table 3, we can see that this warm-start technique does not help to improve the downstream performance. However, it does help the 48-layer network to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from a 6-layer network.  8  a 1-layer ALBERT model, although they have the same number of parameters, the performance increases significantly. However, there are diminishing returns when continuing to increase the number of layers: the results of a 12-layer network are relatively close to the results of a 24-layer network, and the performance of a 48-layer network appears to decline. Number of layers 1 3 6 12 24 48  Parameters 18M 18M 18M 18M 18M 18M  SQuAD1.1 31.1/22.9 79.8/69.7 86.4/78.4 89.8/83.3 90.3/83.3 90.0/83.1  SQuAD2.0 50.1/50.1 64.4/61.7 73.8/71.1 80.7/77.9 81.8/79.0 81.8/78.9  MNLI 66.4 77.7 81.2 83.3 83.3 83.4  SST-2 80.8 86.7 88.9 91.7 91.5 91.9  RACE 40.1 54.0 60.9 66.7 68.7 66.9  Avg 52.9 71.2 77.2 81.5 82.1 81.8  Table 7: The effect of increasing the number of layers for an ALBERT-large configuration. A similar phenomenon, this time for width, can be seen in Table 8 for a 3-layer ALBERT-large configuration. As we increase the hidden size, we get an increase in performance with diminishing returns. At a hidden size of 6144, the performance appears to decline significantly. We note that none of these models appear to overfit the training data, and they all have higher training and development loss compared to the best-performing ALBERT configurations. Hidden size 1024 2048 4096 6144  Parameters 18M 59M 223M 497M  SQuAD1.1 79.8/69.7 83.3/74.1 85.0/76.4 84.7/75.8  SQuAD2.0 64.4/61.7 69.1/66.6 71.0/68.1 67.8/65.4  MNLI 77.7 79.7 80.3 78.1  SST-2 86.7 88.6 90.4 89.1  RACE 54.0 58.2 60.4 56.0  Avg 71.2 74.6 76.3 74.0  Table 8: The effect of increasing the hidden-layer size for an ALBERT-large 3-layer configuration. 4.8  W HAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME ?  The speed-up results in Table 3 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 9, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). Models BERT-large ALBERT-xxlarge  Steps 400k 125k  Time 34h 32h  SQuAD1.1 93.5/87.4 94.0/88.1  SQuAD2.0 86.9/84.3 88.3/85.3  MNLI 87.8 87.8  SST-2 94.6 95.4  RACE 77.3 82.5  Avg 87.2 88.7  Table 9: The effect of controlling for training time, BERT-large vs ALBERT-xxlarge configurations. After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.9  D O VERY WIDE ALBERT MODELS NEED TO BE DEEP ( ER ) TOO ?  In Section 4.7, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a 24-layer configuration is small. Does this result still hold for much wider ALBERT configurations, such as ALBERT-xxlarge (H=4096)? Number of layers 12 24  SQuAD1.1 94.0/88.1 94.1/88.3  SQuAD2.0 88.3/85.3 88.1/85.1  MNLI 87.8 88.0  SST-2 95.4 95.2  RACE 82.5 82.3  Avg 88.7 88.7  Table 10: The effect of a deeper network using an ALBERT-xxlarge configuration. 9  The answer is given by the results from Table 10. The difference between 12-layer and 24-layer ALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg score being the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer configuration. 4.10  A DDITIONAL TRAINING DATA AND DROPOUT EFFECTS  The experiments done up to this point use only the Wikipedia and B OOK C ORPUS datasets, as in (Devlin et al., 2019). In this section, we report measurements on the impact of the additional data used by both XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019). Fig. 3a plots the dev set MLM accura"
"Low-Resource Response Generation with Template Prior  †  Ze Yang† , Wei Wu♦ , Jian Yang† , Can Xu♦ , Zhoujun Li†∗ State Key Lab of Software Development Environment, Beihang University, Beijing, China ♦ Microsoft Corporation, Beijing, China {tobey, jiaya, lizj}@buaa.edu.cn {wuwei, caxu}@microsoft.com  arXiv:1909.11968v1 [cs.CL] 26 Sep 2019    1  Introduction  Human-machine conversation is a long-standing goal of artificial intelligence. Early dialogue systems are designed for task completion with conversations restricted in specific domains (Young et al., 2013). Recently, thanks to the advances in deep learning techniques (Sutskever et al., 2014; Vaswani et al., 2017) and the availability of large amounts of human conversation on the internet, building an open domain dialogue system with data-driven approaches has become the new fashion in the research of conversational AI. Such dia∗  Corresponding Author  logue systems can generate reasonable responses without any needs on rules, and have powered products in the industry such as Amazon Alexa (Ram et al., 2018) and Microsoft XiaoIce (Shum et al., 2018). State-of-the-art open domain response generation models are based on the encoder-decoder architecture (Vinyals and Le, 2015; Shang et al., 2015). On the one hand, with proper extensions to the vanilla structure, existing models now are able to naturally handle conversation contexts (Serban et al., 2016; Xing et al., 2018), and synthesize responses with various styles (Wang et al., 2017), emotions (Zhou et al., 2018), and personas (Li et al., 2016a). On the other hand, all the existing success of open domain response generation builds upon an assumption that the large scale of paired data (Shao et al., 2016) or conversation sessions (Sordoni et al., 2015) are available. In this work, we challenge this assumption by arguing that one cannot always obtain enough pairs or sessions for training a neural generation model. For example, although it has been indicated by existing work (Li et al., 2016b; Wang et al., 2018) that question asking in conversation can enhance user engagement, we find that in a public dataset1 with 5 million conversation sessions crawled from Weibo, only 7.3% sessions have a question response and thus can be used to learn a question generator for responding2 . When we attempt to generate responses that express positive sentiment, we only get 360k pairs (18%) with positive responses from a dataset with 2 million messageresponse pairs crawled from Twitter. Indeed, existing big conversation data mix various intentions, styles, emotions, personas, and so on. Thus, 1 http://tcci.ccf.org.cn/conference/ 2018/dldoc/trainingdata05.zip 2 Questions are detected with the rules in (Wang et al., 2018).  we have to face the data sparsity problem, as long as we attempt to create a generation model with constraints on responses. In this work, we jump out of the paradigm of learning from large scale paired data3 , and investigate how to build a response generation model with only a few pairs at hand. Aside from the paired data, we assume that there are a large number of unpaired data available. The assumption is reasonable since it is much easier to get questions or sentences with positive sentiment than to get such responses paired with messages. We formalize the problem as low-resource response generation from paired and unpaired data, which is less explored by existing work. Since the paired data are insufficient for learning the mapping from a message to a response, the challenge of the task lies in how to effectively leverage the unpaired data to enhance the learning on the paired data. Our solution to the challenge is a two-stage approach where we first distill templates from the unpaired data and then use them to guide response generation. Targeting on an unsupervised approach to template learning, we propose representing the templates as a neural hidden semimarkov model (NHSMM) estimated through maximizing the likelihood of the unpaired data. Such latent templates encode both semantics and syntax of the unpaired data and then are used as prior in an encoder-decoder architecture for modeling the paired data. With the latent templates, the whole model is end-to-end learnable and can perform response generation in an explainable manner. To ensure the relevance of responses regarding input messages and at the same time make full use of the templates, we propose learning the generation model with an adversarial approach. Empirical studies are conducted on two tasks: question response generation and sentiment response generation. For the first task, we exploit the dataset published in (Wang et al., 2018) and augment the data with questions crawled from Zhihu4 . For the second task, we build a paired dataset from Twitter by filtering responses with an off-theshelf sentiment classifier and augment the dataset with tweets in positive sentiment extracted from a large scale tweet dataset published in (Cheng et al., 2010). Evaluation results on both auto3 The study in this work starts from response generation for single messages. One can easily extend the proposed approach to handle conversation history. 4 https://en.wikipedia.org/wiki/Zhihu  matic metrics and human judgment indicate that with limited message-response pairs, our model can significantly outperform several state-of-theart response generation models. The source code is available online. 5 Our contributions in this work are three-folds: (1) proposal of low-resource response generation with paired and unpaired data for open domain dialogue systems; (2) proposal of encoder-decoder with template prior; and (3) empirical verification of the effectiveness of the model with two largescale datasets.  2  Related Work  Inspired by neural machine translation, early work applies the sequence-to-sequence with attention model (Shang et al., 2015) to open domain response generation, and gets promising results. Later, the basic architecture is extended to suppress generic responses (Li et al., 2015; Zhao et al., 2017; Xing et al., 2017); to model the structure of conversation contexts (Serban et al., 2016); and to incorporate different types of knowledge into generation (Li et al., 2016a; Zhou et al., 2018). In addition to model design, how to learn a generation model (Li et al., 2016c, 2017), and how to evaluate the models (Liu et al., 2016; Lowe et al., 2017; Tao et al., 2018), are drawing attention in the community of open domain dialogue generation. In this work, we study how to learn a response generation model from limited pairs, which breaks the assumption made by existing work. We propose response generation with paired and unpaired data. As far as we know, this is the first work on low-resource response generation for open domain dialogue systems. Traditional template-based text generation (Becker, 2002; Foster and White, 2004; Gatt and Reiter, 2009) relies on handcrafted templates that are expensive to obtain. Recently, some work explores how to automatically mine templates from plain text and how to integrate the templates into neural architectures to enhance interpretability of generation. Along this line, Duan et al. (2017) mine patterns from related questions in community QA websites and leverage the patterns with a retrieval-based approach and a generation-based approach for question generation. Wiseman et al. (2018) exploit a hidden semi-markov model for joint template extraction and text generation. In 5  https://github.com/TobeyYang/S2S_Temp  addition to structured templates, raw text retrieved from indexes is also used as “soft templates” in various natural language generation tasks (Guu et al., 2018; Pandey et al., 2018; Cao et al., 2018; Peng et al., 2019). In this work, we leverage templates for open domain response generation. Our idea is inspired by (Wiseman et al., 2018), but latent templates estimated from one source are transferred to another source in order to handle the low-resource problem, and the generation model is learned by an adversarial approach rather than by maximum likelihood estimation. Before us, the low-resource problem has been studied in tasks such as machine translation (Gu et al., 2018b,a), pos tagging (Kann et al., 2018), word embedding (Jiang et al., 2018), automatic speech recognition (Tüske et al., 2014), taskoriented dialogue systems (Tran and Nguyen, 2018; Mi et al., 2019), etc. In this work, we pay attention to low-resource open domain response generation which is untouched by existing work. We propose attacking the problem with unpaired data, which is related to the effort in low-resource machine translation with monolingual data (Gulcehre et al., 2015; Sennrich et al., 2015; Zhang and Zong, 2016). Our method is unique in that rather than using the unpaired data through multitask learning (Zhang and Zong, 2016) or backtranslation (Sennrich et al., 2015), we extract linguistic knowledge from the data as latent templates and use the templates as prior in generation.  3  Low-Resource Response Generation  In this section, we first formalize the setting upon which we study low-resource response generation and then elaborate the model of response generation with paired and unpaired data, including how to learn latent templates from the unpaired data, and how to perform generation with the templates. 3.1  Problem Formalization  Suppose that we have a dataset DP = {(Xi , Yi )}ni=1 , where ∀i, (Xi , Yi ) is a pair of message-response, and n represents the number of pairs in DP . Different from existing work, we assume that n is small (e.g., a few hundred thousands) and further assume that there is another set DU = {Ti }N i=1 with Ti a piece of plain text sharing the same characteristics with {Yi }ni=1 (e.g., both are questions) and N > n. Our goal is to learn a generation probability P (Y |X) with both DP and  DU . Thus, given a new message X, we can generate a response Y for X following P (Y |X). Since the limited resource in DP may not support accurately learning of P (Y |X), we try to transfer the linguistic knowledge in DU to response generation. The challenges then lie in two aspects: (1) how to represent the linguistic knowledge in DU ; and (2) how to effectively leverage the knowledge extracted from DU for response generation, given that DU cannot provide any information of correspondence between a message X and a response Y . The remaining part of the section will describe our solutions to the two problems. 3.2  Learning Templates from DU  In the representation of the knowledge in DU , we hope that both semantic information and syntactic information can be kept. Thus, we consider extracting templates from DU as the knowledge. A template segments a piece of text as a structured representation. With the templates, semantically and functionally similar text segments are grouped together. Since the templates encode the structure of language in DU , they can inform the generation model about how to express a response in a desired way (e.g., as a question or with the specific sentiment). Here, we prefer an unsupervised and parametric approach to learning templates, since “unsupervised” means that the approach is generally applicable to various tasks, and “parameteric” allows us to naturally incorporate the templates into the generation model. Then, a natural choice for template learning is the neural hidden semi-markov model (NHSMM) (Dai et al., 2016; Wiseman et al., 2018). NHSMM is an HSMM parameterized with neural networks. HSMM (Murphy, 2002) extends HMM by allowing a hidden state to emit a sequence of observations and thus can segment a piece of text with the latent variables and group similar segments by the variables. Formally, given an observed sequence Y = (y1 , . . . , yS ), the joint distribution of Y and its segmentation is 0  S Y t=1  0  P (zt+1 , lt+1 |zt , lt )  S Y  P (yi(t−1)+1:i(t) |zt , lt ),  t=1  where zt ∈ {1, . . . , K} is the hidden state for step t, lt ∈ {1, . . . , D} is the duration variable for zt that representsPthe number of tokens emitt ted by zt , i(t) = j=1 lj with i(0) = 0 and 0 i(S ) = S, and yi(t−1)+1:i(t) is the sequence  of (yi(t−1)+1 , . . . , yi(t) ). P (zt+1 , lt+1 |zt , lt ) is factorized as P (zt+1 |zt ) × P (lt+1 |zt+1 ) where P (lt+1 |zt+1 ) is a uniform distribution on {1, . . . , D} and P (zt+1 |zt ) can be viewed as a transition matrix [A(i, j)]K×K which is defined by A(i, j) , P (zt+1 = j|zt = i) exp(e> j ei + bi,j ) = PK , > o=1 exp(eo ei + bi,o ) where ei , ej , eo ∈ Rd1 are embeddings of state i, j, o respectively, and bi,j , bi,o are scalar bias terms. In practice, we set bi,j = −∞ ⇔ i = j to disable self-transition, because the adjacent states play different syntactic or semantic roles in a desired template. The emission distribution P (yi(t−1)+1:i(t) |zt , lt ) is defined by P (yi(t−1)+1:i(t) |zt , lt ) = P (yi(t−1)+1 |zt , lt ) ×  lt Y  P (yi(t−1)+j |yi(t−1)+j−1 , zt , lt ),  j=2  and parameterized with a recurrent neural network with gated recurrent unit (GRU) (Cho et al., 2014). The hidden vector for position j is formulated as otj =GRUH (otj−1 , [ezt ; eyi(t−1)+j−1 ]) vjt =gzt  otj ,  (1)  where otj ∈ Rd2 , eyi(t−1)+j−1 ∈ Rd3 is the embedding of word yi(t−1)+j−1 , [·; ·] is a concatenation operator, refers to element-wise multiplication, and gzt ∈ Rd2 is a gate (in total, there are K gate vectors as parameters). P (yi(t−1)+j |yi(t−1)+j−1 , zt , lt ) is then defined by softmax(W1 vjt + b1 ), where W1 ∈ RV ×d2 and b1 ∈ Rd2 are parameters with V the vacabulary size. Following Murphy (2002), the marginal distribution of Y can be obtained by the backward algorithm which is formulated as βt (i) , P (yt+1:S |qt = i) =  K X  βt∗ (j)A(i, j)  j=1  βt∗ (j)  , P (yt+1:S |qt+1 = j) D h i X = βt+d (j)P (d|j)P (yt+1:t+d |j, d) d=1  P (Y ) =  K X  β0∗ (j)P (q1 = j).  j=1  (2)  where qt is the hidden state of the t-th word in Y , and the base cases βS (i) = 1, ∀i ∈ {1, . . . , K}. Specifically, to learn more reasonable segmentations, we parsed every sentence by stanford parser (Manning et al., 2014) and forced NHSMM not to break syntactic elements such as VP and NP, etc. The parameters of the NHSMM are estimated by maximizing the log-likelihood of DU through backpropagation. 3.3  Response Generation with Template Prior  We propose incorporating the templates parameterized by the NHSMM learned from DU into response generation as prior. Figure 1 illustrates the architecture of the generation model. In a nutshell, the model first samples a chain of states with duration as a template. The template specifies a segmentation of the response to generate. Then, the hidden representations of the segments defined by Equation (1) are fed to an encoder-decoder architecture for response generation, where the hidden states of the decoder are calculated with both attention over the hidden states of the input message given by the encoder and the hidden representations of the segments given by the template prior. The template prior acts as a base and assists the encoder-decoder in response generation regarding to an input message, when paired information is insufficient for learning the correspondence between a message and a response. Note that similar to the conditional variational autoencoder (CVAE) (Zhao et al., 2017), our model also exploits hidden variables for response generation. The difference is that the hidden variables in our model are structured and learned from extra resources, and thus encode more semantic and syntactic information. Specifically, we segment responses in DP with Viterbi algorithm (Zucchini et al., 2016), collect all chains of states as a pool and sample a chain from the pool uniformly. We do not sample states according to the transition matrix [A(i, j)]K×K , since it is difficult to determine the end of a chain. Suppose that the sampled chain is (z1 , . . . zS ), then ∀1 ≤ t ≤ S, we sample an lt for zt according to P (lt |zt ), and finally form a latent template (< z1 , l1 >, . . . < zS , lS >). Given a message X = (x1 , . . . , xL ), the encoder exploits a GRU to transform X into a hidden sequence HX = (hX,1 , . . . , hX,L ) with the i-th hidden state  State Transition Duration  GRUX  Decoder Input  GRUH  𝑧1  𝑧2  𝑧3  𝑙1 = 2  𝑙2 = 3  𝑙3 = 1  [ 𝑠 ; 𝑧1 ] [It; 𝑧1 ]  [′s; 𝑧2 ][a; 𝑧2 ][brilliant; 𝑧2 ] [mov𝑖𝑒; 𝑧3 ]  GRUY  What  do  you  think  of Interstellar ?  a  ’s  It  brilliant movie  !!  Figure 1: The architecture of the generation model.  hX,i ∈ Rd2 given by hX,i = GRUX (hX,i−1 , exi ), where exi ∈ Rd3 is the embedding of word xi and hX,0 = 0. Then when predicting the t-th word of the response, the decoder calculates the probability P (yt |y1:t−1 , X, T ) via P (yt |y1:t−1 , X, T ) =softmax(W2 [st ; ct ] + b2 ) st =GRUY (st−1 , vkm ). with parameters W2 ∈ RV ×2d2 and b2 ∈ RV , st ∈ Rd2 and st−1 ∈ Rd2 are the hidden states of the decoder for step t and step t − 1 respectively, vkm is defined by Equation (1) where m satisfies i(m − 1) < t ≤ i(m), k = t − i(m − 1), and m d2 is om k = GRUH (ok−1 , [ezm ; eyt−1 ]), and ct ∈ R a context vector of X obtained via attention over HX (Bahdanau et al., 2015): ct =  L X  αt,i hX,i ,  i=1  exp(st,i ) αt,i = PL j=1 exp(st,j )  sufficiently supervised by the pairs. In that case, the linguistic knowledge in DU will play a more important role in response generation and result in irrelevant responses regarding to messages; or (2) if we let the training go deep, then the template prior will be overwhelmed by the pairs in DP . As a result, the generation model will lose the knowledge obtained from DU . Since response generation starts from a latent template, we consider learning the model with an adversarial approach (Goodfellow et al., 2014) that can well balance the effect of the latent template and the input message. The learning involves a generator G described in Section 3 and a discriminator D. G is updated with REINFORCE algorithm (Williams, 1992) with rewards defined by D, and D is updated to distinguish human responses in DP from responses generated by G. Generator Pre-training. To improve the stability of adversarial learning, we first pre-train G with MLE on DP . ∀(Xi , Yi ) ∈ DP , the template prior Ti is obtained by running Viterbi algorithm (Zucchini et al., 2016) on Yi rather than by sampling. Let Yi = (yi,1 , . . . , yi,Si ), then the objective of pre-training is given by  st,i = v > tanh(W st + U hi + b), where v, b ∈ Rd2 , W, U ∈ Rd2 ×d2 are parameters.  4  Learning Approach  Intuitively, we can estimate the parameters of the encoder-decoder and fine-tune the parameters of NHSMM by maximizing the likelihood of DP (i.e., MLE). However, since DP only contains a few pairs, the MLE approach may suffer from a dilemma: (1) if we stop training early, then both the template prior and the encoder-decoder are not  1 n  X  Si X  log P (yi,t |yi,1:t−1 , Xi , Ti ). (3)  (Xi ,Yi )∈DP t=1  Discriminator Update. The discriminator D is defined by a convolutional neural network (CNN) based binary classifier (Kim, 2014). D takes a message-response pair as input and outputs a score that indicates how likely the response is from humans. In the model, the message and the response are separately embedded as vectors by CNNs, and then the concatenation of the two vectors are fed to  a 2-layer MLP to calculate the score. Let Ŷi be the response generated by G for Xi , then D is updated by maximizing the following objective: X log D(Xi , Yi ) + log(1 − D(Xi , Ŷi )) (Xi ,Yi )∈DP  (4) Generator Update. The generator G is updated by the policy gradient method (Yu et al., 2017; Li et al., 2017). Let ŷ1:t be a partial response generated by G from beam search for message X until step t, then we adopt the Monte Carlo (MC) search method and sample N paths that supplement ŷ1:t as responses {Ŷi }N reward for i=1 . The intermediate 1 PN ŷ1:t is defined as Rt = N i=1 D(X, Ŷi ). The gradient for updating G is given by ∇J(θ) ≈  S X    ∇ log P (yˆt |ŷ1:t−1 , X, T ) · Rt ,  t=1  (5) where θ represents the parameters of G, and T is a sampled template. To control the quality of MC search, we sample from top 50 most probable words at each step. The learning algorithm is summarized in Algorithm 1. Note that in learning of the generation model from DP , we freeze the embedding of states (i.e., ezt in Equation (1)) and the embedding of words given by the NHSMM, and update all other parameters in generator pre-training and the following adversarial learning.  5  Experiments  We test the proposed approach on two tasks: question response generation and sentiment response generation. The first task requires a model to generate a question as a response to a given message; while in the second task, as a showcase, responses should express the positive sentiment. 5.1  Experiment Setup  Datasets. For the question response generation task, we choose the data published in (Wang et al., 2018) as the paired dataset. The data are obtained by filtering 9 million message-response pairs mined from Weibo with 20 handcrafted question templates and are split as a training set, a validation set, and a test set with 481k, 5k, and 5k pairs respectively. In addition to the paired data, we crawl 776k questions from Zhihu, a Chinese community QA website featured by high-quality  Algorithm 1 Learning a generation model with paired and unpaired data. Require: NHSMM H, generator G, discriminator D, DU , and DP . 1: Initialize H, G, D. 2: Learn H from DU according to Equation (2). 3: Pre-train G using MLE on DP . 4: Pre-train D using G according to Equation (4). 5: repeat 6: for g-steps do 7: Sample (X, Y ) from DP . 8: Sample a template T for (X, Y ) with H. 9: Generate Ŷ = (ŷ1 , . . . , ŷS ) ∼ G(·|X, T ). 10: for t in 1 : S do 11: MC search and compute reward Rt using D. 12: end for 13: Update G on (X, Ŷ ) via Equation (5). 14: end for 15: for d-steps do 16: Sample (X, Y ) from DP . 17: Sample a template T for (X, Y ) with H. 18: Generate Ŷ ∼ G(·|X, T ) and pair (X, Ŷ ) with (X, Y ). 19: Update D by Equation (4). 20: end for 21: until convergence  content, as an unpaired dataset. Both datasets are tokenized by Stanford Chinese word segmenter6 . We keep 20, 000 most frequent words in the two data as a vocabulary for the encoder, the decoder, and the NHSMM. The vocabulary covers 95.8% words appearing in the messages, in the responses, and in the questions. Other words are replaced with “UNK”. For the sentiment response generation task, we mine 2 million message-response pairs from Twitter FireHose, filter responses with the positive sentiment using Stanford Sentiment Annotator toolkit (Socher et al., 2013), and obtain 360k pairs as a paired dataset. As pre-processing, we remove URLs and usernames, and transform each word to its lower case. After that, the data is split as a training set, a validation set, and a test set with 350k, 5k, and 5k pairs respectively. Besides, we extract 1 million tweets with positive sentiment from a public corpus (Cheng et al., 2010) as an unpaired dataset. Top 20, 000 most frequent words in the two data are kept as a vocabulary that covers 99.3% words. Words excluded from the vocabulary are treated as “UNK”. In both tasks, human responses in the test sets are taken as ground truth for automatic metric calculation. From each test set, we randomly sample 500 distinct messages and recruit human annotators to judge the quality of responses generated for these messages. 6  https://stanfordnlp.github.io/CoreNLP  Evaluation Metrics. We conduct evaluation with both automatic metrics and human judgements. For automatic evaluation, besides BLEU1 (Papineni et al., 2002) and Rouge-L (Lin, 2004), we follow (Serban et al., 2017) and employ Emebedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) as metrics. All these metrics are computed by a popular NLG evaluation project available at https://github.com/Maluuba/ nlg-eval. In terms of human evaluation, for each task, we recruit 3 well-educated native speakers as annotators, and let them compare our model and each of the baselines. Every time, we show an annotator a message (in total 500) and two responses, one from our model and the other from a baseline model. Both responses are top 1 results in beam search, and the two responses are presented in random order. The annotator then compare the two responses from three aspects: (1) Fluency: if the response is fluent without grammatical error; (2) Relevance: if the response is relevant to the given message; and (3) Richness: if the response contains informative and interesting content, and thus may keep conversation going. For each aspect, if the annotator cannot tell which response is better, he/she is asked to label a “tie”. Each pair of responses receive 3 labels on each of the three aspects, and agreements among the annotators are measured by Fleiss’ kappa (Fleiss and Cohen, 1973). 5.2  Baselines  We compare our model with the following baselines: (1) Seq2Seq: the basic sequence-tosequence with attention architecture (Bahdanau et al., 2015). (2) CVAE: the conditional variational autoencoder that represents the relationship between messages and responses with latent variables (Zhao et al., 2017). We use the code published at https://github.com/ snakeztc/NeuralDialog-CVAE. (3) HTD: the hard typed decoder model proposed in (Wang et al., 2018) that exhibits the best performance on the dataset selected by this work for question response generation. The model estimates distributions over three types of words (i.e., interrogative, topic, and ordinary) and modulates the final distribution during generation. Since our experiments are conducted on the same data as those in (Wang et al., 2018), we run the code shared  BLEU-1 ROUGE-L AVERAGE EXTREME GREEDY Seq2Seq CVAE HTD  0.037 0.094 0.073  0.111 0.088 0.103  0.656 0.685 0.647  0.438 0.414 0.425  0.456 0.422 0.439  S2S-Temp-MLE S2S-Temp-None S2S-Temp-50% S2S-Temp  0.097 0.069 0.091 0.102  0.119 0.092 0.113 0.128  0.699 0.677 0.702 0.710  0.438 0.429 0.442 0.451  0.457 0.416 0.461 0.469  Table 1: Automatic evaluation results for the task of question response generation. Numbers in bold mean that the improvement over the best performing baseline is statistically significant (t-test with p-value< 0.01). BLEU-1 ROUGE-L AVERAGE EXTREME GREEDY Seq2Seq CVAE ECM  0.065 0.088 0.051  0.118 0.081 0.102  0.726 0.727 0.708  0.474 0.408 0.462  0.582 0.563 0.559  S2S-Temp-MLE S2S-Temp-None S2S-Temp-50% S2S-Temp  0.103 0.078 0.102 0.106  0.124 0.089 0.121 0.130  0.732 0.687 0.691 0.738  0.458 0.479 0.491 0.492  0.593 0.501 0.586 0.603  Table 2: Automatic evaluation results for the task of sentiment response generation. Numbers in bold mean that the improvement over the best performing baseline is statistically significant (t-test, with p-value< 0.01).  at https://github.com/victorywys/ Learning2Ask_TypedDecoder with the default setting. (4) ECM: emotional chatting machine proposed in (Zhou et al., 2018). We implement the model with the code published at https://github.com/tuxchow/ecm. Since the model can handle various emotions, we train the model with the entire 2 million Twitter message-response pairs labeled with a positive, negative, and neutral sentiment. Thus, when we only focus on responses with positive sentiment, ECM actually performs multi-task learning for response generation. In the test, we set the sentiment label as “positive”. We name our model S2S-Temp. Besides the full model, we also examine three variants in order to understand the effect of unpaired data and the role of adversarial learning: (1) S2S-TempNone. The proposed model is trained only with the paired data, where the NHSMM is estimated from responses in the paired data; (2) S2S-Temp50%. The proposed model is trained with 50% unpaired data; and (3) S2S-Temp-MLE. The pretrained generator described in Section 4. These variants are only involved in automatic evaluation. 5.3  Implementation Details  In both tasks, we set the number of states (i.e., K) and the the maximal number of emissions (i.e., D) in NHSMM as 50 and 4 respectively. d1 , d2 and  Models  W(%)  Fluency L(%) T(%)  W(%)  Relevance L(%) T(%)  W(%)  Richness L(%) T(%)  Kappa  S2S-Temp vs. Seq2Seq S2S-Temp vs. CVAE S2S-Temp vs. HTD  20.8 41.7 35.1  18.3 5.7 19.2  60.9 52.6 45.8  30.8 50.8 30.8  22.5 12.5 25.1  46.7 36.7 44.1  42.5 37.5 37.5  19.2 15.8 30.8  38.3 46.7 31.7  0.63 0.71 0.64  S2S-Temp vs. Seq2Seq S2S-Temp vs. CVAE S2S-Temp vs. ECM  15.6 48.4 27.1  11.5 9.0 12.3  72.9 42.6 60.6  34.4 31.9 36.9  17.2 5.7 13.9  48.4 62.4 49.2  31.9 31.4 27.9  7.3 8.2 10.6  60.8 60.4 61.5  0.68 0.69 0.78  Table 3: Human annotation results. W, L, and T refer to Win, Lose and Tie respectively. The first three rows are results on question response generation, and the last three rows are results on sentiment response generation. The ratios are calculated by combining labels from the three judges.  d3 are set as 600, 300, and 300 respectively. In adversarial learning, we use three types of filters with window sizes 1, 2 and 3 in the discriminator. The number of filters is 128 for each type. The number of samples obtained from MC search (i.e., N ) at each step is 5. We learn all models using Adam algorithm (Kingma and Ba, 2015), monitor perplexity on the validation sets, and terminate training when perplexity gets stable. In our model, learning rates for NHSMM, the generator, and the discriminator are set as 1 × 10−3 , 1 × 10−5 , and 1 × 10−3 respectively. 5.4  Evaluation Results  Table 1 and Table 2 report the results of automatic evaluation on the two tasks. We can see that on both tasks, S2S-Temp outperforms all baseline models in terms of all metrics, and the improvements are statistically significant (t-test with p-value< 0.01). The results demonstrate that when only limited pairs are available, S2STemp can effectively leverage unpaired data to enhance the quality of response generation. Although lacking fine-grained check, from the comparison among S2S-Temp-None, S2S-Temp-50%, and S2S-Temp, we can conclude that the performance of S2S-Temp improves with more unpaired data. Moreover, without unpaired data, our model is even worse than CVAE since the structured templates cannot be accurately estimated from such a few data, and as long as half of the unpaired data are available, the model outperforms the baseline models on most metrics. The results further verified the important role the unpaired data plays in learning of a response generation model from low resources. S2S-Temp is better than S2S-TempMLE, indicating that the adversarial learning approach can indeed enhance the relevance of re-  sponses regarding to messages. Table 3 shows the results of human evaluation. In terms of all the three aspects, S2S-Temp is better than all the baseline models. The values of kappa are all above 0.6, indicating substantial agreement among the annotators. When the size of paired data is small, the basic Seq2Seq model tends to generate more generic responses. That is why the gap between S2S-Temp and Seq2Seq is much smaller on fluency than those on the other two aspects. With the latent variables, CVAE brings both content and noise into responses. Therefore, the gap between S2S-Temp and CVAE is more significant on fluency and relevance than that on richness. HTD can greatly enrich the content of responses, which is consistent with the results in (Wang et al., 2018), although sometimes the responses might be irrelevant to messages or ill-formed. ECM does not perform well on both automatic evaluation and human judgement. 5.5  Case Study  To further understand how S2S-Temp leverages templates for response generation, we show two examples with the test data, one for question response generation in Table 4 and the other for sentiment response generation in Table 5, where subscripts refer to states of the NHSMMs. First, we can see that a template defines a structure for a response. By varying templates, we can have responses with different syntax and semantics for a message. Second, some states may have consistent functions across responses. For example, state 36 in question response generation may refer to pronouns, and “I’m” and “it was” correspond to the same state 23 in sentiment response generation. Finally, some templates provide strong syntactic signals to response generation. For example, the  Message:  真的假的？我瘦了16斤 Really? I lost 17.6 pounds  Responses:  [你]36 [瘦 了]31 [吗 ?]13 [You]36 [lost weight]31 [?]13  References  [你 是 怎么]14 [做到 的]42 [?]26 [How do you]14 [make it]42 [?]26 [真的 吗 ?]48 [我]36 [不 信]32 [Really ?]48 [I]36 [don’t believe it]32  Table 4: Question response generation with various templates. Message:  "
"Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation  arXiv:1909.11974v1 [cs.CL] 26 Sep 2019  Ze Yang† , Can Xu♦ , Wei Wu♦ , Zhoujun Li†∗ † State Key Lab of Software Development Environment, Beihang University, Beijing, China ♦ Microsoft Corporation, Beijing, China {tobey, lizj}@buaa.edu.cn {wuwei, caxu}@microsoft.com  1  Introduction  Online news commenting allows Internet users to express their opinions, show their attitudes, and communicate with each other under the news articles they read. The commentaries extend the content of the articles and improve user engagement of the news websites by encouraging users to browse the comment stream, to share new information, and to debate with one another. With the prevalence of online news articles with comments, it is of great interest to build an automatic news commenting system with data-driven approaches. Such systems can enable commenting service for a news website from cold start, enhance the reading experience for less commented news articles, and enrich skill lists of other artificial intelligence applications, such as chatbots (Shum et al., 2018). In this work, we study the problem of automatic news comment generation, which is a less ∗  Corresponding Author  Comment A: If it’s heavily based on the 2018 WC, hence England leaping up the rankings, how are Brazil at 3? Comment B: England above Spain, Portugal and Germany. Interesting.  Table 1: A news example from Yahoo!  explored task in the literature of natural language generation (Gatt and Krahmer, 2018). Existing work does some preliminary studies, where a comment is generated either from the title of a news article only (Zheng et al., 2018; Qin et al., 2018) or by feeding the entire article (title plus body) to a basic sequence-to-sequence (s2s) model with an attention mechanism (Qin et al., 2018). News titles are short and succinct, and thus only using news titles may lose quite a lot of useful information in comment generation. On the other hand, a news article and a comment is not a pair of parallel text. The news article is much longer than the comment and contains much information that is irrelevant to the comment. Thus, directly applying the s2s model, which has proven effective in machine translation (Bahdanau et al., 2015), to the task of news comment generation is unsuitable, and may bring a lot of noise to generation. Both approaches oversimplify the problem of news comment generation and are far from how people behave on news websites. In practice, people read a news article, draw attention to some  points in the article, and then present their comments along with the points they are interested in. Table 1 illustrates news commenting with an example from Yahoo! News.1 The article is about the new FIFA ranking, and we pick two comments among the many to explain how people behave in the comment section. First, both commenters have gone through the entire article, as their comments are built upon the details in the body. Second, the article gives many details about the new ranking, but both commenters only comment on a few points. Third, the two commenters pay their attention to different places of the article: the first one notices that the ranking is based on the result of the new world cup, and feels curious about the position of Brazil; while the second one just feels excited about the new position of England. The example indicates a “read-attend-comment” behavior of humans and sheds light on how to construct a model that is close to the real practice. To mimic human behavior, we propose a reading network and a generation network that generate a comment from the entire news article. The reading network simulates how people digest a news article, and acts as an encoder of the article. The generation network then simulates how people comment the article after reading it, and acts as a decoder of the comment. Specifically, from the bottom to the top, the reading network consists of a representation layer, a fusion layer, and a prediction layer. The first layer represents the title of the news with a recurrent neural network with gated recurrent units (RNN-GRUs) (Cho et al., 2014) and represents the body of the news through selfattention which can model long-term dependency among words. The second layer forms a representation of the entire news article by fusing the information of the title into the representation of the body with an attention mechanism and a gate mechanism. The attention mechanism selects useful information in the title, and the gate mechanism further controls how much such information flows into the representation of the article. Finally, the third layer is built on top of the previous two layers and employs a multi-label classifier and a pointer network (Vinyals et al., 2015) to predict a bunch of salient spans (e.g., words, phrases, and sentences, etc.) from the article. With the reading network, our model comprehends the news arti1  https://www.yahoo.com/news/ fifa-rankings-france-number-one-112047790. html  cle and boils it down to some key points (i.e., the salient spans). The generation network is an RNN language model that generates a comment word by word through an attention mechanism (Bahdanau et al., 2015) on the selected spans and the news title. In training, since salient spans are not explicitly available, we treat them as a latent variable, and jointly learn the two networks from articlecomment pairs by optimizing a lower bound of the true objective through a Monte Carlo sampling method. Thus, training errors in comment prediction can be back-propagated to span selection and used to supervise news reading comprehension. We conduct experiments on two large scale datasets. One is a Chinese dataset published recently in (Qin et al., 2018), and the other one is an English dataset built by crawling news articles and comments from Yahoo! News. Evaluation results on the two datasets consistently indicate that our model can significantly outperform existing methods on both automatic metrics and human judgment. The Yahoo! News corpus and our source code are available online.2 Our contributions are four-folds: (1) proposal of “read-attend-comment” procedure for news comment generation with a reading network and a generation network; (2) joint optimization of the two networks with an end-to-end learning approach; (3) empirical verification of the effectiveness of the proposed model on two public datasets with both automatic metrics and human annotations; and (4) release of an English news commenting dataset to the research community.  2  Related Work  News comment generation is a sub-task of natural language generation (NLG). Among various NLG tasks, the task studied in this paper is most related to summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017) and product review generation (Tang et al., 2016; Dong et al., 2017). However, there is stark difference between news comment generation and the other two tasks: the input of our task is an unstructured document, while the input of product review generation is structured attributes of a product; and the output of our task is a comment which often extends the content of the input with additional information, while the output of summarization is a condensed version of the input that contains the main infor2  https://github.com/TobeyYang/DeepCom  mation from the original. Very recently, there emerge some studies on news comment generation. For example, Zheng et al. (2018) propose a gated attention neural network model to generate news comments from news titles. The model is further improved by a generative adversarial net. Qin et al. (2018) publish a dataset with results of some basic models. Different from all the existing methods, we attempt to comprehend the entire news articles before generation and perform endto-end learning that can jointly optimize the comprehension model and the generation model. Our model is partially inspired by the recent success of machine reading comprehension (MRC), whose prosperity can be attributed to an increase of publicly available large scale annotated datasets, such as SQuAD (Rajpurkar et al., 2016, 2018) and MS Marco (Nguyen et al., 2016) etc. A great number of models have been proposed to tackle the MRC challenges, including BiDAF (Seo et al., 2016), r-net (Wang et al., 2017), DCN (Xiong et al., 2016), Document Reader (Chen and Bordes, 2017), QANet (Yu et al., 2018), and s-net (Tan et al., 2018) etc. Our work can be viewed as an application of MRC to a new NLG task. The task aims to generate a comment for a news article, which is different from existing MRC tasks whose goal is to answer a question. Our learning method is also different from those in the MRC works.  3 3.1  Approach Problem Formalization  Suppose we have a dataset D = {(Ti , Bi , Ci )}N i=1 , where the i-th triple (Ti , Bi , Ci ) consists of a news title Ti , a news body Bi , and a comment Ci . Our goal is to estimate a probability distribution P (C|T, B) from D, and thus, given a new article (T, B) with T the news title and B the news body, we can generate a comment C following P (C|T, B). 3.2  Model Overview  Figure 1 illustrates the architecture of our model. In a nutshell, the model consists of a reading network and a generation network. The reading network first represents a news title and a news body separately in a representation layer, then forms a representation of the entire article by fusing the title into the body through a fusion layer, and finally distills some salient spans from the article by a prediction layer. The salient spans and the news  title are then fed to the generation network to synthesize a comment. With the two networks, we can factorize the generation probability P (C|T, B) as P (S|T, B) · P (C|S, T ), where S = (s1 , . . . , sw ) refers to a set of spans in B, P (S|T, B) represents the reading network, and P (C|S, T ) refers to the generation network. 3.3  Reading Network  In the representation layer, let T = (t1 , . . . , tn ) be a news title with tj the j-th word, and B = (b1 , . . . , bm ) be the associated news body with bk the k-th word, we first look up an embedding table and represent tj and bk as eT,j ∈ Rd1 and eB,k ∈ Rd1 respectively, where eT,j and eB,k are randomly initialized, and jointly learned with other parameters. Different from the title, the body is long and consists of multiple sentences. Hence, to emphasize positional information of words in the body, we further expand eB,k with oB,k and sB,k , where oB,k , sB,k ∈ Rd2 are positional embeddings with the former indexing the position of bk in its sentence and the latter indicating the position of the sentence in the entire body. The representation of bk is then given by êB,k = MLP([eB,k ; oB,k ; sB,k ]), where êB,k ∈ Rd1 , MLP(·) refers to a multi-layer perceptron with two layers, and [·; ·; ·] means the concatenation of the three arguments. Starting from ET = (eT,1 , . . . , eT,n ) and EB = (êB,1 , . . . , êB,m ) as initial representations of T and B respectively, the reading network then transforms T into a sequence of hidden vectors HT = (hT,1 , . . . , hT,n ) with a recurrent neural network with gated recurrent units (RNNGRUs) (Cho et al., 2014). In the meanwhile, B is transformed to HB = (hB,1 , . . . , hB,m ) with the k-th entry hB,k ∈ Rd1 defined as MLP([êB,k ; cB,k ]) (two layers) and cB,k is an attention-pooling vector calculated by a scaled dot-product attention (Vaswani et al., 2017) denoted as dot-att(EB , êB,k ): cB,k =  Pm  αk,j êB,j , P αk,j = exp(sk,j )/ m l=1 exp(sk,l ), √ > sk,j = (êB,k êB,j )/ d1 . j=1  (1)  In Equation (1), a word is represented via all words in the body weighted by their similarity. By this means, we try to capture the dependency among words in long distance. The fusion layer takes HT and HB as inputs, and produces V = (v1 , . . . , vm ) as new represen-  Generation network  Reading network Prediction Layer  v1  v2  v3  Comment  Gate  Fusion Layer  Weighted Sum  Representation Layer  cT ,1  cT ,2  cT ,3  hB ,1  hB ,2  hB ,3  cB,2  cB ,1  c1  c2  h1  ht −1  S  c1  S ,t  ct ht  T ,t  ct −−11  cB,3 hT ,1  hT ,2  hT ,3  hT ,n  t1  t2  t3  tn  Embedding  b2  b1  b3  Body  Title  Figure 1: Architecture of our model. The black solid arrows represent differentiable operations and the dashed arrows are non-differentiable operations which represent distilling points from news body.  tation of the entire news article by fusing HT into HB . Specifically, ∀hB,k ∈ HB , we first let hB,k attend to HT and form a representation cT,k = dot-att(HT , hB,k ), where dot-att(HT , hB,k ) is parameterized as Equation (1). With this step, we aim to recognize useful information in the title. Then we combine cT,k and hB,k as vk ∈ Rd1 with a gate gk which balances the impact of cT,k and hB,k and filters noise from cT,k . vk is defined by: vk = hB,k + gk cT,k , gk = σ(Wg [hB,k ; cT,k ]),  (2)  The top layer of the reading network extracts a bunch of salient spans based on V. Let S = ((a1 , e1 ), . . . , (aw , ew )) denote the salient spans, where ai and ei refer to the start position and the end position of the i-th span respectively, we propose detecting the spans with a multi-label classifier and a pointer network. Specifically, we formalize the recognition of (a1 , . . . , aw ) as a multilabel classification problem with V as an input and L = (l1 , . . . , lm ) as an output, where ∀k ∈ {1, . . . , m}, lk = 1 means that the k-th word is a start position of a span, otherwise lk = 0. Here, we assume that (a1 , . . . , aw ) are independent with each other, then the multi-label classifier can be defined as m binary classifiers with the k-th classifier given by ŷk = softmax(MLPk (vk )),  (3)  where MLPk (·) refers to a two-layer MLP, and ŷk ∈ R2 is a probability distribution with the  first entry as P (lk = 0) and the second entry as P (lk = 1). The advantage of the approach is that it allows us to efficiently and flexibly detect a variable number of spans from a variable-length news article, as there is no dependency among the m classifiers, and they can be calculated in parallel. Given ak , the end position ek is recognized via a probability distribution (αak ,1 , . . . , αak ,m ) which is defined by a pointer network: αak ,j = exp(sak ,j )/  Pm  l=1  exp(sak ,l ),  >  sak ,j = V tanh(Wv vj + Wh hak ,1 ), hak ,1 = GRU(h0 , [c0 ; vak ]),  (4)  where h0 = att(V, r) is an attention-pooling vector based on parameter r: h0 =  Pm  j=1  βj · vj , Pm  βj = exp(βj0 )/ βj0  =  l=1  exp(βl0 ),  V1> tanh(Wv,1 vj  (5)  + Wh,1 r),  c0 = att(V, h0 ) is defined in a similar way with r replaced by h0 . Let us denote (a1 , . . . , aw ) as start and P (li = 1) as pi , then P (S|T, B) can be formulated as P (S|T, B) =  w Y  [pak · αak ,ek ]  k=1  Y  [1 − pi ]. (6)  i∈start,1≤i≤m /  In practice, we recognize the i-th word as a start position if pi > 1 − pi , and determine the associated end position by arg max1≤k≤m αi,k . Note that we do not adopt a pointer network to detect the start positions, because in this case, either we  have to set a threshold on the probability distribution, which is sensitive to the length of the news article and thus hard to tune, or we can only pick a fixed number of spans for any articles by ranking the probabilities. Neither of them is favorable. 3.4  Generation Network  L=  With S = ((a1 , e1 ), . . . , (aw , ew )) the salient spans, V = (v1 , . . . , vm ) the representation of the news article, and HT the representation of the news title given by the three layers of the reading network respectively, we define a representation of S as HS = (va1 , va1 +1 , . . . , ve1 , . . . , vaw , vaw +1 , . . . , vew ). The generation network takes HT and HS as inputs and decodes a comment word by word via attending to both HT and HS . At step t, the hidden state is ht = GRU(ht−1 , [eC,t−1 ; CT,t−1 ; CS,t−1 ]), ht ∈ Rd1 . eC,t−1 is the embedding of the word generated at step t-1, CT,t−1 = att(HT , ht−1 ) and CS,t−1 = att(HS , ht−1 ) are context vectors that represent attention on the title and the spans respectively. att(·, ·) is defined as Equation (5). With ht , we calculate CT,t and CS,t via att(HT , ht ) and att(HS , ht ) respectively and obtain a probability distribution over vocabulary by Pt = softmax(V [ht ; CT,t ; CS,t ] + b). Let C = (c1 , . . . , co ) be a comment where ∀k ∈ {1, . . . , o}, ck is the index of the k-th word of C in vocabulary, then P (C|S, T ) is defined as P (C|S, T ) = P (c1 |S, T )  o Y  P (ct |c1 , . . . , ct−1 , S, T )  t=2  = P1 (c1 )  o Y  Pt (ct ),  t=2  where Pt (ct ) refers to the ct -th entry of Pt . In decoding, we define the initial state h0 as an attention-pooling vector over the concatenation of HT and HS given by att([HT ; HS ], q). q is a parameter learned from training data. 3.5  Learning Method  We aim to learn P (S|T, B) and P (C|S, T ) from D = {(Ti , Bi , Ci )}N i=1 , but S is not explicitly available, which is a common case in practice. To address the problem, we treat S as a latent variable, and consider the following objective: J =  N X  log P (Ci |Ti , Bi )  i=1  =  N X i=1  (7) log   X Si ∈S   P (Si |Ti , Bi )P (Ci |Si , Ti ,  where S refers to the space of sets of spans, and Si is a set of salient spans for (Ti , Bi ). Objective J is difficult to optimize, as logarithm is outside the summation. Hence, we turn to maximizing a lower bound of Objective J which is defined as: N X X  P (Si |Ti , Bi ) log P (Ci |Si , Ti ) < J  (8)  i=1 Si ∈S  Let Θ denote all parameters of our model and ∂Li ∂Θ denote the gradient of L on an example i (Ti , Bi , Ci ), then ∂L ∂Θ is given by X Si ∈S  P (Si |Ti , Bi )  h ∂ log P (C |S , T ) i i i + ∂Θ  (9)  ∂ log P (Si |Ti , Bi ) i log P (Ci |Si , Ti ) . ∂Θ  To calculate the gradient, we have to enumerate all possible Si s for (Ti , Bi ), which is intractable. Thus, we employ a Monte Carlo sampling method i to approximate ∂L ∂Θ . Suppose that there are J sami ples, then the approximation of ∂L ∂Θ is given by J 1 X h ∂ log P (Ci |Si,n , Ti ) + J n=1 ∂Θ  (10)  ∂ log P (Si,n |Ti , Bi ) i log P (Ci |Si,n , Ti ) , ∂Θ  where ∀n, Si,n is sampled by first drawing a group of start positions according to Equation (3), and then picking the corresponding end positions by Equations (4)-(5). Although the Monte Carlo estimator is unbiased, it typically suffers from high variance. To reduce variance, we subtract baselines from log P (Ci |Si,n , Ti ). Inspired by (Mnih and Gregor, 2014), we introduce an observationdependent baseline Bψ (Ti , Ci ) to capture the systematic difference in news-comment pairs during training. Besides, we also exploit a global baseline B to further control the variance of the estimator. i The approximation of ∂L ∂Θ is then re-written as J 1 X h ∂ log P (Ci |Si,n , Ti )  + log P (Ci |Si,n , Ti ) J n=1 ∂Θ  ∂ log P (S |T , B ) i i,n i i − Bψ (Ti , Ci ) − B . ∂Θ (11)  To calculate Bψ (Ti , Ci ), we first encode the word sequences of Ti and Ci with GRUs respectively, and then feed the last hidden states of the GRUs to a three-layer MLP. B is calculated as an average of P (Ci |Si,n , Ti ) − Bψ (Ti , Ci ) over the current mini-batch. The parameters  of the GRUs and the MLP are estimated via min(Ti ,Bi )∈mini-batch,1≤n≤J [log P (Ci |Si,n , Ti ) − Bψ (Ti , Ci ) − B]2 . The learning algorithm is summarized in Algorithm 1. To speed up convergence, we initialize our model through pre-training the reading network and the generation network. Specifically, ∀(Ti , Bi , Ci ) ∈ D, we construct an artificial span set S̃i , and learn the parameters of the two networks by maximizing the following objective: N X  log P (S̃i |Ti , Bi ) + log P (Ci |Ti , S̃i )  (12)  i=1  S̃i is established in two steps: first, we collect all associated comments for (Ti , Bi ), extract n-grams (1 ≤ n ≤ 6) from the comments, and recognize an n-gram in Bi as a salient span if it exactly matches with one of the n-grams of the comments. Second, we break Bi as sentences and calculate a matching score for a sentence and an associated comment. Each sentence corresponds to a group of matching scores, and if any one of them exceeds 0.4, we recognize the sentence as a salient span. The matching model is pre-trained with {Ti , Ci }N i=1 with Ci as a positive example and a randomly sampled comment from other news as a negative example. In the model, Ti and Ci are first processed by GRUs separately, and then the last hidden states of the GRUs are fed to a three-layer MLP to calculate a score. Algorithm 1: Optimization Algorithm  1 2 3 4 5 6 7 8 9 10 11  4  Input: training data D, initial learning rate lr, MaxStep, sample number n. Init: Θ Construct {S̃i }N i=1 and pre-train the model by maximizing Objective (12). while step < MaxStep do Randomly sample a mini-batch k from D. Compute distributions of start positions. for n < J do Sample start positions. Compute distributions of end positions. Sample end positions. Compute the terms related to Si,n in Eq. (11). Compute Bψ (Ci , Ti ) and B and finish Eq. (11). Update the parameters of the model and Bψ (Ci , Ti ) with SGD. Output: Θ  Experiments  We test our model on two large-scale news commenting datasets.  Train  Dev  Test  Tencent  # News Avg. # Cmts per News  191,502 27  5,000 27  1,610 27  Yahoo  # News Avg. # Cmts per News  152,355 20.6  5,000 20.5  3,160 20.5  Table 2: Statistics of the two datasets.  4.1  Experimental Setup  The first dataset is a Chinese dataset built from Tencent News (news.qq.com) and published recently in (Qin et al., 2018). Each data point contains a news article which is made up of a title and a body, a group of comments, and some side information including upvotes and categories. Each test comment is labeled by two annotators according to a 5-scale labeling criteria presented in Table 3. All text in the data is tokenized by a Chinese word segmenter Jieba (https: //github.com/fxsjy/jieba). The average lengths of news titles, news bodies, and comments are 15 words, 554 words and 17 words respectively. In addition to the Chinese data, we also build another dataset by crawling news articles and the associated comments from Yahoo! News. Besides upvotes and categories, side information in Yahoo data also includes paragraph marks, WIKI-entities, downvotes, abusevotes, and sentiment tagged by Yahoo!. Text in the data is tokenized by Stanford CoreNLP pipline (Manning et al., 2014). As pre-processing, we filter out news articles shorter than 30 words in the body and comments shorter than 10 words or longer than 100 words. Then, we remove news articles with less than 5 comments. If the number of comments of an article exceeds 30, we only keep top 30 comments with the most upvotes. On average, news titles, news bodies, and comments contain 12 words, 578 words and 32 words respectively. More information about Yahoo data can be found in Appendix A. After the pre-processing, we randomly sample a training set, a validation set, and a test set from the remaining data, and make sure that there is no overlap among the three sets. Table 2 summarizes the statistics of the two datasets. Note that we only utilize news titles, news bodies and comments to learn a generation model in this work, but both datasets allow modeling news comment generation with side information, which could be our future work. Following (Qin et al., 2018), we evaluate the  Score 5 4 3 2 1  Criteria Rich in content; attractive; deep insights; new yet relevant viewpoints Highly relevant with meaningful ideas Less relevant; applied to other articles Fluent/grammatical; irrelevant Hard to read; Broken language; Only emoji  Table 3: Human judgment criteria  a title and a body (Att-TC). In Seq2seq, Att, and Att-TC, top 1 comment from beam search (beam size=5) is returned. GANN: the gated attention neural network proposed in (Zheng et al., 2018). The model is further improved by a generative adversarial net. We denote our model as “DeepCom” standing for “deep commenter”, as it is featured by a deep reading-commenting architecture. All baselines are implemented according to the details in the related papers and tuned on the validation sets.  performance of different models with both automatic metrics and human judgment. In terms of automatic evaluation, we employ BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE (Lin, 2004), and CIDEr (Vedantam et al., 2015) as metrics on both data. Besides these metrics, Qin et al. (2018) propose human score weighted metrics including W-BLEU, WMETEOR, W-ROUGE and W-CIDEr. These metrics, however, requires human judgment on each comment in the test set. Thus, we only involve results w.r.t. these metrics in Tencent data. As Qin et al. (2018) do not publish their code for metric calculation, we employ a popular NLG evaluation project available at https://github.com/ Maluuba/nlg-eval, and modify the scripts with the scores provided in the data according to the formulas in (Qin et al., 2018) to calculate all the metrics. In human evaluation, for each dataset, we randomly sample 500 articles from the test data and recruit three native speakers to judge the quality of the comments given by different models. For every article, comments from all models are pooled, randomly shuffled, and presented to the annotators. Each comment is judged by the three annotators under the criteria in Table 3.  For each dataset, we form a vocabulary with the top 30k frequent words in the entire data. We pad or truncate news titles, news bodies, and comments to make them in lengths of 30, 600, and 50 respectively. The dimension of word embedding and the size of hidden states of GRU in all models are set as 256. In our model, we set d1 as 256 and d2 (i.e., dimension of the position embedding in the reading network) as 128. The size of hidden layers in all MLPs is 512. The number of samples in Monte Carlo sampling is 1. In pre-training, we initialize our model with a Gaussian distribution N (0, 0.01) and optimize Objective (12) using AdaGrad (Duchi et al., 2011) with an initial learning rate 0.15 and an initial accumulator value 0.1. Then, we optimize L using stochastic gradient descent with a learning rate 0.01. In decoding, top 1 comment from beam search with a size of 5 is selected for evaluation. In IR-T and IR-TC, we use three types of filters with window sizes 1, 3, and 5 in the CNN based matching model. The number of each type of filters is 128.  4.2  4.4  Baselines  The following models are selected as baselines: Basic models: the retrieval models and the generation models used in (Qin et al., 2018) including (1) IR-T and IR-TC: both models retrieve a set of candidate articles with associated comments by cosine of TF-IDF vectors. Then the comments are ranked by a convolutional neural network (CNN) and the top position is returned. The difference is that IR-T only utilizes titles, while IR-TC leverages both titles and news bodies; (2) Seq2seq: the basic sequence-to-sequence model (Sutskever et al., 2014) that generates a comment from a title; and (3) Att and Att-TC: sequence-to-sequence with attention (Bahdanau et al., 2015) in which the input is either a title (Att) or a concatenation of  4.3  Implementation Details  Evaluation Results  Table 4 reports evaluation results in terms of both automatic metrics and human annotations. On most automatic metrics, DeepCom outperforms baseline methods, and the improvement is statistically significant (t-test with p-value < 0.01). The improvement on BLEU-1 and W-BLEU-1 is much bigger than that on other metrics. This is because BLEU-1 only measures the proportion of matched unigrams out of the total number of unigrams in the generated comments. In human evaluation, although the absolute numbers are different from those reported in (Qin et al., 2018) due to the difference between human judgements, the overall trend is consistent. In human evaluation, the values of Fleiss’ kappa over all models are more  Dataset  Models  METEOR  W-METEOR  Rouge L  W-Rouge L  CIDEr  W-CIDEr  BLEU-1  W-BLEU-1  Human  Kappa  Tencent  IR-T IR-TC Seq2seq Att Att-TC GANN DeepCom  0.107 0.127 0.064 0.080 0.114 0.097 0.181  0.086 0.101 0.047 0.058 0.082 0.075 0.138  0.254 0.266 0.196 0.246 0.299 0.282 0.317  0.217 0.225 0.150 0.186 0.223 0.222 0.250  0.018 0.056 0.011 0.010 0.023 0.010 0.029  0.014 0.044 0.008 0.007 0.017 0.008 0.023  0.495 0.474 0.374 0.481 0.602 0.312 0.721  0.470 0.436 0.320 0.453 0.551 0.278 0.656  2.43 2.57 1.68 1.81 2.26 2.06 3.58  0.64 0.71 0.83 0.79 0.69 0.73 0.65  Yahoo  IR-T IR-TC Seq2seq Att Att-TC GANN DeepCom  0.114 0.117 0.061 0.075 0.089 0.079 0.107  -  0.214 0.219 0.203 0.217 0.246 0.228 0.263  -  0.014 0.017 0.011 0.017 0.022 0.019 0.024  -  0.472 0.483 0.365 0.462 0.515 0.496 0.665  -  2.71 2.86 2.26 2.29 2.74 2.52 3.35  0.67 0.61 0.68 0.78 0.63 0.64 0.68  Table 4: Evaluation results on automatic metrics and human judgment. Human evaluation results are calculated by combining labels from the three judges. “Kappa” means Fleiss’ kappa. Numbers in bold mean that improvement over the best baseline is statistically significant. Dataset  Metrics  No Reading  No Prediction  No Sampling  Full Model  Tencent  METEOR W-METEOR Rouge L W-Rouge L CIDEr W-CIDEr BLEU-1 W-BLEU-1  0.096 0.072 0.282 0.219 0.012 0.009 0.426 0.388  0.171 0.129 0.307 0.241 0.024 0.019 0.674 0.614  0.171 0.131 0.303 0.239 0.026 0.021 0.667 0.607  0.181 0.138 0.317 0.250 0.029 0.023 0.721 0.656  Yahoo  METEOR Rouge L CIDEr BLEU-1  0.081 0.232 0.017 0.490  0.092 0.245 0.023 0.531  0.102 0.244 0.020 0.609  0.107 0.263 0.024 0.665  Table 5: Model ablation results  than 0.6, indicating substantial agreement among the annotators. Although built in a complicated structure, GANN does not bring much improvement over other baseline methods, which demonstrates that only using news titles is not enough in comment generation. IR-TC and Att-TC represent the best retrieval model and the best generation model among the baselines on both datasets, implying that news bodies, even used in a simple way, can provide useful information to comment generation. 4.5  Discussions  Ablation study: We compare the full model of DeepCom with the following variants: (1) No Reading: the entire reading network is replaced by a TF-IDF based keyword extractor, and top 40 keywords (tuned on validation sets) are fed to the generation network; (2) No Prediction: the prediction layer of the reading network is removed, and thus the entire V is used in the generation network; and (3) No Sampling: we directly use the model pre-trained by maximizing Objective (12). Table 5 reports the results on automatic metrics.  We can see that all variants suffer from performance drop and No Reading is the worst among the three variants. Thus, we can conclude that (1) span prediction cannot be simply replaced by TFIDF based keyword extraction, as the former is based on a deep comprehension of news articles and calibrated in the end-to-end learning process; (2) even with sophisticated representations, one cannot directly feed the entire article to the generation network, as comment generation is vulnerable to the noise in the article; and (3) pre-training is useful, but optimizing the lower bound of the true objective is still beneficial. To further understand why DeepCom is superior to its variants, we calculate BLEU-1 (denoted as BLEUspan ) with the predicted spans and the ground truth comments in the test sets of the two data, and compare it with a baseline BLEU-1 (denoted as BLEUbase ) which is calculated with the entire news articles and the ground truth comments. On Tencent data, BLEUspan and BLEUbase are 0.31 and 0.17 respectively, and the two numbers on Yahoo data are 0.29 and 0.16 respectively. This means that by extracting salient spans from news articles, we can filter out redundant information while keeping the points that people like to comment on, which explains why DeepCom is better than No Prediction. When comparing DeepCome with No Sampling, we find that spans in DeepCom are longer than those in No Sampling. In the test set of Tencent data, the average lengths of salient spans with and without sampling are 11.6 and 2.6 respectively, and the two numbers in Yahoo data are 14.7 and 2.3 respectively. Thus, DeepCom can leverage discourse-level informa-  1  2  3  4  5  Tencent  IR-TC Att-TC DeepCom  0.3% 19.3% 1.5%  55.2% 49.5% 3.3%  33.3% 18.5% 32.0%  9.50% 11.7% 61.9%  1.7% 1.0% 1.3%  Yahoo  IR-TC Att-TC DeepCom  1.3% 20.3% 1.7%  53.0% 22.3% 17.6%  14.7% 22.6% 33.3%  20.3% 32.1% 39.4%  10.7% 2.7% 8.0%  Table 6: Human score distributions  tion rather than a few single words or bi-grams in comment generation, which de"
"Spoken Conversational Search for General Knowledge Lina M. Rojas-Barahona, Pascal Bellec, Benoit Besset, Martinho Dos-Santos, Johannes Heinecke, Munshi Asadullah, Olivier Le-Blouch, Jean Y. Lancien, Géraldine Damnati, Emmanuel Mory and Frédéric Herledan Orange Labs, 2 Avenue de Pierre Marzin, Lannion, France {linamaria.rojasbarahona,pascal.bellec,benoit.besset,martinho.dossantos, johannes.heinecke,munshi.asadullah,olivier.leblouch,jeanyves.lancien geraldine.damnati,emmanuel.mory,frederic.herledan}@orange.com  arXiv:1909.11980v1 [cs.AI] 26 Sep 2019   Introduction  Conversational question answering is an open research problem. It studies the integration of question answering (QA) systems in a dialogue system(DS). Not long ago, each of these research subjects were studied separately; only very recently has studying the intersection between them gained increasing interest (Reddy et al., 2018; Choi et al., 2018). We present a spoken conversational question answering system that is able to answer questions about general knowledge in French by calling two distinct QA systems. It solves coreference and ellipsis by modelling context. Furthermore, it is extensible, thus other components such as neural approaches for question-answering can be easily integrated. It is also possible to collect a dialogue corpus from its iterations. In contrast to most conversational systems which support only speech, two input and output modalities are supported speech and text. Thus it is possible to let the user check the answers by either asking relevant Wikipedia excerpts or by navigating through the retrieved name entities or by exploring the answer details of the QA components: the confidence score as well as the set of explored triplets. Therefore, the user has the final word to consider the answer as correct or incorrect and to 1  https://www.wikidata.org  provide a reward, which can be used in the future for training reinforcement learning algorithms.  2  Architectural Description  The high-level architecture of the proposed system consists of a speech-processing frontend, an understanding component, a context manager, a generation component, and a synthesis component. The context manager provides contextualised mediation between the dialogue components and several question answering back-ends, which rely on data provided by Wikidata1 . Interaction with a human user is achieved through a graphical user interface (GUI). Figure 1 depicts the components together with their interactions.  Figure 1: High-level depiction of the proposed spoken conversation question answering system. Arrows indicate data flow and direction.  In the remainder of this section, we explain the components of our system. 2.1  Speech and Speaker Recognition  The user vocally asks her question which is recorded through a microphone driven by the GUI. The audio chunks are then processed in parallel by a speech recognition component and a speaker recognition component. Speech Recognition The Speech Recognition component enables the translation of  speech into text. Cobalt Speech Recognition for French is a Kaldi-based speech-to-text decoder using a TDNN (Povey et al., 2016) acoustic model; trained on more than 2 000 hours of clean and noisy speech, a 1.7-millionword lexicon, and a 5-gram language model trained on 3 billion words. Speaker Recognition The Speaker Recognition component answers the question “Who is speaking?”. This component is based on deep neural network speaker embeddings called “x-vectors” (Snyder et al., 2018). Our team participated to the NIST SRE18 challenge (Sadjadi et al., 2019), reaching the 11th position among 48 participants. Once identified, it is possible to access the information of the speaker by accessing a speaker database which includes attributes such as nationality. This is a key module for personalising the behaviour of the system, for instance, by supporting questions such as ”Who is the president of the country where I was born?”. 2.2  The Dialogue System  The transcribed utterance and the speaker information are passed to the dialogue system. This system contains an understanding component, a context manager, and a generation component (Figure 2).  Figure 2: Internal structure of the proposed dialogue system, with emphasis placed on the interactions of the context manager.  Understanding The understanding component relies on a linguistic module to parser the user’s inputs. The linguistic module supports part-of-the-speech (POS) tagging, lemmatisation, dependency syntax and semantics provided by an adapted version  words sentences mentions incl. prons. chains  Train  Dev  Test  208 245 10 166 15 013 1 465 3 793  45 001 2 976 3 008 280 901  89 330 4 853 6 232 538 1 533  Table 1: Subset of the corpus CALOR used for training, developing and testing of the coreference resolution module. Note that the values given for the mentions include pronouns.  of UDpipe (Straka and Straková, 2017), extended with a French full-form lexicon. UDpipe was trained on the French GSD treebank version 2.32 . Since the syntax of questions in French differs from that of declaratives, we annotated manually about 500 questions to be merged into the UD treebank (which originally did not contain questions). Tests show that the labelled attached score (LAS) is thereby increased by 10% absolute, to 92%. Context Manager The Context Manager component is able to solve coreferences by using an adaptation of the end-to-end model presented in (Lee et al., 2017), that we trained for French by using fasttext multilingual character embeddings (Bojanowski et al., 2017). The data used to train the coreference resolution model is a subset of the corpus CALOR (Marzinotto et al., 2018) (Table 1), which has been manually annotated with coreferences. This corpus contains coreference chains of named entities, nouns and pronouns (such as “the president” – “JFK” – “he” – “his”). The dependency tree and semantic frames provided by the linguistic module are used to solve ellipsis by taking into account the syntactic and semantic structure of the previous question. Once the question has been resolved, it calls the QA systems and passes their results to the generation module. Generation The generation component either returns the short answer provided by QA systems or relies on an external generation module that uses dependency grammar templates to generate more elaborated answers. 2.3  QA Systems  Two complementary question answering components were integrated into the system: the Reasoning QA and Search QA. Each of these 2  http://universaldependencies.org/  QA systems computes a confidence score for every answer by using icsiboost (Favre et al., 2007), an Adaboost-based classifier trained on a corpus of around 21 000 questions. The Context Manager takes into account these scores to pick the higher-confidence of the two answers. Besides the QA components, there are two other components that are able to provide complementary information about the Wikidata’s entities under discussion: Documentary and Entity Sheet. Reasoning QA The Reasoning QA system first parses the question by using a Prolog definite clause grammar (DCG), extended with word-embeddings to support variability in the vocabulary. Then it explores a graph containing logical patterns that are used to produce requests in SPARQL3 that agree with the question. Search QA The Search QA system uses an internal knowledge base, which finely indexes data using Elasticsearch. It is powered by Wikidata and enriched by Wikipedia, especially to calculate a Page-Rank (Page et al., 1997) on each entity. This QA system first determines the potential named entities in the question (i.e. subjects, predicates, and types of subjects). Second, it constructs a correlation matrix by looking for the triplets in Wikidata that link these entities. This matrix is filtered according the coverage of the question and the relevance of each entity in order to find the best answer. Documentary The documentary component is able to extract pertinent excerpts of Wikipedia. It uses an internal documentary base, which indexes Wikipedia’s paragraphs by incorporating the Wikidata entity’s IDs into elasticsearch indexes. Thus, it is possible to find paragraphs (ranked by elasticsearch) illustrating the answer to the given question by taking into account the entities detected in the question and in the answer. Entity Sheet The entity sheet component summarises an entity in Wikidata returning the description, the picture and the type of the entity. 3  https://www.w3.org/TR/sparql11-query/  Figure 3: Distribution of question topics used to evaluate system performance on out-of-context questions.  2.4  Speech Synthesis  Finally, the generated response is passed to the GUI, which in turn passes it to the Voxygen synthesis solution.  3  Evaluation  The evaluation of the individual components of the proposed system was performed outside the scope of this work. We evaluated out-ofcontext questions, as well as the coreference resolution module. Performance on out-of-context questions was evaluated on Bench’It, a dataset containing 150 open ended questions about general knowledge in French (Figure 3)4 . The system reached a macro precision, recall and F-1 of 64.14%, 64.33% and 63.46% respectively5 . We also evaluated the coreference resolution model on the test-set of CALOR (Table 1), obtaining an average precision, recall and F-1 of 65.59%, 48.86% and 55.77% respectively. The same model reached a average F-1 of 68.8% for English (Lee et al., 2017). Comparable measurements are not available for French. F-1 scores for French are believed to be lower because of the lower amount of annotated data.  4  Examples  On the one hand, the system is able to answer complex out-of-context questions such as “What are the capitals of the countries of the Iberian Peninsula?”, by correctly answering the list of capitals: “Andorra la Vella, Gibraltar, Lisbon, Madrid”. 4 Publicly available in https://github.com/ lmrojasb/benchit.git 5 Following the metrics of the task-4 of QALD-7 https://project-hobbit.eu/challenges/qald2017/  U: Who is Michael Jackson ? S: Michael Jackson is an American author,composer, singer and dancer U: What is his father’s name? S: Joseph Jackson U: and his mother’s? S: Katherine Jackson U: and his brothers’ and sisters’ ? S: Tito Jackson, Rebbie Jackson, Randy Jackson, Jackie Jackson, Marlon Jackson, La Toya Jackson, Jermaine Jackson, Janet Jackson  Figure 4: English translation of French conversation involving in-context questions.  On the other hand, consider the dialogue presented in Figure 4, in which the user asks several related questions about Michael Jackson. First she asks “Who is Michael Jackson?” and the system correctly answers “Michael Jackson is an American author, composer, singer and dancer”, note that this is the generated long answer. The subsequent questions are related to the names of his family members. In order to correctly answer these questions, the resolution of coreferences is neccesary to solve the possessive pronouns, which in French agree in gender and number with the noun they introduce. In this specific example, while in English “his” is used in all the cases, in French it changes to: son père (father), sa mère (mother), ses frères (brothers). This example also illustrates resolution of elliptical questions in the context, by solving the question “and his mother’s” as “What is the name of his mother”.  5  Conclusion and Future Work  We have presented a spoken conversational question answering system, in French. The DS orchestrates different QA systems and returns the response with the higher confidence score. The system contains modules specifically designed for dealing with common spoken conversation phenomena such as coreference and ellipsis. We will soon integrate a state-of-the art reading comprehension approach, support English language and improve the coreference resolution module. We are also interested in exploring policy learning, thus the system will be able to find the best criterion to chose the answer or to ask for clarification in the case of ambiguity and uncertainty.  References Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the ACL, 5:135–146. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. QuAC: Question answering in context. In Proceedings of EMNLP 2018, pages 2174–2184, Brussels, Belgium. Benoit Favre, Dilek Hakkani-Tür, and Sebastien Cuendet. 2007. Icsiboost. https://github. com/benob/icsiboost. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. 2017. End-to-end neural coreference resolution. In Proceedings of the 2017 EMNLP, pages 188–197, Copenhagen, Denmark. Association for Computational Linguistics. Gabriel Marzinotto, Jeremy Auguste, Frédéric Béchet, Géraldine Damnati, and Alexis Nasr. 2018. Semantic frame parsing for information extraction: The CALOR corpus. In LREC, Miyazaki, Japan. ELRA. Larry Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1997. Pagerank: Bringing order to the web. Daniel Povey, Vijayaditya Peddinti, Daniel Galvez, Pegah Ghahrmani, Vimal Manohar, Xingyu Na, Yiming Wang, and Sanjeev Khudanpur. 2016. Purely sequence-trained neural networks for asr based on lattice-free mmi. In Proceedings of INTERSPEECH, 2016. Siva Reddy, Danqi Chen, and Christopher D Manning. 2018. Coqa: A conversational question answering challenge. arXiv preprint arXiv:1808.07042. Seyed Omid Sadjadi, Craig S. Greenberg, Douglas A. Reynolds, Elliot Singer, Lisa P. Mason, , and Jaime Hernandez-Cordero. 2019. The 2018 nist speaker recognition evaluation. In Proceedings of INTERSPEECH (submitted), Graz, Austria, August 2019. David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev Khundanpur. 2018. X-vectors: Robust dnn embeddings for speaker recognition. In Proceedings of IEEE ICASSP, April 2018. Milan Straka and Jana Straková. 2017. Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 88–99, Vancouver, Canada. ACL.  "
"Synergistic Team Composition: A Computational Approach to Foster Diversity in Teams Ewa Andrejczuka,b , Filippo Bistaffaa , Christian Bluma,∗, Juan A. Rodriguez-Aguilara , Carles Sierraa  arXiv:1909.11994v1 [cs.AI] 26 Sep 2019  a Artificial b ST  Intelligence Research Institute (IIIA-CSIC), Campus of the UAB, Bellaterra, Catalonia, Spain Engineering - NTU Corporate Laboratory, Nanyang Technological University, Singapore   1. Introduction Active learning refers to a broad range of teaching techniques that engage students to participate in all learning activities in the classes. Typically, active learning strategies involve a substantial amount of students working together within teams. Research shows that students learn better when using active learning compared to the traditional ∗ Corresponding  author Email addresses: ewaa@ntu.edu.sg (Ewa Andrejczuk), filippo.bistaffa@iiia.csic.es (Filippo Bistaffa), christian.blum@iiia.csic.es (Christian Blum), jar@iiia.csic.es (Juan A. Rodriguez-Aguilar), sierra@iiia.csic.es (Carles Sierra)  Accepted in Knowledge-Based Systems (https://10.1016/j.knosys.2019.06.007)  15 October 2019  schooling methods [1]. They do not only acquire and retain the information better but also are more content with their classes [2]. Nevertheless, not all teams facilitate learning. For team-based learning to be effective, every team composed in the classroom needs to be heterogeneous, i.e. diverse in individuals’ characteristics. Furthermore, having some significantly weaker teams and some significantly stronger teams is undesirable. Hence, the distribution of teams in a classroom must be balanced in the sense that all teams are more or less equally strong. Even though much research in the industrial, organisational, and educational psychology fields investigated what are the predictors of team success, to the best of our knowledge, there are no computational models to build teams for a given task that are broadly used in the classrooms. Frequently studied individual characteristics that influence team performance are competencies, personality traits, and gender [3, 4, 5, 6]. [3, 6] show a positive correlation between certain personality traits and team composition. [4, 5] show that in order to increase team performance, team members should be heterogeneous in their individual characteristics. Some of those characteristics were also acknowledged by multiagent systems (MAS) research. The most studied characteristic in MAS research are competencies [7, 8, 9, 10, 11]. However, in these works agents’ competencies are generaly represented as True/False characteristics, that is, an agent has or does not have a required competence. This is an oversimplified approach to model agents’ competencies since it disregards any competence grade. In reality, competencies are non-binary because individuals are characterized by different grades of competencies. Unfortunately, MAS research has typically ignored significant psychology findings (with the exception of recent, preliminary works such as [7] and [9]). To the best of our knowledge, neither the current MAS literature nor the current psychology literature has considered team composition based on competencies, personality and gender of individuals at the same time. In this paper, we focus on the following team composition problem that is commonly encountered in education. We consider a complex task that needs to be performed by multiple student teams of even size [12]. The task requires each team to have at least one student with a minimum level of competence for each competence from a given set of competencies. There is a pool of students with varying competencies, genders, and personalities. The objective is to partition students into teams so that each team is even in size and balanced in competencies as well as personalities, and gender. We term those teams as synergistic teams. In this context, the paper makes the following contributions: • We identify and formally define a new type of real-life problem, the so-called synergistic team composition problem (STCP). The goal of the STCP is to partition a set of individuals into a set of synergistic teams: teams that are diverse in personality and gender and whose members cover all required competencies to complete a task. Furthermore, the STCP requires that all teams are balanced in that they are expected to exhibit similar performances when completing the task. • We introduce two different algorithms to tackle the STCP: (i) STCPSolver, an algorithm that employs a reformulation of the problem which is then solved to  2  optimality by an off-the-shelf integer linear programming (ILP) solver; and (ii) SynTeam, an anytime heuristic that can produce solutions of high quality within a limited computation time. • We perform an exhaustive computational comparison of STCPSolver and SynTeam over realistic settings in education, considering actual-world data. Overall, our analysis indicates that STCPSolver is efficient for rather small problem instances, whereas SynTeam is able to cope with larger problem instances. First, we notice that the runtime of the optimal algorithm greatly increases with a growing team size and a growing number of students, which causes the algorithm not to be applicable to larger instances of the problem. This is not the case for SynTeam, which is capable of composing teams for larger problem instances while providing good quality approximate solutions (whose values range, in the worst case, between 75% and 95% of the value of an optimal solution). Second, we compare the anytime performance of both algorithms. We observe that SynTeam outperforms STCPSolver for large team sizes (beyond 3), whereas the opposite occurs for small team sizes. We also compared our optimal approach (i.e., STCPSolver) to ODP-IP [13], the state-of-the-art algorithm to solve the coalition structure generation problem. Results show that STCPSolver outperforms ODP-IP both in terms of runtime (since ODP-IP cannot exploit the presence of cardinality constraints to reduce the space of feasible solutions) and scalability (due to ODP-IP’s exponential memory requirements). Outline. The remainder of this paper is structured as follows. Section 2 provides an overview of the related work. Section 3 introduces the basic definitions used in this paper. Section 4 introduces the key notions used to measure the synergistic value of a team and formally defines the synergistic team composition problem. Sections 5 and 5.2 describe STCPSolver and SynTeam, the two algorithms that we introduce in this work. Then, Section 7 discusses our empirical comparison of the proposed algorithms over synthetically-generated instances of the STCP. Section 8 briefly introduces a web application that is freely available and offers team composition as its main functionality. Finally, Section 9 discusses both the conclusions and directions for future research. 2. Related work In this section we review relevant related work. Sections 2.1 and 2.2 go through related work in the education literature and the organisational psychology literature respectively. Sections 2.3 and 2.4 revise related work in the computer science literature; section 2.3 revises the multiagent systems literature, whereas section 2.4 discusses relevant work in the coalition formation literature. 2.1. Relation with the education literature There are many works that advise on how to handcraft heterogeneous teams with the purpose of increasing team-based learning and improving team performance, for instance, [14] or [15].  3  [14] offers a manual method to divide a classroom based on students’ personalities and genders. In this paper, we extend the method in [14] by adding competencies and propose an algorithm to compose teams in an automatic way. [15] advises beginning a team composition process by simply asking questions to a group of students. These questions are used to gather information about those competencies that are important for the successful completion of a given task. Students respond to each question either orally or with a show of hands. Then, students are lined up based on the number of required competencies that they have, derived from the answers to the questions. Ties are broken randomly. Finally, teams are built by asking students to count off down the line. For instance, if teams of five students are required, the count is as follows: 1, 2, 3, 4, 5, 1, 2, . . .. Hereby, the number associated to a student indicates the team to which he/she is assigned. Some authors have tried to automatise the team composition process. That is, they have aimed at composing a set of teams so that all teams are as similar as possible with regard to the mean values of multiple attributes [16, 17, 18, 19, 20]. As opposed to our approach, none of these works imposes heterogeneity in a direct way when composing teams. They are rather limited to studying a set of fixed constraints (such as avoiding clustering particular majors, ensuring that no international student and no female are isolated on a team, etc). Additionally, compared to our approach where we compose teams for particular tasks, they do not explicitly consider the notion of the task when composing balanced teams. This is also the case in the work by Agrawal et al. [21], though the authors diverge from the above-mentioned approaches to team composition. Their team composition approach focuses on grouping students so that, in the end, the value gained by less capable students through collaboration is maximised. Despite the novelty of their team composition (grouping) problem, Agrawal et al. only consider students capabilities (disregarding the findings of the organisational psychology literature about other individual attributes, i.e. personality and gender, that we include), consider that students count on abilities for a single competence (instead of multiple competencies as we do), and are not concerned about yielding balanced team compositions, which is our main goal. To the best of our knowledge, the only available web tool supporting team composition is the Comprehensive Assessment of Team Member Effectiveness (CATME)1 that composes teams based on individual students’ responses to an online survey. Teachers define student surveys by selecting the desired students’ characteristics from a given inventory [19]. The application calculates a “question score” for each characteristic that informs how well each team’s distribution of that characteristic satisfies the teacher’s aims. The application also measures a global “compliance score” for each composed team characterizing how well the team satisfies the teacher’s wishes. The higher these values the better the team. Their team composition algorithm starts by randomly distributing students across teams of a pre-specified size. Next, the algorithm calculates both question and compliance scores. Then, it iteratively changes the teams with the purpose of maximising the minimum compliance score of all teams. This work is similar to our approach, however, there are also substantial differences. In addition to the 1 http://www.catme.org  4  differences discussed above, the authors do not analyze their solutions’ quality. They assume that the groupings produced by their algorithm are near-optimal. The analysis performed by [22] shows, however, that it is implausible the CATME method achieves near-optimal results. 2.2. Relation with the organisational psychology literature As far as we are concerned, there are no methods in the organisational psychology literature that would provide a complete guideline on how to compose teams. Instead, the researchers in this field study how individual characteristics influence team performance. The most studied individual characteristic that is associated with team performance is cognitive ability. [23] define it as the “capacity to understand complex ideas, learn from experience, reason, solve problems, and adapt” [23, p.507]. It is a very wide concept that—in addition to competencies, broadly used in multiagent systems research— covers many other characteristics such as experience, gender or even age. [24] and [23] discovered the positive correlation between team performance and the average team values of cognitive ability. [23] also showed that the variance of team members’ cognitive ability was not a good predictor of team performance. Additionally, these authors observed that the average value is two times more informative for the prediction of team performance than the lowest and the highest member scores. [25] suggested the existence of collective intelligence in teams that can predict team performance. This collective intelligence is not strongly correlated with the maximum or average intelligence of team members. Instead, it is positively correlated with increasing equality in conversational turn-taking, the mean social sensitivity of group members, and gender balance [26]. The organisational psychology literature, in addition to cognitive ability, has examined the impact of personality traits on team performance [27, 6]. The most popular questionnaires to determine personality include: (1) the Five Factor Model (known as “Big Five” or FFM), which uses five traits to define individual personality [28]; (2) Belbin theory [29], which provides a theory of nine different personality role types; and (3) the Myers-Briggs Type Indicator (MBTI) questionnaire that uses four traits to specify psychological preferences concerning the way people perceive the world and make decisions [30]. Concerning FFM, [24] found that for each personality trait examined individually (i.e. extraversion, agreeableness, conscientiousness, emotional stability, openness to experience), team means were associated with team performance. The study in [31] confirmed these findings for all traits except for the Openness to Experience trait which was not considered. However, the sizes of studied samples were small and it is unclear whether these findings are statistically significant [32]. [33] reported contradictory findings after studying student teams. Each team was asked to improve processes based on problems encountered in organisations. The researchers measured team orientation, extraversion, agreeableness, conscientiousness, and emotional stability of each team using the team average and team variability. Interestingly, they did not find any meaningful connection between team performance and any of these personality traits (when examined individually).  5  According to Belbin, there are nine team roles that should be covered in every team [29]. These roles are: completer–finisher, coordinator, implementer, monitor evaluator, plant, resource investigator, shaper, specialist and teamworker. Although some studies with very limited sample sizes (such as 10 teams in [34]) reported support for the theory, studies based on a larger number of samples did not find the relation between the Belbin roles and team performance [35, 36, 37]. Finally, the MBTI has four binary dimensions, that is: intuition vs sensing (N–S), thinking vs feeling (T–F), extraversion vs introversion (E–I), and perceiving vs judging (P–J). Within this questionnaire, every individual can be categorised into one of the sixteen possible four-letter combinations, where each letter represents one personality dimension. This approach is easy to interpret by non-psychologists. Reliance on dichotomous preference scores rather than on continuous scores, however, excessively restricts the level of statistical analysis [38]. 2.3. Relation with the multiagent systems literature To our knowledge, the only computational model in the context of team composition that takes both personality and competencies into account was presented in [39]. In particular, the influence of personality on different strategies for allocating tasks is studied in this paper. However, there are substantial differences with our work. Firstly, instead of proposing an algorithm for the composition of teams both based on personality and competence, they only describe a model to evaluate teams. Secondly, they give no importance to gender balance. And finally, they do not evaluate their algorithm with real data (only via agent-based simulation). We separate the remaining literature that is relevant to this article into the following two categories: works that deal with agent competencies (individual and social capabilities of agents), and works that consider agent personality (individual behaviour models). Competencies. Various previous works have focused on the competency dimension. However, in contrast to our work, in which competencies are graded, the majority of works assume agents to have multiple binary skills (either an agent has a required skill or not). In [40] and [10], for instance, one k-robust team is composed for a single task, based on the agents’ capabilities. Hereby, a team is called k-robust if by removing any k members from the team, the completion of the task is not compromised. In [41], each task requires a specific set of competencies. Moreover, tasks arrive sequentially over time. The team composition algorithm, whose focus is on balancing the workload of the agents across teams, builds teams based on competencies and communication cost. Personality. There are, to our knowledge, two works in the literature that consider agents’ personality to compose teams, namely [7] and [9]. In [7], Belbin’s theory is used to obtain human predominant roles (see Section 2.2). As discussed in subsection 2.2, these roles do not tend to be related to team performance. Additionally, gender is not considered for the composition of heterogeneous teams. In [9], Farhangian et al. make use of the classical MBTI personality test (see Section 2.2). Their aim is to build the best possible team around a selected leader. In other words, they compose the best possible team for a particular task. However, gender  6  balance is again not considered. Finally, although real data was considered in [9], the resulting teams’ performance was not validated. Instead, Bayesian theory was used to predict the success probability in a variety of team composition conditions. 2.4. Relation with the coalition formation literature The STCP can be seen as a coalition structure generation (CSG) problem [13] over the entire set of students with a characteristic function that assigns a synergistic value to every feasible coalition (i.e., with the desired number of students), and −∞ to every unfeasible coalition (since the characteristic function has to be defined for every possible subset of agents in the standard definition of CSG). Solving the STCP requires to compute the coalition structure (team partition) with the largest total value, i.e., the optimal solution to the CSG problem. In principle, state-of-the-art CSG approaches such as ODP-IP [13] could be used to solve the STCP problem. Unfortunately, these approaches are not able to exploit the presence of cardinality constraints to reduce the space of feasible solution, and hence, are limited to problem instances of up to 25 agents, due to their exponential memory requirements, as shown by our experiments in Section 7.2. On the other hand, given a STCP we can also define a constrained coalition formation (CCF) [42] game G = hA, Pm (A), si, where Pm (A) is the set of feasible coalition structures. More precisely, the STCP poses a specific type of CCF game, namely, a basic CCF game [42]. Intuitively, basic CCF game express constraints in the form of: (1) allowed sizes of coalitions that can be formed; and (2) subsets of agents whose presence in any coalition is permitted or not. On the one hand, a STCP naturally defines constraints on the size of coalitions. On the other hand, expressing a STCP as a CCF problem would require to define one positive constraint per feasible team, while the set of negative constraints would be empty. As a consequence, the number of positive constraints quickly becomes very large (i.e., > 3000 in our case), hence making the use of the approach by Rahwan et al. [42] impossible. 3. Team Composition Model There are three diversity dimensions of students used in our model: gender, personality, and competencies. We measure personality using the theory of personality called Post-Jungian [14] which is a reduced variant of the Myers-Briggs Type Indicator (MBTI) [30]. The numbers are obtained from the answers of a short questionnaire of 20 quick questions (much shorter than the common 93 questions of the Boolean MBTI). This is very efficient in terms of time and effort for both teachers and students, as completing the test takes only a few minutes (see [14, p.21] for details). Douglass J. Wilde claimed that this numerical method is a coherent extension of the psychological dimensions of MBTI [43]. The test is based on the personality model proposed by C. G. Jung [44] containing two sets of functions and attitudes: 1. 2. 3. 4.  Sensing – Intuition (SN), Thinking – Feeling (TF) Extroversion – Introversion (EI), Perception – Judgment (PJ). 7  The numerical values along each dimension (SN, TF, EI, PJ) are the result of combining the answers to the questionnaire mentioned above where each question can be answered by selecting one out of five possible answers; Each possible answer has a value (in a scale from -1 to +1). Each personality trait is assessed by five questions. The values of the answers are added up and divided by 5 (the number of questions) to give the final value along each personality dimension. This method seems promising as—within one decade—Prof. Wilde multiplied the number of teams of Stanford that were awarded prizes by the Lincoln Foundation [43] by three. Accordingly, the definition of a personality profile in our context is as follows. Definition 1. A personality profile is a tuple hsn, tf , ei, pji ∈ [−1, 1]4 of personality traits. A competence is understood as the knowledge, skills and attitudes that enable a student to successfully solve tasks and face challenges [45]. Moreover, a student possesses every competence with a certain level. Let C = {c1 , . . . , ck } be the set of competencies. Definition 2. A student is represented as a tuple hid, g, p, li such that: • id is the student’s identifier; • g ∈ {man, w oman} stands for the student’s gender; • p is a personality profile tuple; • l : C → [0, 1] gives the students competence levels, that is, l(c) is the student competence level for competence c. We assume that when a student does not have a competence (or we do not know about it), l(c) = 0. Henceforth, the set of considered students is denoted by A = {a1 , . . . , an }. The notion of a team is defined as follows, in a straightforward way, as a group of two or more students. Definition 3 (Team). A team is any subset of A with at least two students. We denote by KA = (2A \ {∅}) \ {{ai }|ai ∈ A} the set of all possible teams from students in A. w(K) and m(K) are the number of women and men respectively in team K. Students are organised in teams to solve tasks. We understand a task as an instance of a task type. A task type not only determines the competences that are required to successfully solve any instance, but also specifies the competence levels and the relative importance of competences. Task types thus differ in requiring different competence levels. A specific task type may require, for instance, a high level of creativity (e.g. to design a city brochure), while another one may require analitycal competences (e.g. to solve mathematical equations). This is formalized as follows. Definition 4. A task type τ is a tuple hλ, {(ci , li , wi )}i∈Iτ i where: • Iτ is the index set of the required competencies. 8  • λ ∈ [0, 1] is the importance given to proficiency; the higher the value of λ, the higher the team proficiency importance. • ci ∈ C is a competence required to perform the task; • li ∈ [0, 1] is the required competence level for ci ; • wi ∈ [0, 1] is the importance P of competence ci for the success in solving an instance of task type τ ; and i∈Iτ wi = 1. Tasks are instances of task types plus a required number of students. Definition 5. A task t is a tuple hτ, mi such that τ is a task type and m is the required number of students, where m ≥ 2. We note by T the set of tasks and by T the set of task types. We will note by Cτ = {ci |i ∈ Iτ } the set of competencies required by task type τ . Given a team and a task, we must consider how to assign responsibilities for the competencies within the team. This competence assignment is defined as follows. Definition 6. Given a task type τ and a team K S ∈ KA , a competence assignment is a function ητ : K → 2Cτ satisfying that Cτ = a∈K ητ (a). We note by ΘK τ the set of competence assignments for task type τ and team K. The list of students assigned to each competence is defined as follows. Definition 7. Given a task type τ , a team K, and competence assignment ητ , the set δ(ci , K, ητ ) = {a ∈ K|ci ∈ ητ (a)} stands for those students in team K responsible for competence ci . In team-based learning, it is a key requirement that students share responsibilities in order to achieve a successful performance. Hence, our objectives are: (a) to distribute responsibilities in a balanced way across a team; and (b) to have each team member responsible of at least one competence. This is especially important in an education context, where no one should be cornered within a team. We shall refer to such an assignment as a balanced competence assignment. Note that we will be concerned with this particular assignment in this paper. Hereafter, we note by Θ̄K τ the set of K balanced competence assignments for task type τ and team K, where Θ̄K τ ⊆ Θτ . 4. The Problem of Composing Synergistic Teams Next, we present our computational model to compose and evaluate teams. First, we introduce a way of measuring proficiency, namely the degree of matching between a competence assignment and the competences of the members of a team. Thereafter, we provide a measure of congeniality, namely of the diversity of personalities of the members in a team. Then, the synergistic value of a team results from combining both proficiency and congeniality values. 9  4.1. How to assess the proficiency value of a team Our goal is to calculate the proficiency degree of a team for a particular task from a competence assignment. With this aim, our measure of proficiency will adhere to the following principle: the closer the competence levels of the team members in a competence assignment to the competence levels required by the task, the larger the proficiency degree of the team. In this way, we pursue to avoid both under-proficient and over-proficient competence assignments, since they involve under-qualified and over-qualified teams. On the one hand, students in under-proficient teams may get frustrated because of their lack of knowledge to undertake their assignments. On the other hand, as argued in [46], students in over-qualified teams are bound to lose attention and motivation because of the lack of challenge in their assignments. Our formal definitions of under-proficiency degree and over-proficiency degree are based on measuring the distance between what is required (in terms of competence levels) by a task and what a team offers to perform the task (according to a competence assignment). Definition 8 (Under-proficiency degree). The under-proficiency degree of a team K to perform a task of type τ according to a competence assignment ητ is: P a X a∈δ(ci ,K,ητ ) | min(l (ci ) − li , 0)| u(ητ ) = wi · |δ(ci , K, ητ )| + 1 i∈Iτ  Definition 9 (Over-proficiency degree). The over-proficiency degree of a team K to perform a task of type τ according to a competence assignment ητ is: P a X a∈δ(ci ,K,ητ ) max(l (ci ) − li , 0) o(ητ ) = wi · |δ(ci , K, ητ )| + 1 i∈Iτ  We combine the under-proficiency and over-proficiency degrees of a team as a weighted average to finally obtain the proficiency degree of a team as follows: Definition 10. The proficiency degree of a team K to perform a task of type τ following a competence assignment ητ , and considering an under-proficiency penalty υ ∈ [0, 1] is: uprof (K, τ ) = max (1 − (υ · u(ητ ) + (1 − υ) · o(ητ )). (1) ητ ∈Θ̄K τ  Definition 10 is restricted to the set of balanced competence assignments Θ̄K τ , which are the relevant competence assignments in education scenarios, as discussed above.2 Furthermore, It is worth noticing that function uprof (K, τ ) in Definition 10 is well defined for any team, task type and competence assignment. Indeed, for any task type τ , team K, and η ∈ ΘK τ , we observe that u(ητ ) + o(ητ ) ∈ [0, 1) and 2 A more general definition of proficiency could be readily obtained by considering the set of all competence assignments ΘK τ instead. However, we propose this definition for the sake of simplicity.  10  0 ≤ uprof (K, τ ) < 1. This is true since no student can be over-proficient and underproficient at the same time. From equation 1 we observe that the larger the value of importance of the proficiency penalty (υ), the larger the importance of the over-proficiency degree. And the other way around, the lower the proficiency penalty, the less important the underproficiency degree. Hence, setting large values to the proficiency penalty guarantees that competence assignments that make a team under-competent (unable to cope with competence requirements) are penalised. Analogously, small proficiency penalties are meant to penalise over-competent teams. The correct setting of the proficiency penalty parameter will depend on each task type. On the one hand, if our objective is to foster effective teams, then we must set the proficiency penalty to a large value to penalise more under-proficiency. In order to computer uprof (K, τ ) we must solve a constrained optimisation problem: find the balanced competence assignment with minimum cost (in terms of underand over-proficiency). This problem can be formulated and solved as a minimum cost flow problem. More precisely, given a team K ∈ KA and a task type τ , we derive the balanced competence assignment η that maximises equation 1 by solving a minimum cost assignment problem, which in turn can be expressed as an integer linear program (ILP) as follows. The ILP employs a binary variable xij to encode the decision of whether student ai ∈ K is tasked with competence cj ∈ Cτ , where Cτ is the set of competencies required by task τ . Hereby, the cost of assigning a student ai to a competence cj , denoted by pij , is defined as follows: ( (lai (cj ) − lj ) · (1 − υ) · wj if lai (cj − lj ) ≥ 0 pij := −(lai (cj ) − lj ) · υ · wj if lai (cj − lj ) < 0 where v ∈ [0, 1] is the penalty applied to the under-proficiency of team K (see Section 4.1 for a detailed introduction of this term) and wj ∈ [0, 1] weighs the importance of competence cj to succeed in completing a task of type τ (see definition 4). The above-mentioned minimum cost assignment problem can then be expressed in the following way as an ILP model. X X min xij · pij (2) ai ∈K cj ∈Cτ  subject to: X cj ∈Cτ    |Cτ | xij ≤ |K|  X  xij ≥ 1   ∀ ai ∈ K  (3)  ∀ ai ∈ K  (4)  xij = 1 ∀ cj ∈ Cτ  (5)  cj ∈Cτ  X ai ∈K  l m τ| Constraint (3) makes sure that each student is assigned to at most |C |K| competencies, while constraint (4) makes sure that each student is assigned to at least one competence. 11  Note that constraints (4) are only used if |Cτ | ≥ |K|. Finally, constraint (5) ensures that each competence has exactly one student assigned to it. The solution to the ILP above allows us to build the balanced competence assignment required to compute uprof in equation 1 as follows: for each student ai in team K, ητ (ai ) = {cj ∈ Cτ |xij = 1}. At this point we have learned how to compute the proficiency value for a team given a particular competence assignment. However, as argued in the introduction, the degree of proficiency alone is not enough for a team to succeed. Next we introduce a function to measure the congeniality within a team from the personalities and genders of its team members. Thus, our congeniality measure does not consider any competence assignment, hence differing from our above-defined proficiency measure. 4.2. How to assess the congeniality value of a team Recent studies in organisational psychology have proven the existence of a trade-off between the creative productivity caused by “meta-cognitive conflict” and “harmony” —good feeling— in a team [47]. On the one hand, meta-cognitive conflict stems from the different views of the world that people exhibit based on opposing personality and gender. On the other hand, harmony ori"
"arXiv:1909.12032v1 [cs.AI] 26 Sep 2019  Query Optimization Properties of Modified Valuation-Based Systems Mieczyslaw A. Klopotek Institute of Computer Science Polish Academy of Sciences ul. Ordona 21, 01-237 Warszawa, Poland  Slawomir T. Wierzchoń Institute of Computer Science Polish Academy of Sciences ul. Ordona 21, 01-237 Warszawa, Poland  email: klopotek@ipipan.waw.pl  email: stw@ipipan.waw.pl 1  1988] or [Cooper & Herskovits, 1992] for a deeper discussion. Studies by Wen [1991], and Wong, Xiang and Nie [1993] establish a link between knowledgebased systems for probabilistic reasoning and relational databases. Particularly, they show that the belief update in a Bayesian network can be processed as an ordinary query, and the techniques for query optimization are directly applicable to updating beliefs. The same idea we find in Thoma’s [1991] works, who proposed a scheme for storing Shafer’s belief functions which generalizes graphical models. In this paper after introducing the valuation based system framework (Section 2), we present Markov-like properties of VBS (Section 3) and a method for resolving queries to VBS (Section 4).  Introduction  Though graphical representation of a domain knowledge has quite long history, its full potential has not been recognized until recently. We should mention here pioneering works of J. Pearl, reported in his monography published in 1988 [1988]. Further development in this domain has been achieved by Shenoy and Shafer [1986] who adopted a method used in solving nonserial dynamic programming problems [Bertele & Brioschi, 1972]. This trick proved to be very fruitful and gave growth to a unified framework for uncertainty representation and reasoning, called ValuationBased System, VBS for short [Shenoy, 1989]. It can represent knowledge in different domains including probability theory, Dempster-Shafer theory and possibility theory. More recent studies show that the framework of VBS is also appropriate for representing and solving Bayesian decision problems [Shenoy, 1993] and optimization problems [Shenoy, 1991]. The graphical representation is called a valuation network, and the method for solving problems is called the fusion algorithm. Closely related to VBS is the algorithm of Lauritzen and Spiegelhalter [1988] and HUGIN approach developed by Jensen and his co-workers [1990]. A Bayesian network (as well as its generalization VBS) can be regarded as a summary of an expert’s experience with an implicit population. Detailed documentation of such knowledge with an explicit population is stored in a database. It appears that there exists a strong connection between these two approaches. First of all, databases are used for knowledge acquisition and Bayesian network identification - see [Pearl,  2  Valuation Based Systems  The VBS framework was introduced in [Shenoy, 1989]. In VBS, a domain knowledge is represented by entities called variables and valuations. Further, two operations called combination and marginalization are defined on valuations to perform a local computational method for computing marginals of the joint valuation. The basic components of VBS can be characterized as follows. Valuations Let X = {x1 , x2 , ...xn } be a finite set of variables and Θi be the domain (called also frame), i.e. a discrete set of possible values of i-th variable. If h is a finite non-empty set of variables then Θ(h) denotes the Cartesian product of Θi for xi in h, i.e. Θ(h) = ×{Θi |xi ∈ h}. R stands for a set of nonnegative reals. For each subset s of X there is a set D(s) called the domain of a valuation. For instance in the case of probabilistic systems D(s) equals to Θ(s), while under the belief function framework D(s) equals to the power set of Θ(s), i.e. D(s) = 2Θ(s) . Valuations, being primitives in the VBS framework, can be characterized as mappings σ : D(s) → R. In the sequel valuations will be denoted by lower-case Greek letters, ρ, σ, τ , and so on. Following Shenoy [1994] we distinguish three categories of valuations: • Proper valuations, P, represent knowledge that is partially coherent. (Coherent knowledge means  knowledge that has well defined semantics.) This notion plays an important role in the theory of belief functions: by proper valuation it is understood an unnormalized commonality function. • Normal valuations, N , represent another kind of partially coherent knowledge. For instance, in probability theory, a normal valuation is a function whose values sum to 1. Particularly, the elements of P ∩ N are called proper normal valuations; they represent knowledge that is completely coherent or knowledge that has welldefined semantics. • Positive normal valuations: it is a subset U s of N s consisting of all valuations that have unique identities in N s . Further there are two types of special valuations: • Zero valuations represent knowledge that is internally inconsistent, i.e. knowledge whose truth value is always false; e.g., in probability theory by zero valuation we understand a valuation that is identically zero. It is assumed that for each s ⊆ X there is at most one valuation ζs ∈ V s . The set of all zero valuations is denoted by Z. • Identity valuations, I, represent total ignorance, i.e. lack of knowledge. In probability theory an identity valuation corresponds to the uniform probability distribution. It is assumed that for each s ⊆ X the commutative semigroup (w.r.t. the binary operation ⊗ defined later) N s ∪ {ζs } has an identity ιs ∈ V s . Commutative semigroup may have at most one identity [Clifford & Preston, 1961]. Combination By combination we understand a mapping ⊗ : V × V → N ∪ Z that satisfies the following six axioms: (C1) If ρ ∈ V r and σ ∈ V s then ρ ⊗ σ ∈ V r∪s ;  probability theory combination corresponds to pointwise multiplication followed by normalization, and in Dempster-Shafer theory to the Dempster rule of combination. In the field of uncertain reasoning combination corresponds to aggregation of knowledge: when ρ and σ represent our knowledge about variables in subsets r and s of X then the valuation ρ ⊗ σ represents the aggregated knowledge about variables in r ∪ s. Moreover Wen [1991], and Wong, Xiang and Nie [1993] showed that under probabilistic context combination corresponds to the (generalized) join operation used in the data-based systems. Hence the belief update in a Bayesian network can be processed as an ordinary query, and the techniques for query optimization are directly applicable to updating beliefs. Similar idea we find in Thoma’s [1991] works, who proposed a scheme for storing Shafer’s belief functions. If ρ ⊗ σ is a zero valuation, we say that ρ and σ are inconsistent. On the other hand, if ρ ⊗ σ is a normal valuation, then we say that ρ and σ are consistent. It is important to notice, that an implication of axioms C1 - C3 is that the set N s ∪ {ζs } together with the combination operator is a commutative semigroup [Clifford & Preston, 1961]. If zero valuation ζs exists then ζs is - by axiom C4 - the zero of this semigroup. Similarly, by axiom C5, the identity valuation is the identity of the semigroup N s ∪ {ζs }. Marginalization While combination results in knowledge expansion, marginalization results in knowledge contraction. Let s be a non-empty subset of X . It is assumed that for each variable X in s there is a mapping ↓ (s − {X}) : V s → V s−{X} , called marginalization to s − {X} or deletion of X, that satisfies the next six axioms: (M1) Suppose σ ∈ V s and suppose X, Y ∈ s. Then (σ ↓(s−{X}) )↓(s−{X,Y }) = (σ ↓(s−{Y }) )↓(s−{X,Y }) ; ↓(s−{X})  (M2) If zero valuation exists, then ζs ζs−{X} ;  =  (C2) ρ ⊗ (σ ⊗ τ ) = (ρ ⊗ σ) ⊗ τ ;  (M3) σ ↓(s−X) ∈ N if and only if σ ∈ N ;  (C3) ρ ⊗ σ = σ ⊗ ρ;  (M4) If σ ∈ U then σ ↓(s−X) ∈ U;  (C4) If ρ ∈ V r and zero valuation ζs exists then ρ ⊗ ζs ∈ V r∪s .  (CM1) Suppose ρ ∈ V r and σ ∈ V s . Suppose X 6∈ r and X ∈ s. Then (ρ ⊗ σ)↓((r∪s)−{X}) = ρ ⊗ σ ↓(s−{X})  (C5) For each s ⊆ X there exists an identity valuation ιs ∈ N s ∪ {ζs } such that for each valuation σ ∈ N s ∪ {ζs }, σ ⊗ ιs = σ. (C6) It is assumed that the set N ∅ consists of exactly one element denoted ι∅ . In practice combination of two valuations is implemented as follows. Let (+) be a binary operation on R. Then (σ ⊗ ρ)(x) = σ(x.s)(+)ρ(x.r) where x is an element from D(s) and x.r, x.s stand for the projection (relying upon dropping unnecessary variables) of x onto the appropriate domain D(r) or D(s). In  (CM2) Suppose σ ∈ N s . Suppose r ⊆ s and suppose that ι is an identity for σ ↓r . Then σ ⊗ ι = σ. Axiom M1 states that if we delete from s, the domain of a valuation s ∈ V s , two variables, say X and Y , then the resulting valuation defined over the subset r = s − {X, Y } is invariant to the order of these variables deletion. Particularly, deleting all variables from the set s we obtain the valuation whose domain is the empty set (its existence is guaranteed by axiom  Figure 1: Hypergraph H1 - graphical representation. C6); by axiom M3 this element equals to ι∅ if and only if σ is a normal valuation. Axioms M2 - M4 state that the marginalization preserves coherence of knowledge. Axiom CM1 plays an important role in designing the Message Passing Algorithm (MPA, for short) which will be described later, and axiom CM2 allows to characterize properties of the identity valuations; some of them are given in the Lemma 1 below. Lemma 1 [Shenoy, 1994]. If axioms C1 - C6, M1 M4, CM1 and CM2 are satisfied then the following statements hold. 1. Let σ ∈ V s and r ⊆ s. σ ∈ N s ∪ {ζs } if and only if σ ⊗ ιr = σ. 2. If σ ∈ V s and r ⊆ s then σ ⊗ ιr = σ ⊗ ι∅ . 3. ιs ⊗ ιr = ιs∪r . 4. If r ⊆ s then ι↓r s = ιr . Removal Removal, called also direct difference, is an ”inverse” operation to the combination. Formally, it can be defined as a mapping R : V × (N ∪ Z) → N ∪ Z, that satisfies the three axioms: (R1) If σ ∈ V s and ρ ∈ N r ∪ Z r then σ R ρ ∈ N r∪s ∪ Z r∪s . (R2) For each ρ ∈ N r ∪ Z r and for each r ⊆ X there exists an identity ιr such that ρ R ρ = ιr . (CR) If σ, τ ∈ V and ρ ∈ N ∪ Z then (σ ⊗ τ ) R ρ = σ ⊗ (τ R ρ). Note that we can define the (pseudo)-inverse of a normal valuation by setting ρ−1 = ι∅ R ρ. The main properties of removal are summarized in Lemma 2 given below. Lemma 2 [Shenoy, 1994]. Suppose that σ, τ ∈ Vand ρ ∈ N ∪ Z. Then: 1. (σ ⊗ τ ) R ρ = (σ R ρ ⊗ τ ) ⊗ τ . 2. If σ ∈ V s and r ⊆ s, then σ R ιr = σ ⊗ ι∅ = σ.  Figure 2: Subgraph of hypergraph H1 from Fig. 1 for answering query q. 3. [(σ ⊗ ρ) R ρ] ⊗ ρ = σ ⊗ ρ. 4. ρ−1 ⊗ ρ = ρ ⊗ ρ−1 . 5. σ R ρ = σ ⊗ ρ−1 . The propagation algorithm With the concepts already introduced we define a VBS as a 5-tuple (X , S, (σs )s∈S , ⊗, ↓), where S is a family of subsets of the set of variables X . The aim of uncertain reasoning is to find a marginal valuation ρ = (⊗σs |s ∈ S)↓r , r ⊆ s, s ∈ S.  (1)  To apply the method of local computations, called the message-passing algorithm (MPA, for brevity) observe first that (X , S) is nothing but a hypergraph. With this hypergraph we associate so-called Markov tree T = (H,E) i.e. a hypertree, or acyclic hypergraph, (X , H), being a covering of (X , S) and organized in a tree structure - see [Shenoy, 1989] for details. We say that (X , H) covers (X , S) if for each s in S there exists h in H such that s ⊆ h. Now if (X , H) is a hypertree if it can be reduced to the empty set by recursively: 1) deleting vertices which are only in one edge, and 2) deleting hyperedges which are subsets of other hyperedges. These two steps define so-called Graham’s test. The sequence of hyperedge deletion determines a tree construction sequence (i.e. a set of undirected edges E) for a Markov tree T. Let us consider e.g. the hypergraph H1 = {{X1 , X7 , X8 }, {X2 , X5 , X6 , X7 }, {X3 , X6 }, {X4 , X5 }, {X8 , X9 , X10 }, {X10 , X11 , X12 }} (see Fig.1). We see that variables X1 , X3 , X4 , X11 , X12 are contained in only one edge. We delete them getting the hypergraph H10 = {{X7 , X8 }, {X2 , X5 , X6 , X7 }, {X6 }, {X5 }, {X8 , X9 , X10 }, {X10 }}. But then hyperedges {X6 } and {X5 } are contained in {X2 , X5 , X6 , X7 }, and {X10 } in {X8 , X9 , X10 }. So we delete them getting H100 = {{X7 , X8 }, {X2 , X5 , X6 , X7 }, {X8 , X9 , X10 }}. Now variables X2 , X5 , X6 , X9 , X10 are contained in only one edge each. We get H1000 = {{X7 , X8 }, {X7 }, {X8 }}. Now hyperedge {X7 , X8 } contains  both {X7 } and {X8 }, hence we get finally H10000 = {{X7 , X8 }}, as the result of the Graham test which indicates that H1 is a hypertree. Now, the message passing algorithm can be summarized as follows: it tells the nodes of a Markov tree in what sequence to send their messages to propagate the local information throughout the tree. The algorithm is defined by two parts: a fusion rule, which describes how incoming messages are combined to make marginal valuations and outgoing messages for each node; and a propagation algorithm, which describes how messages are passed from node to node so that all of the local information is globally distributed. Just as propagation takes place along the edges of the tree, fusion takes place within the nodes. It is important to notice that in fact the MPA coincides with the two steps determining the tree construction sequence (i.e. Graham’s test).  3  Computing marginals in a Markov tree  Assume that we have constructed a Markov T = (H,E) tree representative of a given VBS, and let us assign a unique number i ∈ I = {1, 2, ..., n}, n =Card(H), to each node in the tree. Denote Vi the original valuation stored in the i-th node of the tree, and Rj the resultant valuation computed for j-th node according to the rule (1). Following [Shenoy, 1989] this Rj is computed due to the rule Rj = Vj ⊗ (⊗{Mi→j |i ∈ N (j)})↓j  (2)  where N (j) stands for the set of neighbours of the node j in the Markov tree, ↓ j means marginalization to the set of variables corresponding to the node j, and Mi→j is the message sent by node i to the node j calculated according to the equation (3) Mi→j = (Vi ⊗ (⊗{Mk→i |k ∈ (N (i) − {j})}))↓j  (3)  It is obvious, that to find Rj we place the node j in the root of the Markov tree and we move successively from leaves of the tree to its root. Note that if k is a leaf node and ι is its neighbour, then Mk→i = (Vk )↓i , hence (2) and (3) are defined properly. A disadvantage of this algorithm is such that we can compute marginals for sets contained in the family H, or for subsets of these sets only. To find marginal for a any subset of variables we need a more elaborated approach. This problem was studied firstly by Xu [1995]. Below we present its more economical modification. First of all we need a generalization of a set chain representation, which has the next form under probabilistic context [Lauritzen & Spiegelhalter, 1988]: For a given tree construction sequence {h1 , h2 , ..., h2 } by a separator we understand a set si such that si = hi ∩ (h1 ∪ h2 ∪ ... ∪ hi−1 ). Separators are easily identified in a Markov tree, namely if {hi , hj } ∈ E then si = hi ∩ hj . Now, with given tree construction sequence the joint probability distribution can be represented as follows Y P (x1 , x2 , ..., xn ) = R1 {(Ri /Si )|i = 2, ..., n} (4)  where Ri and Si are the marginal probabilities defined over the set of variables represented by the sets hi and si , respectively. It appears, that for all VBS’s this property can be nicely extended, as we can see below. First we prove a lemma on an important property of VBS removal operator 1 Lemma 3 In Valuation-Based Systems, the following property of removal operator holds: (ρ R ρ↓r ) ⊗ ρ↓r = ρ PROOF: From CM2: ρ ⊗ ι∅ = ρ.From CR: (ρ ⊗ ι∅ ) R ρ↓r = ρ ⊗ (ι∅ R ρ↓r ). But by definition: ι∅ R ρ↓r = (ρ↓r )−1 , hence (ρ R ρ↓r ) ⊗ ρ↓r = (ρ⊗(ρ↓r )−1 )⊗ρ↓r . From C2 (ρ⊗(ρ↓r )−1 )⊗ρ↓r = ρ ⊗ ((ρ↓r )−1 ⊗ ρ↓r ).But we know that: From R2 ρ R ρ = ιρ . From CM2 (ρ ⊗ ι∅ ) R ρ = ιρ . From CR ρ⊗(ι∅ R ρ) = ιρ . hence ρ⊗ρ−1 = ιρ .Therefore ρ ⊗ ((ρ↓r )−1 ⊗ ρ↓r ) = ρ ⊗ ιρ↓r .So we get due to axiom CM2 ρ ⊗ ιρ↓r = ρ.which proves our claim. Q.e.d.2 Now let us try to transform a Markov tree valuation to the form similar to equation (4). Assume that we have constructed a Markov tree T = (H,E) representative of a given VBS, and let us assign a unique number i ∈ I = {1, 2, ..., n}, n =Card(H), to each node in the tree. Denote Vi the original valuation stored in the i-th node of the tree. Let us consider the following transformation algorithm: starting with the node k=n down to 1 we run a ”valuation move” step such that we will ”move” valuation from nodes with smaller number i to ones with higher one so that final valuation stored in the k-th node of the tree will be Rk R Sk , where Rk and Sk are the marginal valuations defined over the set of variables represented by the sets hk and sk , respectively. Each step is a kind of unidirectional message-passing (towards the actual node k) in that a message is calculated at a node and then (1) removed from the valuation of the node and (2) added to the node closer to k. The valuation of nodes i = 1, ..., k at the beginning of step concerning node k is denoted with Vi,k . At the end of a step, the valuation is denoted with Vi,k−1 except for node k which is denoted with Rk R Sk The Algorithm: begin 1. for k := n step -1 downto 1 Vk,n := Vk 2. for k:=n step -1 downto 2 begin (a) Construct a subtree Γk = (Hk , Ek ) of T consisting only of nodes Hk = {1, ..., k}. (b) Introduce the order <k compatible with the tree Γk , but such that the node k is considered as its root (the smallest element in <k ). (c) Mark all nodes of the Γk inactive 1 Shenoy [1994] assumes implicitly this property but does not prove it.  (d) while the the direct successor of node k in ordering <k inactive if, in ordering <k , all direct successors of node i are active, then: begin i. Active node i ii. Denote all its direct successors as inactive iii. Let j be direct predecessor of i in <k iv. Calculate 0 Vi,k := (Vi,k ⊗(⊗{Ml→i |l ∈ (N (i)k −{j}})  0 Mi→j := Vi,k  ↓j∩i  If (i, k) 6∈ Ek calculate: 0 Vi,k−1 := Vi,k R Mi→j  where N (i)k stands for the set of neighbours of the node i in the Markov subtree Γk , end (e) Let k+ denote the direct successor of node k in <k Calculate Rk := Vk,k ⊗ Mk+→k  0 = ((Vk+,k R Mk+→k ) ⊗ Sk ) ⊗ (Rk R Sk ) =  = Vk+,k−1 ⊗ (Rk R Sk ) The theorem is then provable by induction (on k running from n to 1) Q.e.d.2 THEOREM 5 In the previous theorem, Ri = R↓hi PROOF: It is easily seen that Rk is always the projection of the joint valuation of the subtree Γk (compare the message passing algorithm of Shenoy and Shafer [1986]). Hence especially Rn is the projection of R onto node n. 0 Further, let RΓk = Vk,k × (⊗{Vi,k |i = 1, 2, ..., k − 1} Then, due to CM1 we have: RΓk−1 = RΓ1∪2∪....∪k−1 . This implies, by induction, that k Rk is the projection of R onto node k for every k = 1, 2, ..., n. Q.e.d.2 These two theorems 4, 5 may be summarized as follows. THEOREM 6 Let T = (H,E) be a Markov representative of a VBS (X , S, (σs )s∈S , ⊗, ↓). Let Ri stands for the valuation marginalized to the set vi of variables and Sj stands for the marginal potential assigned to the separator of the pair {hi , hj }. Then R = ⊗{Vi |i = 1..n} = R1 ⊗ (⊗{(Ri R Si )|i = 2..n})  Sk :=  Rk↓k∩k+  0 Vk+,k−1 := (Vk+,k R Mk+→k ) ⊗ Sk  end 3. Calculate R1 := V1,1 end THEOREM 4 If Ri and Si have been calculated by the above algorithm for the Markov tree T, then R = ⊗{Vi |i = 1..n} = = R1 ⊗ (⊗{(Ri R Si )|i = 2..n}) where R stands for the joint valuation defined over X . PROOF: In any subtree Γk for any node i with predecessor j in <k except k and k+ we have, due to Lemma 3 0 Vi,k ⊗ (⊗{Vl,k |l ∈ (N (i)k − {j}}) =  = Vi,k ⊗ (⊗{Vl,k−1 ⊗ Ml→i |l ∈ (N (i)k − {j}}) = 0 = Vi,k ⊗ (⊗{Vl,k−1 |l ∈ (N (i)k − {j}})  where R stands for the joint valuation defined over X . Note that this theorem and the subsequent one are generalizations of theorems presented by Wierzchoń, [1995], in that the restricting condition that the removal operation has to satisfy the property (ρ ⊗ σ) R (δ ⊗ σ) = (ρ R δ) for any normal valuations ρ, σ and δ has been dropped. They represent also generalizations of properties of Dempster-Shafer belief functions presented in [Klopotek, 1994] and [Klopotek, 1995]. With the theorem 6 we can easily compute join valuations for subsets being set theoretical union of members of the family H. This fact presents Theorem 7 below. THEOREM 7 Let Γ = (N, F) be a subtree of a Markov tree T = (H, E) satisfying assumptions of Theorem 6 Assume that for each node hj ∈ H the marginal valuation, Rj , has been already computed. If hr stands for the root node in the subtree Γ then R↓∪N = ⊗{Vi |i ∈ H}↓∪N = = Rr ⊗ (⊗{(Ri R Si )|i ∈ N − {vr })} where ∪N stands the set theoretical union of all sets contained in N.  PROOF: The result is straight-forward if we recall the axiom CM1 and the separator property of the Markov tree. Q.e.d.2 Now, if h ⊆ ∪N then R↓h is computed as (R↓∪N )↓h . 0 0 Vk+,k ⊗Vk,k = (Vk+,k R Mk+→k )⊗(Vk,k ⊗Mk+→k ) = Xu [1995] proposed the local computation technique to find such a marginal: it is simple consequence of 0 = (Vk+,k R Mk+→k ) ⊗ Rk = Theorem 7 above and of Lemma 2.5 in [Shenoy, 1994]. hence update on passage of activation does not change the joint valuation. Also we have that  4  Query processing in VBS  The problem of query processing was formulated by Pearl [1988] first. In this approach we modify the original Bayesian belief network by adding new nodes with appropriate edges. Consider for instance the query q = (x1 ∧ x2 ) ∨ x3 - see ([Pearl, 1988], p. 224). Obviously, this q introduces new subset h = {x1 , x2 , x3 } to H. In Pearl’s approach we add two additional nodes joined to the original network by three edges. This approach suffers from several disadvantages. Adding new nodes to a belief network may change, even radically, the structure of the corresponding hypergraph which makes reconstruction of the Markov tree necessary, which is time-consuming. In the process, also Markov tree may change radically and practically all valuations have to be recalculated. In practice, query q may be frequently expressed in form of a single conjunction of elementary (that is mutually exclusive) expressions or a disjunction of a few conjunctions. E.g.: the query q = (x1 ∧ x2 ) ∨ x3 may be restated as q = (x1 ∧ x2 ∧ ¬x3 ) ∨ x3 where conjuctions (x1 ∧ x2 ∧ ¬x3 ) and x3 are mutually exclusive. It can be shown that calculation of such a query can be done without modification of Markov tree. Under probabilistic settings as well as in DST, if A and B are mutually excluding conditions then ρ(A ∨ B) = ρ(A)+ρ(B). (In our example: ρ((x1 ∧x2 ∧¬x3 )∨x3 ) = ρ(x1 ∧x2 ∧¬x3 )+ρ(x3 ). Therefore, in our approach we must simply compute R↓h (with h being the set of variables appearing in the query q), and next we should find valuations over the set of configurations logically equivalent to q. In our example we find valuation first for configuration: X1 = true, X2 = true, X3 = f alse, universe value for other variables, and then for configuration X3 = true, universe value for other variables. The only problem is to find the subtree Γ with h ⊆ ∪N. In [Wierzchon, 1995] it was shown that the minimal subtree, in the sense that ∪N is as small as possible, can be found by applying modified Graham’s test. The modification concerns step (1) of this test: a variable is deleted only if it does not belong to the set h. Consider e.g. again the hypergraph H1 = {{X1 , X7 , X8 }, {X2 , X5 , X6 , X7 }, {X3 , X6 }, {X4 , X5 }, {X8 , X9 , X10 }, {X10 , X11 , X12 }} (see Fig.1). We see that variables X1 , X3 , X4 , X11 , X12 are contained in only one edge, but X1 , X3 are in h. We delete only the other getting the hypergraph H10 = {{X1 , X7 , X8 }, {X2 , X5 , X6 , X7 }, {X3 , X6 }, {X5 }, {X8 , X9 , X10 }, {X10 }}. But then hyperedge {X5 } is contained in {X2 , X5 , X6 , X7 }, and {X10 } in {X8 , X9 , X10 }. So we delete them getting H100 = {{X1 , X7 , X8 }, {X2 , X5 , X6 , X7 }, {X3 , X6 } , {X8 , X9 , X10 }}. Now variables X2 , X5 , X9 , X10 are contained in only one edge each, however X2 is in h. We get H1000 = {{X1 , X7 , X8 }, {X2 , X6 , X7 }, {X3 , X6 }, {X8 }}. Now hyperedge {X1 , X7 , X8 } contains {X8 }, hence we get H10000 = {{X1 , X7 , X8 }, {X2 , X6 , X7 }, {X3 , X6 }}. X8 appears only in one edge, so we get finally H100000 = {{X1 , X7 }, {X2 , X6 , X7 }, {X3 , X6 }}. No further reduction by modified Graham test is possible. We con-  clude that out of 6 hyperedges of H1 only three {X1 , X7 , X8 }, {X2 , X5 , X6 , X7 }, {X3 , X6 } are necessary for query answering calculations (see Fig.2), and that {X2 , X5 , X6 , X7 } may be projected to {X2 , X6 , X7 }, and {X1 , X7 , X8 } onto {X1 , X7 }. This procedure is much more effective than that one suggested by Xu [1995], because this last method heavily depends on the topology of a Markov tree. We can, however, pose the question, whether or not the optimal subtree of the Markov tree T = (H, E) with hypertree H covering an original hypergraph S would be ”better” for query answering than an optimal hypertree cover T’ = (H’, E’) of the result of the above-mentioned modified Graham test run over the original hypergraph S. The answer to this question is rather ambiguous. We can clearly construct examples where T’ would be more optimal than the subtree T in terms e.g. of the maximum number of nodes in an edge. However, we must take into account that for each query not only T’ but also the valuation for each node of the tree T’ has to be calculated from the entire hypergraph S. But we do not need to do that with subtrees of T, because we have to calculate the Rj ’s for a given tree T once and we do not need to recalculate them when selecting a subtree, and if the subtree is small enough we save much calculation compared with processing of T’ (even if T’ has a more optimal structure for a given query). Concluding this paper we want to stress that this approach is implemented in the VBS system designed by our group.  References [Bertele & Brioschi, 1972] U. Bertele and F. Brioschi. Nonserial Dynamic Programming. Academic Press, NY, 1972. [Clifford & Preston, 1961] A.H. Clifford and G.B. Preston. The Algebraic Theory of Semigroups. American Mathematical Society, Providence, Rhode Island, vol. 1, (1961) [Cooper & Herskovits, 1992] G.F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9:309-347, 1992. [Jensen et al., 1990] F.V. Jensen, S.L. Lauritzen, and K.G. Olesen. Bayesian updating in causal probabilistic networks by local computations. Computational Statistics Quarterly, 4: 269-282, 1990. [Klopotek, 1994] M.A. Klopotek. Beliefs in Markov Trees - From Local Computations to Local Valuation. in: R. Trappl, ed.: Proc. EMCSR’94 Vol.1. pages 351-358, 1994. [Klopotek, 1995] M.A. Klopotek. On (Anti)Conditional Independence in DempsterShafer Theory to appear in Journal Mathware and Softcomputing, 1995.  [Lauritzen & Spiegelhalter, 1988] S.L. Lauritzen and D.J. Spiegelhalter. Local computation with probabilities on graphical structures and their application to expert systems. J. Roy. Stat. Soc., B50: pages 157-244, 1988. [Pearl, 1988] J. Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufman, 1988. [Shafer, 1976] G. Shafer. A Mathematical Theory of Evidence. Princeton University Press, Princeton, NJ, 1976. [Shenoy, 1989] P.P. Shenoy. A valuation-based language for expert systems. International Journal of Approximate Reasoning, 3:383-411, 1989. [Shenoy, 1991] P.P. Shenoy. Valuation-based systems for discrete optimization. in P.P. Bonissone, M. Henrion, L.N. Kanal and J.F. Lemmer, eds: Uncertainty in Artificial Intelligence 6, NorthHolland, Amsterdam, pages 385-400, 1991. [Shenoy, 1993] P.P. Shenoy. A new method for representing and solving Bayesian decision problems. in D.J. Hand, ed.: Artificial Intelligence Frontiers in Statistics: AI and Statistics III , Chapman & Hall, London, pages 119-138, 1993. [Shenoy, 1994] P.P. Shenoy. Conditional independence in valuation-based systems. International Journal of Approximate Reasoning, 10:203-234, 1994. [Shenoy & Shafer, 1986] P.P. Shenoy and G. Shafer. Propagating belief functions using local computations. IEEE Expert, 1(3), pages 43-52, 1986. [Thoma, 1991] H.M. Thoma. Belief function computations. in: I.R. Goodman et al (Eds.): Conditional Logics in Expert Systems, North-Holland, pages 269-308, 1991. [Wen, 1991] W.X. Wen. From relational databases to belief networks, in: B.D’Ambrosio, Ph. Smets, and P.P. Bonissone (Eds.), Proc. 7-th Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, pages 406-413, 1991. [Wierzchon, 1995] S.T. Wierzchoń. Markov-like properties of joint valuations, submitted, 1995. [Wong et al., 1993] S.K. Wong, Y. Xiang, and X. Nie. Representation of Bayesian networks as relational databases. in: D. Heckerman, and A. Mamdani, (Eds.), Proc. 9-th Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, pages 159-165, 1993. [Xu, 1995] H. Xu. Computing marginals for arbitrary subsets from marginal representation in Markov trees. Artificial Intelligence, 74, pages 177-189, 1995.  "
"Published as a conference paper at ICLR 2020  ATTRIBUTED G RAPH L EARNING WITH 2-D G RAPH C ONVOLUTION  arXiv:1909.12038v1 [cs.LG] 26 Sep 2019  Qimai Li∗ , Xiaotong Zhang∗ , Han Liu∗ & Xiao-Ming Wu† Department of Computing The Hong Kong Polytechnic University {csqmli,cshliu,csxtzhang,csxmwu}@comp.polyu.edu.hk,   1  I NTRODUCTION  In an attributed graph, each node is associated with a feature vector, and nodes are connected by edges encoding their relations. Commonly seen attributed graphs include citation networks where each node is a document represented by a bag-of-words feature vector and edges are citation links, web graphs where each webpage is also represented as a vector of words and edges are hyperlinks, social networks where each user is represented by a user profile vector and edges indicate user friendship, and protein-protein interaction networks where each protein is represented by a list of protein signatures and edges encode interactions between proteins. Learning on attributed graphs including node classification and clustering finds many important applications in real-world networks. Since the connectivity patterns and node contents of an attributed graph usually contain different information, it often requires joint modelling both aspects of information to achieve good learning performance. A major class of methods (Yang et al., 2015; Pan et al., 2016; Huang et al., 2017) is devoted to learning efficient node representations of an attributed graph via nonnegative matrix factorization or random walk statistics and then perform downstream learning tasks with the learned representations. A number of semi-supervised classification methods (Belkin et al., 2006; Weston et al., 2008; Yang et al., 2016) classify nodes in an attributed graph by training a supervised classifier on node features with some kind of graph regularizer. Recently, a series of works based on graph convolutional neural networks including ChebyNet (Defferrard et al., 2016), graph convolutional networks (GCN) (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017) and graph attention networks (GAT) (Velickovic et al., 2018) have been shown to achieve state-of-the-art performance in node classification and clustering (Kipf & Welling, 2016; Wang et al., 2017) on attributed graphs. The key component of these models is one-dimensional (1-D) graph convolution, a function that naturally combines graph structures and node contents by aggregating a node’s features with its ∗ †  Equal Contribution Corresponding author  1  Published as a conference paper at ICLR 2020  neighbours’. As shown in (Li et al., 2019), the graph convolutional operator used in GCN and many follow-up works acts as a low-pass graph filter that smooths a node’s features with its neighbours’. Under the assumption that nearby nodes tend to be in the same class, it can produce similar feature representations for nodes in the same class, thereby making them easier to be classified or clustered. While this works well for attributed graphs with clear cluster structures, real-world networks could be highly noisy and sparse. For example, in a web graph such as Wikipedia, a hyperlink between two webpages does not necessarily indicate that they belong to the same category, so mixing their features could be harmful for learning. Furthermore, it has been shown that many real-world networks are scale-free (Albert & Barabási, 2002), which means there exist many low-degree nodes. Since these nodes may have very few or even no links to other nodes, it would be difficult and even impossible for them to learn similar feature representations as other same-class nodes, which is an intrinsic limitation of the 1-D graph convolution that is commonly adopted in existing models. To address these limitations, we propose to explore relational information on a different dimension – the relations between feature attributes, in addition to node relations. Likewise, a relation between two feature attributes should reflect some kind of similarity between them. The assumption is that attributes that tend to indicate same classes should have strong relations. For example, in a citation network, node attributes are words, and documents of AI category usually contain words such as “learning”, “robotics”, “machine”, “neural”, etc. These indicative words for AI category should have much stronger relations among themselves than with other non-indicative words. These informative relations can be used to construct an attribute affinity graph to smooth node features in a similar way as the node relations do, only in a different dimension. Importantly, attribute relations can complement node relations in node representation learning. For instance, consider a document that has no links to others and hence it is impossible to do feature smoothing with node relations. But with attribute relations, it can still learn similar feature representations as other same-class nodes. In this paper, we make the following contributions. • Methodology: We propose to use 2-D graph convolution to jointly model node relations and attribute relations for learning node representations of attributed graphs. Further, we develop a computationally efficient dimensionwise separable 2-D graph convolution (DSGC), which is equivalent to performing 1-D graph convolution alternately on the node dimension and the attribute dimension respectively. • Theoretical Insight: We show that regular 1-D graph convolution on the node dimension can reduce intra-class variance of node features, which explains the success of many existing methods. Further, we show that with a properly constructed attribute affinity graph, graph convolution on the attribute dimension can also reduce intra-class variance of node features. Jointly, our analysis provides a theoretical justification of DSGC. • Empirical Study: We implement DSGC for node classification and clustering on attributed graphs, and compare it with state-of-the-art methods on a citation network, a web graph, and an email network. The results demonstrate the superiority of DSGC over regular 1-D graph convolution on spare and noisy real-world attributed networks. We also show that DSGC can be plugged into existing models to substantially improve their performance.  2  R ELATED W ORKS  Structural Graph Learning. Methods for structural graph learning only utilize graph structures (node relations). A common approach is to learn smooth low-dimensional embeddings of nodes using Markov random walks (Szummer & Jaakkola, 2002; Perozzi et al., 2014; Grover & Leskovec, 2016), Laplacian eigenmaps (Belkin & Niyogi, 2004), spectral kernels (Chapelle et al., 2003; Zhang & Ando, 2006), autoencoders (Wang et al., 2016; Cao et al., 2016; Ye et al., 2018) and generative adversarial nets (Dai et al., 2018; Wang et al., 2018). Another direction is graph-based semi-supervised classification, which includes methods based on low-density graph partition (Blum & Chawla, 2001; Blum et al., 2004; Joachims, 2003) and the popular label propagation method and its variants (Zhu et al., 2003; Zhou et al., 2004; Bengio et al., 2006; Hein & Maier, 2007; Wu et al., 2012). Attributed Graph Learning. Methods for attributed graph learning take into account both graph structures and node features. A major class of methods learns node representations (Yang et al., 2015; Pan et al., 2016; Huang et al., 2017) or clustering nodes (Xia et al., 2014; Zhou et al., 2010; 2  Published as a conference paper at ICLR 2020  Li et al., 2018) by applying nonnegative matrix factorization, random walk statistics, or Laplacian eigenmaps on both graph structures and node features. Some node clustering methods use a Bayesian model (Xu et al., 2012; Bojchevski & Günnemann, 2018) or design a distance measure that trades off structural and feature information (Zhou et al., 2009; Cheng et al., 2011). Statistical relational learning methods model node relations and features with probabilistic graphical models, e.g., relational Markov networks (Taskar et al., 2002). Another category of related work is the graph-based semi-supervised node classification methods that exploit both graph structures and node features. For example, iterative classification algorithm (Sen et al., 2008) iteratively classifies an unlabeled node using its neighbours’ labels and features. Manifold regularization (Belkin et al., 2006), deep semi-supervised embedding (Weston et al., 2008), and Planetoid (Yang et al., 2016) classify node features by training a supervised classifier with a Laplacian or embedding-based regularizer. Graph Neural Networks. Another line of research on attributed graph learning is based on graph neural networks (Scarselli et al., 2009; Li et al., 2016). Inspired by the success of convolutional neural networks (CNN) on Euclidean data, recent works (Bruna et al., 2014; Henaff et al., 2015; Duvenaud et al., 2015; Atwood & Towsley, 2016) proposed to use 1-D graph convolution for attributed graph learning. To avoid the expensive eigen-decomposition, ChebyNet (Defferrard et al., 2016) uses a polynomial filter represented by k-th order polynomials of graph Laplacian via Chebyshev expansion. Graph convolutional networks (GCN) (Kipf & Welling, 2017) further simplifies ChebyNet by designing an efficient layer-wise propagation rule via a first-order approximation of spectral graph convolution. GCN achieved outstanding results in semi-supervised classification and inspired many follow-up works including MoNet (Monti et al., 2017), GraphSAGE (Hamilton et al., 2017), graph attention networks (Velickovic et al., 2018), gated attention networks (Zhang et al., 2018a), FastGCN (Chen et al., 2018b), dual graph convolutional neural network (Zhuang & Ma, 2018), stochastic GCN (Chen et al., 2018a), Bayesian GCN (Zhang et al., 2018b), attributed network representation learning (Zhang et al., 2018c), LanczosNet (Liao et al., 2019), deep graph infomax (Velickovic et al., 2019), graph Markov neural networks (Qu et al., 2019), DisenGCN (Ma et al., 2019), MixHop (Abu-El-Haija et al., 2019), etc. Some attributed graph clustering methods based on 1-D graph convolution also showed promising performance, including graph autoencoder and graph variational autoencoder (Kipf & Welling, 2016), marginalized graph autoencoder (Wang et al., 2017), adversarially regularized graph autoencoder and adversarially regularized variational graph autoencoder (Pan et al., 2018), and adaptive graph convolution (Zhang et al., 2019). More comprehensive reviews of graph neural networks can be found in (Cai et al., 2018; Zhang et al., 2018d; Zhou et al., 2018).  3  2-D G RAPH C ONVOLUTION  In this section, we present 2-D graph convolution for attributed graph learning. A comprehensive introduction of multi-dimensional graph Fourier transform can be found in (Kurokawa et al., 2017). Different from Kurokawa et al. (2017), here we propose a localized 2-D graph convolution to circumvent the computationally intensive graph Fourier transform. Furthermore, we propose an even simpler dimensionwise separable 2-D graph convolution to efficiently model both node relations and attribute relations along the two dimensions of the feature matrix of an attributed graph. 3.1  2-D G RAPH S IGNAL  A 2-D graph signal is a function defined on the Cartesian product of the vertex sets of two graphs. Formally, given two weighted undirected graph G (1) and G (2) , denote the vertex sets by V (1) and V (2) , the edge set by E (1) and E (2) , and the weighted adjacency matrix by A(1) and A(2) . A 2D graph signal x on (G (1) , G (2) ) is a real-valued function f : V (1) × V (2) → R, which can be conveniently represented in matrix form: (1)  X = (xij ) ∈ Rn×m ,  (2)  xij = f (νi , νj ),  (1)  where n = |V (1) | and m = |V (2) |. In this paper, G (1) represents the given node graph; G (2) represents the constructed attribute affinity graph; and X is the node feature matrix, which is a 2-D signal on the two graphs. We call the adjacency matrices A(1) and A(2) node affinity matrix and attribute affinity matrix respectively. 3  Published as a conference paper at ICLR 2020  3.2  2-D G RAPH F OURIER T RANSFORM  Define the graph Laplacian of G (1) and G (2) as Ll = D (1) − A(1) and Lr = D (2) − A(2) respectively. Denote by λi and µj the eigenvalues of Ll and Lr , and U = [u1 , · · · , un ] and V = [v1 , · · · , vm ] the corresponding eigenbasis respectively, then the n × m outer products ui vj> form a basis for the linear space Rn×m . It is known as 2-D graph Fourier basis – an analogy of the Fourier basis in classical harmonic analysis in graph domain. The corresponding eigenvalue pair (λi , µj ) is known as the frequency of basis matrix ui vj> . Then, a 2-D graph signal X can be decomposed as: X X= sij (ui vj> ) = U SV > , S = (sij ) ∈ Rn×m , (2) ij  where S is called the spectrum of signal X or Fourier coefficients and S = U > XV is 2-D graph Fourier transform. 3.3  2-D S PECTRAL G RAPH C ONVOLUTION  Based on 2-D graph Fourier transform, we can now manipulate 2-D graph signals in the spectral (frequency) domain and define 2-D spectral graph convolution. By the convolution theorem, the convolution of two signals equals to point-wise multiplication of their spectrum in the spectral domain. 2-D spectral graph convolution is a function conv : Rn×m → Rn×m that takes signal X as input and outputs a new signal Z: X Z= p(λi , µj )sij (ui vj> ) = U (S ◦ P )V > , (3) ij  where p(λ, µ) : R × R → R is the frequency response of the convolution; ◦ is Hadamard (elementwise) product; and P ∈ Rn×m with Pij = p(λi , µj ) is the frequency response in matrix form. 3.4  FAST L OCALIZED 2-D S PATIAL G RAPH C ONVOLUTION  Although Eq. (3) well defines 2-D graph convolution, it is often impractical to perform convolution in the spectral domain, due to the high cost of computing the eigenbasis U , V needed for Fourier transform. Similar to (Defferrard et al., 2016) on 1-D graph convolution, here we propose 2-D spatial graph convolution to avoid intensive computation. Without loss of generality, we restrict the frequency response p(·, ·) to be a polynomial of two variables λ, µ with parameters Θ ∈ Rn×m , i.e., P p(λ, µ) = i,j θij λi µj . Then, the 2-D spectral graph convolution in Eq. (3) becomes Z=  n−1 X m−1 X  θij Lil XLjr .  (4)  i=0 j=0  Eq. (4) is called 2-D spatial graph convolution, as it manipulates the signal X in the spatial domain. Parameter Θ is called the kernel of the convolution. Here, the spatial convolutional filter is localized. Denote by k1 and k2 the largest exponent of λ and µ in the polynomial p respectively, then i > k1 (1) (2) and j > k2 imply θij = 0. The convoluted signal zij of vertex pair (νi , νj ) only depends on the (1)  (2)  neighbourhood of νi within k1 hops and the neighbourhood of νj within k2 hops, so the filter is said to be k1 -localized on G (1) and k2 -localized on G (2) , and the size of the kernel Θ is k1 × k2 . 3.5  D IMENSIONWISE S EPARABLE 2-D G RAPH C ONVOLUTION (DSGC)  Although the above spatial graph convolution avoids the computationally expensive Fourier transform, its general form with kernel size k1 × k2 still involves at least k1 × k2 matrix multiplications. Inspired by the depthwise separable convolution proposed in (Howard et al., 2017), we streamline spatial graph convolution by restricting the rank of Θ to be one. Consequently, Θ is decomposed as an outer product of two vectors θ (1) ∈ Rn and θ (2) ∈ Rm . The frequency response p can be decomposed as a product of two single variable polynomials, i.e., p(λ, µ) = p1 (λ)p2 (µ), where P (1) P (2) p1 (λ) = i θi λi and p2 (µ) = j θj µj . Finally, the 2-D spatial graph convolution in Eq. (4) becomes Z = GXF , where G = p1 (Ll ) and F = p2 (Lr ). (5) 4  Published as a conference paper at ICLR 2020  Att1 Att2 Att3 Att4  a 1 b  1  c  1  d  1  e  1  X  Node affinity graph a e b c d a b c a 2/3 1/3  d  e  d  c  Att2  d  Att1 Att2 Att3 Att4  a 1/2 1/2 b 1/2 1/2  1 1  1  Att3  Att1 Att2 Att3 Att4  1  1  e  Att1  Att4  GX  1/3 2/3  Attribute affinity graph  b 1/3 2/3  e  b 1/3 1/3 1/3 c  Att1 Att2 Att3 Att4  a 2/3 1/3  Att1  1/2 1/2  Att2  1/2 1/2  c 1/2 1/2 d  1/2 1/2  e  1/2 1/2  GXF  Att3  1/2 1/2  Node of Class 1  Att4  1/2 1/2  Node of Class 2  G  F  Attribute  Figure 1: Illustration of DSGC. We call Eq. (5) dimensionwise separable graph convolution (DSGC). The fastest way to compute it only requires k1 + k2 matrix multiplications, much less than the k1 × k2 matrix multiplications needed by a general 2-D spatial graph convolution. We call GX node graph convolution and G the node graph convolutional filter. Similarly, we call XF attribute graph convolution and F the attribute graph convolutional filter. Notably, G and F can complement each other to learn better node representations. As illustrated in Fig. 1, the node affinity graph only captures node relations of Class 1, and applying node graph convolution on the feature matrix X smooths the node features of Class 1, but it does not affect those of Class 2. Fortunately, the attribute affinity graph encodes complementary relational information on another dimension, and further applying attribute graph convolution makes the node features of Class 2 similar and those of Class 1 even more similar, thereby making them much easier to be classified or clustered.  4  I NTRA - CLASS VARIANCE R EDUCTION BY DSGC  Given a data distribution, the lowest possible error rate an classifier can achieve is the Bayes error rate (Fukunaga, 2013), which is caused by the intrinsic overlap between different classes and cannot be avoided. In this section, we show that DSGC with proper graph convolutional filters can reduce intra-class variance of the data distribution while keeping class centers roughly unchanged, hence reducing the overlap between classes and improving learning performance. Intra-class Variance and Inter-class Variance. Suppose samples xi and their labels yi are observations of a random vector X = [X1 , · · · , Xm ]> and a random variable Y respectively. We define the variance of random vector X to be the sum of the variance of each dimension Xj , i.e., the trace of the covariance matrix of X. According to law of total variance (Grinstead & Snell, 2012), the variance of X can be divided into intra-class variance and inter-class variance: Var (X) = E [Var (X|Y)] + Var (E [X|Y]) , | {z } | {z } Intra-class Variance  (6)  Inter-class Variance  where the conditional variance Var (X|Y = k) is the variance of class k and the conditional expectation E [X|Y = k] is the k-th class center. Intra-class variance (IntraVar) measures the average divergence within each class, while inter-class variance (InterVar) measures the divergence among class centers. We are interested in the IntraVar/InterVar ratio. Here, we assume that each class of data Pr (X|Y = k) follows a unimodal distribution (e.g. Gaussian, chi-square, Laplace), a class of most common distributions in the real world, and with roughly convex contours. Under this assumption, a low IntraVar/InterVar ratio generally indicates low classification error, since the class overlap is reduced. This is also verified by our experiments on real attributed networks in section 7. 4.1  I NTRA - CLASS VARIANCE R EDUCTION BY N ODE G RAPH C ONVOLUTION  As variance of sample mean is always less than variance of individual samples, averaging samples of the same class can always reduce intra-class variance. Actually this is how node graph convolution works. For any node i, node graph convolution GX produces a new feature vector: X zi = Gij xj . (7) j  5  Published as a conference paper at ICLR 2020  When G is a stochastic matrix, the output feature vector zi is a weighted average of the neighbours of xi . Denote by Z a random vector of zi . Intuitively, as long as each node i has enough same-class neighbours, Z will have a smaller IntraVar/InterVar ratio than X. Formally, assume that nodes from the same class are connected with probability p, and nodes from different classes are connected with probability q, i.e., the adjacency matrix A(1) of the node graph G (1) is generated by   p, if yi = yj 1 − p, if yi = yj Pr(aij = 1) = and Pr(aij = 0) = . (8) q, if yi 6= yj 1 − q, if yi 6= yj Then, with the stochastic graph filter G = D −1 A(1) , we have the following theorem. Theorem 1. When q is sufficiently small, the IntraVar/InterVar ratio of Z is less than or equal to that of X, i.e., E [Var (Z|Y)] E [Var (X|Y)] ≤ . (9) Var (E [Z|Y]) Var (E [X|Y]) The proof is given in the Appendices. Under the assumption that nodes in the same class are most likely to be connected, node graph convolution G can reduce intra-class variance while keeping inter-class variance roughly unchanged, thereby decreasing the IntraVar/InterVar ratio. 4.2  I NTRA - CLASS VARIANCE R EDUCTION BY ATTRIBUTE G RAPH C ONVOLUTION  In the following, we show that a proper attribute graph convolutional filter F can also reduce the IntraVar/InterVar ratio. We use the convention that the random vector X is a column vector, and hence the attribute graph convolution XF results in a new random vector F > X. We also assume that the node features are mean-centered, i.e. E [X] = 0. Denote by Cov(k) = Cov (X|Y = k) the covariance matrix of X w.r.t. each class, and denote by πk = Pr(Y = k) the portion of each class. Then, after attribute graph convolution, the intra-class variance of data becomes   X X (k) E Var F > X|Y = πk Covij (F F > )ij (10) ij  k  Theorem 2. If the attribute graph convolutional filter F is a doubly stochastic matrix, then the output of attribute graph convolution has an intra-class variance less than or equal to that of X, i.e., X X   Fij = Fij = 1 and Fij ≥ 0, ∀ i, j ⇒ E Var F > X|Y ≤ E [Var (X|Y)] . i  j  The proofs of Eq. (10) and Theorem 2 are given in the Appendices. Eq. (10) suggests that to reduce intra-class variance, the attribute affinity graph should connect attributes Xi and Xj with small or (k) even negative covariance Covij . For example, in Figure 1, the attributes Att3 and Att4 are both indicative of class 2, but negatively correlated w.r.t. class 2, so connecting them in the attribute affinity graph can greatly reduce the intra-class variance of class 2. To achieve a low IntraVar/InterVar ratio, in addition to reducing intra-class variance, we also need to keep the class centers apart after convolution, which then depends on the quality of the attribute affinity graph. A good attribute affinity graph should connect attributes that share similar expectations conditioned on Y. Formally, each attribute Xj has K conditional expectations w.r.t. Y, which are denoted as a vector ej = (E [Xj |Y = 1] , · · · , E [Xj |Y = K]) ∈ RK . We have the following. P Theorem 3. If ∀Fij 6= 0, kei − ej k2 ≤ ε, then the distance between ej and ebj = i Fij ei is also less than or equal to ε, i.e., kei − ej k2 ≤ ε, ∀Fij 6= 0 ⇒ kej − ebj k2 ≤ ε, and ε can be arbitrarily small with a proper F . By Theorem 3, since the conditional expectations of each attribute may change little after attribute graph convolution, one can infer that the class centers will also change little, and so does the interclass variance. Combining Theorems 2 & 3, it suggests that a proper attribute affinity graph should connect attributes that have similar class means but are less positively correlated, so as to achieve a low IntraVar/InterVar ratio and improve performance. Again, it can be seen in Figure 1 that the attributes Att3 and Att4 have exactly the same class means, but are negatively correlated w.r.t. class 2, and connecting them in the attribute affinity graph reduces the intra-class variance of class 2 to 0. 6  Published as a conference paper at ICLR 2020  5 5.1  ATTRIBUTED G RAPH L EARNING WITH DSGC U NSUPERVISED N ODE R EPRESENTATION L EARNING  Given an attributed graph with node feature matrix X, we can learn the node representations Z in an unsupervised manner by applying DSGC on X, i.e., Z = GXF ,  (11)  and then perform various downstream learning tasks with the learned node representations Z. Node Classification. With the node representations Z, we simply train a classifier such as multilayer perceptron with the representations of the labeled nodes, and then apply the trained classifier on the representations of the unlabeled nodes to predict their labels. Node Clustering. With the node representations Z, we propose to cluster nodes as follows. We first apply a linear kernel on Z to learn pairwise proximity between nodes, i.e., K = ZZ > , and then perform spectral clustering (Perona & Freeman, 1998; Von Luxburg, 2007) on K. 5.2  E ND - TO -E ND S EMI -S UPERVISED L EARNING  For semi-supervised learning on attributed graphs, we can also use DSGC to replace the 1-D graph convolution used in existing end-to-end semi-supervised learning models including GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018) and GraphSAGE (Hamilton et al., 2017). For example, to incorporate DSGC into the vanilla GCN, we can modify the first layer propagation of GCN as: H (1) = σ(GXF W (1) ),  (12)  where H (1) is the hidden units in the first layer, W (1) ∈ Rm×l is the trainable parameters of GCN, and σ is a nonlinear function such as ReLU. Both G and F are fixed filters, where G is the node graph convolutional filter of GCN, and F is the proposed attribute graph convolutional filter. Importantly, Eq. (12) can be considered as feeding a filtered feature matrix XF instead of the raw feature matrix X to GCN. By our above analysis, a proper attribute graph filter F can reduce intra-class variance, which makes XF much easier to classify and guarantees to help train a better model. Further, it can be shown that the model trained by Eq. (12) is essentially different from that of GCN almost surely. For GCN, the model is freely chosen from the parameter space W (1) , while the model trained by Eq. (12) is restricted in a subspace F W (1) . Since F is low-pass (section 6.2), it is also low-rank, and F W (1) is a small subspace of Rm×l projected by F . Model parameters in this subspace are generally better in terms of the generalization performance (test accuracy), due to the variance reduction property of F . However, the model learned by Eq. (12) can hardly be learned by GCN, since the subspace F W (1) has measure zero, which is an extremely small subset of Rm×l .  6 6.1  I MPLEMENTATION OF DSGC N ODE G RAPH C ONVOLUTIONAL F ILTERS  The node affinity graph (A(1) ) is given as part of the dataset. There are various graph convolutional filters available (Li et al., 2019), e.g., the one used in GCN (Kipf & Welling, 2017) is a symmetrically normalized node affinity matrix. In our experiments, we use a row normalized node affinity matrix (consistent to our analysis in section 4.1) of order 2 (following GCN) as the filter for node graph convolution: G = (D1−1 A(1) )2 , (13) where D1 is the degree matrix of A(1) . We observe in our experiments that the performance of the symmetrically normalized node affinity matrix (GCN filter) is similar. 6.2  ATTRIBUTE G RAPH C ONVOLUTIONAL F ILTERS  A key issue in implementing DSGC is to construct the attribute affinity graph (A(2) ). Possible ways of constructing A(2) include extracting entity relation information from existing knowledge bases, 7  Published as a conference paper at ICLR 2020  building a similarity graph from features, or identifying correlations by domain knowledge. In the following, we describe two methods for text data, which have been proven useful by our experiments. Positive point-wise mutual information (PPMI). Positive PMI (Church & Hanks, 1990) is a common tool for measuring the association between two words in natural language processing. Given a pair of words wi and wj , the edge weight is defined as the PPMI between wi and wj :   p(wi , wj ) (2) aij = PPMI(wi , wj ) = log , (14) p(wi )p(wj ) + where p(wi , wj ) and p(wi ) are learned by sliding a window over a large corpus of text. PMI reflects word collocation, as it assumes that if two words co-occur more than expected under independence, there must be some kind of semantic relation between them, which is often true in practice. With the constructed attribute affinity graph A(2) by PPMI, in our experiments, we use a symmetrically normalized affinity matrix as the filter: −1  −1  F = D2 2 A(2) D2 2 ,  (15)  where D2 is the degree matrix of A(2) . Word embedding based k-NN graphs. Word embedding is a collection of techniques that map vocabularies to vectors in a Euclidean space. Embeddings of words are pre-trained vectors learned from corpus with algorithms such as GloVe (Pennington et al., 2014). Since word embeddings capture semantic relations between words (Bakarov, 2018), they can be used for constructing an attribute affinity graph. With the embedding vectors, we can construct a k-NN graph with some proximity metric such as the Euclidean distance. With the constructed attribute affinity graph A(2) , in our experiments, we use the following one-step lazy random walk filter (Li et al., 2019): −1  −1  F = (I + D2 2 A(2) D2 2 )/2,  (16)  where D2 is the degree matrix of A(2) . Note that the filters in Eq. (15) and (16) are slightly different, since the diagonal weight of the affinity matrix constructed by word embedding is much smaller than that of the PPMI affinity matrix, and we want to assign sufficient weight to each attribute itself. Also, we use filters of order 1 since the PPMI affinity matrix is very dense. According to the analysis in Li et al. (2019), Eq. (15) and (16) are both low-pass filters.  7  E MPIRICAL S TUDY  To validate the effectiveness of DSGC, we conduct extensive experiments for semi-supervised node classification and node clustering on three large real-world attributed networks including 20 Newsgroups (20 NG) (Lang, 1995), Wikispeedia (Wiki) (West et al., 2009; West & Leskovec, 2012) and Large Cora (L-Cora) (McCallumzy et al., 1999; Li et al., 2019). 1 Due to space limitation, details of datasets, experimental setup and computational time are provided in the Appendices. 7.1  VARIANCE R EDUCTION  First of all, to verify our analysis in section 4, we show the variance reduction effect of both node graph convolution and attribute graph convolution. As shown in Figure 2, 1-D graph convolution with either G or F already greatly reduces the IntraVar/InterVar ratio, and together they further significantly reduce the ratio. In Figure 3, we visualize the results of performing graph convolution on the node features of 20 NG dataset by t-SNE. It can be seen that graph convolution can successfully reduce the overlap among classes, and 2-D graph convolution is more effective than 1-D. 7.2  N ODE C LASSIFICATION  Baselines. We test the proposed node classification method by DSGC with or without using the node graph convolutional filter (G) and the attribute graph convolutional filter (F ) in five cases. We use 1 Note that we did not use the “Cora”, “Citeseer” and “PubMed” datasets as in (Kipf & Welling, 2017; Yang et al., 2016; Sen et al., 2008), since the attribute (word) lists are not provided in these datasets.  8  Published as a conference paper at ICLR 2020  Table 1: Classification accuracy. Label rate  20 labels per class  5 labels per class  Methods  G  F  20 NG  Wiki  L-Cora  20 NG  Wiki  L-Cora  MLP LP GLP GCN GAT DGI GraphSAGE  7 3 3 3 3 3 3  7 7 7 7 7 7 7  65.77 ± 0.22 16.39 ± 0.20 76.21 ± 0.18 76.14 ± 0.24 75.16 ± 0.25 73.34 ± 0.27 65.73 ± 0.17  60.86 ± 0.69 9.53 ± 0.05 33.42 ± 1.44 48.65 ± 0.65 48.88 ± 0.68 49.70 ± 1.63 65.52 ± 0.62  51.05 ± 0.71 55.77 ± 0.97 67.58 ± 1.06 66.69 ± 0.98 66.49 ± 1.01 61.39 ± 0.50 57.28 ± 0.71  36.10 ± 1.11 8.62 ± 0.20 47.86 ± 1.63 47.70 ± 1.64 49.14 ± 1.55 66.57 ± 0.63 42.48 ± 0.77  29.95 ± 1.04 10.54 ± 0.19 15.38 ± 1.37 38.30 ± 1.48 36.90 ± 1.75 43.64 ± 1.89 48.81 ± 0.76  33.56 ± 2.43 38.97 ± 3.15 51.04 ± 1.61 48.62 ± 1.81 49.27 ± 2.25 54.77 ± 1.24 46.79 ± 1.91  3 7 76.27 ± 0.20 49.78 ± 0.50 3 PPMI 81.91 ± 0.20 58.35 ± 0.52 DSGC (ours) 3 Emb 77.38 ± 0.14 58.66 ± 0.65 7 PPMI 76.10 ± 0.21 69.53 ± 0.36 7 Emb 68.08 ± 0.22 68.43 ± 0.37 ? 3 and 7 indicate using/not using G or F .  67.16 ± 1.04 67.60 ± 0.82 68.38 ± 0.85 58.54 ± 0.79 55.34 ± 0.66  47.92 ± 1.57 70.35 ± 0.72 55.24 ± 1.21 61.45 ± 0.74 43.41 ± 0.99  43.79 ± 1.48 46.72 ± 1.66 45.71 ± 1.64 58.44 ± 1.48 54.25 ± 0.99  51.93 ± 1.46 54.07 ± 1.13 52.87 ± 1.56 44.58 ± 2.00 38.13 ± 2.20  X  GX  XF, F=PPMI  XF, F=Emb  GXF, F=PPMI  GXF, F=Emb  128.0 32.0  49.8 33.6  28.0  16.0 8.0  Methods  26.7 26.5 23.7  8.5 5.8  5.7  6.8  Wiki  L-Cora  GAT GCN  7 76.14 ± 0.24 48.65 ± 0.65 66.69 ± 0.98 PPMI 81.80 ± 0.22 60.22 ± 0.71 65.83 ± 0.87  9.4 8.3  4.0 2.0 1.0  20 NG  20 NG  7 75.16 ± 0.2"
"Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  Artificial Intelligence BlockCloud (AIBC) Technical Whitepaper Prof. Qi Deng Founder, CEO and Chief Scientist of Cofintelligence Financial Technology Ltd. qi.deng@cofintelligence.ai   1  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  1  Introduction  1.1  Artificial Intelligence Blockchain (AIBC) The AIBC is an Artificial Intelligence (AI) based blockchain ecosystem. Anchored  on the principles of decentralization, scalability, and controllable cost. Based on the principles of decentralization, scalability and controllable cost, the AIBC seeks to overcome the drawbacks of centralized, non-scalable and high cost traditional cloud computing. AIBC provides a perfect platform for distributed industry solutions (DSOLs) by leveraging the basic blockchain technology and provides system-wide sharing of computing power and storage space. The AIBC stresses on application support. It provides a flexible technical support infrastructure for distributed services of large business scenarios. Its AI-based fundamental layer Delegated Adaptive Byzantine Fault Tolerance (DABFT) distributed consensus enables technical teams in a variety of industries to focus on their own domain improvement without having to understand the underlying blockchain technology. The AIBC emphasizes on ecosystem expansion. Our vision is to build a cross-chain, cross-system, cross-industry, cross-application and cross-terminal distributed and trusted ecosystem. Based on an innovative economic model, the AIBC’s Delegated Proof of Economic Value (DPoEV) incentive consensus enables connections among diverse computing, data and information entities.  Therefore, multi-dimensional business  scenarios can be formed with consensus and trust with standalone yet interconnected DSOLs. The AIBC allows individualized customization based on choices of protocols, modules, and rules. Thus application scenarios in the AIBC ecosystem can be customized according to differentiated requirements of multiple entities on a public chain that provides common bottom-layer services. The customization is from several perspectives, including but not limited to:  2  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  1. Technical perspective: The AIBC provides customization based on an entity’s technical requirements, such as access mechanism, encryption requirement, consensus (DABFT), storage method, etc. 2. Application perspective: The AIBC provides customization based on an entity’s industry standards and guidelines for resource sharing across different domains. 3. Governance perspective: The AIBC provides customization based on the laws, rules, and regulations of the jurisdiction in which an entity resides. In a nutshell, the mission of the AIBC is to become the value exchange hub with the blockchain technology at its foundation. The value in the AIBC is essentially the knowledge that existed in and accumulated by participating entities. The entities then participate in exchanges of values through resource sharing activities, facilitated by token (unit of economic value) transfers. The benefits of the AIBC are then value creation and exchange across entities.  1.2  Blockchain Overview  1.2.1 Bitcoin and Ethereum After the outburst of the 2008 financial crisis, Satoshi Nakamoto publishes a paper titled “Bitcoin: A Point-to-Point Electronic Cash System,” symbolizing the birth of cryptocurrencies (Nakamoto, 2008). Vitalik Buterin (Buterin, 2013) improves upon the Bitcoin with a public platform that provides a Turing-complete computing language, the Ethereum, which introduces the concept of smart contracts, allowing anyone to author decentralized applications where they can create their own arbitrary rules for ownership, transaction formats, and state transition functions. The Bitcoin and Ethereum are the first batches of practical blockchain applications that make use of distributed consensus, decentralized ledger, data encryption and economic incentives afforded by the underlying blockchain technology. Essentially, the blockchain technology enables trustless peer-topeer transactions and decentralized coordination and collaboration among unrelated  3  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  parties, providing answers to many challenges unsolvable by the traditional centralized institutions, including not but limited to, low efficiency, high cost, and low security. 1.2.2. Blockchain Classification Based on the user’s accessibility or the degree of openness, there are three types of blockchains: public chain, coalition chain, and private chain. A public chain is the most open and anyone can participate in its development and maintenance. Several key benefits of the public chain are: the data is readily accessible by all users; it is easy to deploy applications, and it is completely decentralized without any centralized control. A private chain is the most closed, and its accessibility is limited to the concerned private parties. While a private chain does not completely solve the problem of trust, it improves auditability. A coalition chain is semi-open and requires a registered license to access, thus open to only coalition members. The scale of a coalition can be as large as different institutions and countries. Table 1.1 compares and contrasts between the three types of blockchains.  Comparison of Three Different Forms of Blockchain Participant Consensus mechanism Bookkeeper Incentive mechanism Degree of Centralization Salient Features Carrying Capacity Typical Scene  Public Chain  Coalition Chain  Private Chain  anyone PoW/PoS/DPoS/P BFT all participants  coalition members distributed conformance algorithm elected members  personal or internal distributed conformance algorithm user-defined  necessary  optional  unnecessary  decentralization  multi-centralization  (multi-)centralization  self-established credibility  efficiency and cost optimization  transparency and traceability  3-20 deals/s  1000-1w deals/s  1000-10w deals/s  virtual digital currency  payment, settlement  audit, issuance  Table 1.1 – Comparisons of Blockchains by User Accessibility  4  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  1.2.3 Blockchain Technology Architecture From the perspective of architecture design, the blockchain can be divided into three levels: the protocol layer, the intermediate layer, and the application layer. They are independent but not inseparable. The protocol layer consists of a complete suite of blockchain protocols; it is similar to a computer operating system. It can be further divided into storage and network layers, where the network nodes are maintained. The integrity of the protocol layer ensures the high credibility of the system. The protocol layer consists of three components: core technology, core application, and supporting facilities. The core technology component offers the basic protocols and algorithms that the blockchain system depends on, including communication, storage, security and consensus mechanisms. The core application component is built upon the core technology component and provides functions for different application scenarios, such as smart contracts, programmable assets, incentives, etc. The supporting facilities provide resources and tools to the core application component that makes the development process more efficient. Figure 1.1 illustrates the blockchain architecture.  Figure 1.1 – The Blockchain Architecture 5  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  The intermediate layer is also called the expansion layer, which is further divided into two categories: various trading markets (exchanges) that provides channels for the conversions between cryptocurrencies and legal tenders, and the implementation expansion to certain directions, such as the smart contracts. The application layer is similar to various applications in a computer environment, upon which many applications can be built. 1.2.4 Blockchain Ecosystem Bitcoin, the pioneer of the blockchain’s distributed ledger and distributed database revolution, is widely regarded as the “Blockchain 1.0.” The “Blockchain 2.0” is represented by Ethereum, which adds a smart contract mechanism to the Bitcoin Foundation. The blockchain is entering its 3.0 era: it is experiencing a proliferation of application scenarios with no apparent scope limitation. It has the potential to become the low-level protocol of the “Internet of Everything.” The blockchain applications now cover supply chain finance, transportation, medical and health, culture and media, property right certification, charity and donation management, etc., just to name a few. From an application scenario perspective, the blockchain technology addresses three core issues, which are explained below: 1. Business applications can benefit from the fact that the data on the chain are of mutual recognition and mutual verification from multiple entities. Thus, data verification costs and security risk for commercial transactions can be significantly reduced, while at the same time the ease of transactions is improved, and the transactions are more deterministic. 2. The blockchain technology is natural for supply chain applications. All participants of a supply chain can help establish and maintain rules and incentive mechanisms, promote collaboration and interoperability, and enhance transparency. 3. The blockchain enables the establishment of distributed databases to solve the trust problem. A blockchain-based database offers trusted and distributed data storage and sharing, is secure and tamper-proof, and incorporates a digital 6  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  contract platform. Based on these characteristics, the blockchain enables buildup of a social network based, shared data storage, bridging nodes that belong to different entities. This technological approach fundamentally resolves the crossentity trust issue. Figure 1.2 illustrates the blockchain ecosystem.  Figure 1.2 – The Blockchain Ecosystem  1.3  AIBC Overview  1.3.1 AIBC Technological Innovations One of the main innovations of the AIBC is separating the fundamental (blockchain) layer distributed consensus and the application layer incentive mechanism. Prof. Deng proposed the DABFT (Delegated Adaptive Byzantine Fault Tolerance) as the fundamental layer distributed consensus algorithm. The DABFT improves upon the ADAPT algorithm (Bahsoun, Guerraoui and Shoker, 2015) and uses deep learning (a branch of Artificial Intelligence) techniques to predict and dynamically select the most suitable Byzantine Fault Tolerant (BFT) algorithm for the current application scenario in 7  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  order to achieve the best balance of performance, robustness and security. The DABFT is currently the most adaptive distributed consensus solution that meets various technical needs among public chains. At the application level, Prof. Deng proposed an innovative incentive mechanism derived from a model of cooperative economics (macroeconomics, microeconomics, and international trade), the Delegated Proof of Economic Value (DPoEV) consensus. The DPoEV uses the knowledge map algorithm (a branch of Artificial Intelligence) to accurately assess the economic value of digital assets (knowledge). The AIBC is task-driven with a “blocks track task” dynamic sharding structure. It is designed as a two-dimensional (2D) BlockCloud (as opposed to a 1D blockchain) with super nodes that track the status of tasks through side chains. Once a task is initiated, a set of task validators are then selected according to the “rule of relevancy.” A task handler is then chosen among the task validators to handle the task. The task handler and validators manage the task from the beginning to the end with no dynasty change. Thus, effectively, from the task’s perspective, the task validators form a shard that is responsible for managing it, with the task handler being its leader. In essence, a task entails a specific activity initiated by a tasking node in the system, and a blockchain in the AIBC is a side chain consisting of blocks that track the progress of the task (and other tasks) in a dynamically allocated shard in which its task handler is the leader. The 2D dynamic sharding implementation resolves the scalability issue, and at the same time makes it extremely efficient to evaluate the incremental economic value of additional knowledge contributed by each task. On the ecosystem layer, the AIBC is a “dual-token” platform that marks each decentralized application as a unique entity, yet provides a unified cross-platform value measure. AIBC offers a complete end-to-end distributed industry solution (DSOL). Based on this dual-token platform, AIBC creatively issues (CFTX) tokens for DSOL assets and develop a corresponding cross-chain, permission-based protocol for exchange.  8  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  The AIBC combines Artificial Intelligence, big data, cloud computing, and distributed database to provide platforms and algorithms for applications in finance, digital assets, supply chain, education and training, and Internet media, etc. 1.3.2 AIBC Business Scenarios The BlockCloud assets digitization Center provides the first business scenario for AIBC is the Assets Securitization and Tokenization. AIBC provides a decentralized channel for digital assets transactions, effectively transforming these digital assets transactions into social network communications, greatly increasing the flow of digital assets and increasing their value. The Cofintelligence AI Research Center provides a second business scenario for AIBC: a smart investment and asset management platform. The platform uses AI algorithms (mainly neural networks and deep learning) to analyze the secondary market transactions of different frequencies in order to predict the future trend of financial assets, for the purpose of generating a variety of investment portfolios with different styles. The platform seeks to help institutional and individual investors develop suitable investment strategies. In addition, the AIBC has the potential to support the following business scenarios: 1. Traceability Application: Through the irreversible, tamperproof and traceable nature of AIBC blockchain, users can effectively trace the authenticity and uniqueness of physical products. 2. Supply Chain Finance: Through the AIBC distributed consensus algorithm, users can effectively match the investment and financing needs in supply chain finance, and manage the credit lines of upstream and downstream enterprises. 3. BlockCloud Service: The architecture of the AIBC itself makes it a good decentralized cloud service platform, which provides distributed ledger recording service and application development environment to enterprise developers.  9  Cofintelligence BlockCloud Technology Ltd.  2  Artificial Intelligence BlockCloud Whitepaper  AIBC Economic Model Overview The AIBC ecosystem is essentially a closed economy, of which the operations run  on a set of carefully designed economic models. These economic models, at the minimum, must include a macroeconomic model that governs monetary policy (token supply), a trade economic model that enforces fair trade policy, and a microeconomic model that manages supply and demand policy.  2.1  Economic Model Overview  2.1.1 Macroeconomic Model, Monetary Policy and Digital Money Standard The most important economic model is the macroeconomic model that provides tools to govern the monetary policy, which principally deals with money (token) supply. Before the birth of modern central banks, money was essentially in the form of precious metals, particularly gold and silver. Thus, money supply was basically sustained by physical mining of precious metals. Paper money in the modern sense did not come to existence till after the creation of the world’s first central bank, Bank of England in 1694. With the creation of the central banks, the modern monetary policy was born. Initially, the goal of monetary policy was to defend the so-called gold standard, which was maintained by their promise to buy or sell gold at a fixed price in terms of the paper money (Abdel-Monem, 2009). The mechanism for the central banks to maintain the gold standard is through setting/resetting the interest rates that they adjust periodically and on special occasions. However, the gold standard has been blamed for inducing deflationary risks because it limits money supply (Keynes, 1920). The argument gains merit during the great depression of the 1920’s and 1930’s, as the gold standard might have prolonged the economic depression because it prevented the central banks from expanding the money supply to stimulate the economy (Eichengreen, 1995; AEA, 2001). The “physical” reason behind the gold standard's deflationary pressure on the economy is the scarcity of gold, which limits the ability of monetary policy to supply needed capital during economic downturns (Mayer, 2010). In addition, the unequal geographic distribution of gold 10  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  deposits makes the gold standard disadvantageous for countries with limited natural resources, which compounds the money supply problem when their economies are in contrarian mode (Goldman, 1981). An obvious way to combat the gold standard’s natural tendency of devaluation risk is to issue paper money that is not backed by the gold standard, the so-called fiat money. A fiat money has no intrinsic value and is used as legal tender only because its issuing authority (a central government or a central bank) backs its value with nonprecious metal financial assets, or because parties engaging in exchange agree on its value (Goldberg, 2005). While the fiat money seems to be a good solution for the devaluation problem, central governments have always had a variety of reasons to oversupply money, which causes inflation (Barro and Grilli, 1994). Even worse, as the fiat money has no intrinsic value, it can become practically worthless if the issuing authorities either are not able or refuse to guarantee its value, which induces hyperinflation. Case in point is the Deutsche Mark hyperinflation in the Weimar Republic in 1923 (Federal Reserve, 1943). Therefore, neither the gold standard nor the fiat currency can effectively create a “perfect” monetary policy that closely matches the money supply with the state of the economy. After the breakdown of the Bretton Woods framework, all economies, developed and developing alike, still, struggle with choices of monetary policy instruments to combat money supply issues de jour. In addition, because of the physical world’s “stickiness (of everything),” all money supply instruments (e.g., central bank interest rates, reserve policies, etc.) lag behind the economic reality, making real-time policy adjustment impossible. Therefore, eradication of deflation and inflation will always be impractical, unless a commodity money with the following properties can be found or created: 1. That it has gold-like intrinsic value but not its physical scarcity. 2. That it can be mined at the exact pace as the economic growth. 3. And that it can be put into and taken out of circulation instantaneously and in sync with economic reality.  11  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  Such a commodity does not exist in the physical world. However, things might be different in the digital world, if digital assets can be monetized into digital currencies. There have been discussions about a “Bitcoin standard.” For example, Warren Weber (2015) with the Bank of Canada explores the possibility and scenarios that central banks get back to a commodity money standard, only that this time the commodity is not gold, but Bitcoin. However, just like gold, Bitcoin faces a scarcity challenge in that its quantity is finite, and just like gold it needs to be mined at a pace that may lag far behind economic growth (Nakamoto, 2008). As such, other than that Bitcoin resides in the digital worlds, it does not offer obvious and significant benefits over gold as the anchor for a non-deflationary commodity money standard. However, such a digital currency can be created, which instantaneously satisfies the requirement that it can be put into and taken out of circulation instantaneously and in sync with economic reality. The requirements that the digital currency must have gold-like intrinsic value but not its physical scarcity and that it must be mined at the exact pace as the economic growth are not trivial. First of all, there must be an agreement that digital assets are indeed assets with intrinsic value as if they were physical assets. While such an agreement is more of a political and philosophical nature, and therefore beyond the scope of our practicality-oriented interest, it is not a far stretch to regard knowledge as something with intrinsic value, and since all knowledge can be digitized, it thus can form the base of a digital currency with intrinsic value. This is what we call the ""knowledge is value"" principle. Based on our “knowledge is value” principle, there is some merit to Warren Buffett’s argument that Bitcoin has no intrinsic value, “because [Bitcoin] does not produce anything (Buffett 2018).” Warren Buffett’s remarks refer to the facts that during the Bitcoin mining process, nothing of value (e.g., knowledge) is actually produced, and that holding Bitcoin itself does not produce returns the way traditional investment vehicles backed by physical assets do (i.e., through value-added production processes that yield dividends and capital appreciation). 12  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  Therefore, again, based on our ""knowledge is value"" principle, a digital currency that forms the base for a commodity money standard must have intrinsic value in and unto itself; thus not only it is knowledge, it also produces knowledge. This is the fundamental thesis upon which a digital ecosystem that uses a quantitative unit of knowledge as value measurement, thus currency, can be built. In a digital ecosystem, there is both knowledge in existence and knowledge in production. If the value of knowledge in existence can be directly measured by a quantitative and constant unit, then the unit itself can be regarded as a currency. Furthermore, the value of knowledge in production can also be measured by the constant unit (currency) in an incremental manner, thus expansion of knowledge is in sync with the expansion of currency base.  Effectively, the value measurement system is an  autonomous monetary policy that automatically synchronizes economic output (knowledge mining) and money supply (currency mining), because the currency is not a stand-alone money, but merely a measurement unit of the value of knowledge. Thus, this digital currency simultaneously satisfies the requirements that it must have gold-like intrinsic value but not its physical scarcity and that it must be mined at the exact pace as the economic growth, as the currency (measurement unit) and the economic growth (knowledge) are now one and the same; they are unified. In the next section, we discuss how to develop the measurement unit. 2.1.2 Trade Microeconomic Models and Policy Adjustment The trade economic model provides tools to enforce fair trade policy among participants in a “globalized” economic environment. In a conventional open and free trade regime with no restrictions, it is quite likely that a few “countries” over-produce (export) and under-consume (import), thus they accumulate vast surpluses with regard to their trading partners. These countries will eventually appropriate all the wealth in the global economy, reducing their trade partners to an extreme level of poverty. Therefore, there must be a fair trade policy, enforced by a collection of bilateral and multilateral trade agreements, which penalizes the parties with unreasonable levels of surplus, and 13  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  provides incentives to the parties with unreasonable levels of deficit. The penalization can be in the form of tariff levy, and other means to encourage consumption and curb production. The incentives can be tariff credit to encourage production and curb consumption. They are essentially wealth rebalancing devices that a ""world trade organization (WTO)"" like body would deploy to guarantee that trades should be both free and fair (WTO, 2015). The microeconomic model provides tools to help manage supply and demand policy in order to set market-driven transaction prices between participants. When there are multiple products simultaneously competing for consumers, the price of a product is set at the point of supply-demand equilibrium.  The supply and demand policy  discourages initially high-value products to dominate production capability and encourages initially low-value products to be produced. Therefore, consumers can find any product that serves their particular need at reasonable price points.  2.2  AIBC Economic Model Implementation Overview Because of the physical world’s “stickiness (of everything),” all monetary policy  instruments (e.g., central bank interest rates, reserve policies, etc.), fair trade devices and supply-demand balancing tools lag behind the economic reality. This means these economic models can never dynamically track economic activities and adjust economic policies accordingly on a real-time basis. To make things more complicated, because all economic policies are controlled by centralized authorities (central banks, WTO, etc.), they may not necessarily reflect the best interests of majority participants in economic activities. The Internet, however, provides a leveling platform that makes real-time economic policy adjustment practical. This is because the digital world can utilize advanced technological tools in order not to suffer from the reality stickiness and policy effect lag that are unavoidable in the physical world, as well as the potential conflict of interest that cannot be systematically eliminated with centralized authorities. The most important tool of them all, in this sense, is the blockchain technology, which provides a 14  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  perfect platform for a decentralized digital economy capable of real-time economic policy adjustment. On the upper-layer, the AIBC ecosystem is an implementation of the “knowledge is value” macroeconomic model through an innovative Delegated Proof of Economic Value (DPoEV) incentive consensus algorithm. The DPoEV consensus establishes a digital economy, in which a quantitative unit that measures the value of knowledge, the CFTX token, is used as the media of value storage and transactions. Since the token issuance and the knowledge expansion are unified and therefore always in-sync on a real-time basis, no deflation and inflation exist in the ecosystem by design. Along with the trade and microeconomic models, the AIBC provides a framework of decentralized, consensusbased digital economy with real-time policy adjustment that enables resource sharing. On the bottom layer, the AIBC implements a Delegated Adaptive Byzantine Fault Tolerance (DABFT) distributed consensus algorithm that enforces the upper-layer DPoEV policies. It combines some of the best features of the existing consensus algorithms and is adaptive, capable of selecting the most suitable consensus for any application scenario. The DABFT is the blockchain foundation upon which the AIBC ecosystem is built.  15  Cofintelligence BlockCloud Technology Ltd.  3  AIBC Architecture  3.1  AIBC Architecture Overview  Artificial Intelligence BlockCloud Whitepaper  The AIBC is an Artificial Intelligence and blockchain technology based decentralized ecosystem that allows resource sharing among participating nodes. The primary resources shared are the computing power and storage space. The goals of the AIBC ecosystem are efficiency, fairness, and legitimacy. The AIBC consists of four layers: a fundamental layer conducts the essential blockchain functions, a resource layer that provides the shared services, an application layer that initiates a request for resources, and an ecosystem layer that comprises physical/virtual identities that own or operate nodes. The AIBC implements a two-consensus scheme to enforce upper-layer economic policies and achieve fundamental layer performance and robustness: The Delegated Proof of Economic Value (DPoEV) incentive consensus to create and distribute award on the application and resource layers, and the Delegated Adaptive Byzantine Fault Tolerance (DABFT) distributed consensus for block proposition, validation and ledger recording on the fundamental layer. The traditional one-dimensional (1D) single-chain ecosystems are not efficient in information retrieval and economic value assessment on the node level. The AIBC is taskdriven, and it adopts a concept of ""blocks track task."" It is designed as a two-dimensional (2D) BlockCloud with side chains that track tasks managed by its super nodes. As a result, it is extremely efficient to evaluate the incremental economic value of additional knowledge contributed by each task.  3.2  AIBC Layers From a technology perspective, the AIBC ecosystem comprises four layers: 1. The fundamental layer (or blockchain layer) that conducts the essential blockchain functions, namely distributed consensus-based block proposition, validation, and ledger recording. The nodes delegated to perform these fundamental blockchain  16  Cofintelligence BlockCloud Technology Ltd.  Artificial Intelligence BlockCloud Whitepaper  services are the super nodes. The functionality of the super nodes is explained in the next section. 2. The resource layer that provides the essential ecosystem services, namely, computing power and storage space. The AIBC ecosystem is based on the concept that resources are to be shared, and these resources are provided by the computing nodes and storage nodes. While their functions are different, the computing nodes and storage nodes can physically or virtually be collocated or coincide. The functionalities of the computing and storage nodes are explained in the next section. 3. The application layer that requests resources.  Each application scenario is  initiated by a tasking node. In the AIBC ecosystem, tasking nodes are the ones that have needs for computing power and storage space, thus it is their responsibility to initiate tasks, which in turn drive the generation of economic value. The functionality of the tasking nodes is explained in the next section. 4. The ecosystem layer that comprises physical/virtual entities that own or operate the nodes. For example, a tasking node can be a financial trading firm that needs resources from a number of computing nodes, which can be other trading firms or server farms that provides computing power. Figure 3.1 illustrates the AIBC layer structure.  Figure 3.1 – AIBC Layer Structure 17  Cofintelligence BlockCloud Technology Ltd.  3.3  Artificial Intelligence BlockCloud Whitepaper  AIBC Two-Consensus Implementation The AIBC ecosystem consists of four layers, from top to bottom, they are the  ecosystem, application, resource and fundamental (blockchain) layers. These layers have distinguished responsibilities and thus performance/robustness requirements.  For  example, once a task is initiated, the application and resource layers are primarily concerned with delivering resources and distributing reward. Therefore, these layers need to follow the economic policies strictly and run on a deterministic and robust protocol, but not necessarily a high-performance one (in terms of speed). On the other hand, the fundamental layer is the workhorse providing basic blockchain services such as consensus building, block proposition and validation, transaction tracking, and ledger recording. Therefore it needs to follow an adaptive protocol with high throughput without sacrificing robustness. As such, the AIBC implements a two-consensus approach: the DPoEV "
"Towards a Metric for Automated Conversational Dialogue System Evaluation and Improvement Jan Deriu Zurich University of Applied Sciences deri@zhaw.ch  arXiv:1909.12066v1 [cs.AI] 26 Sep 2019  1  Introduction  Conversational dialogue systems (also referred to as chatbots, social bots, or non-task-oriented dialogue systems) allow for a natural conversation between computer and humans. Research on these dialogue systems has recently reemerged due to the availability of large dialogue corpora, (Serban et al., 2018) as well as the popularization of deep learning (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016b). One major challenge in developing high-quality dialogue systems is the evaluation process. Ideally, an evaluation method should be automated, have a high correlation to human judgements and be able to discriminate between different dialogue strategies. Most common techniques to evaluate conversational dialogue systems rely on crowdsourcing, where human judges are asked to rate the appropriateness (or quality) of a generated response given a context. Although this procedure  Mark Cieliebak Zurich University of Applied Sciences ciel@zhaw.ch  allows to discriminate between different strategies, it has several drawbacks: it is time and cost intensive, it has to be redone for every change in dialogue strategy, and the results cannot be used to improve the system. On the other hand, the automated evaluation is usually performed by applying word-overlap metrics borrowed from the machine translation or text summarization community, which have been shown to correlate poorly to human judgements on the utterance level (Liu et al., 2016). Trained Metrics. Recently, the notion of trained metrics was introduced for conversational dialogue systems (Lowe et al., 2017). The main idea is that humans rate the generated response of a dialogue system in relation to a given context (i.e. the dialogue history). Based on these ratings, a regression model is trained which models the human judges. For this, the context, the candidate response, and the gold-standard response are used as input and the judgement is predicted. This approach correlates well with human judgements on the turn level as well as on the system level. However, these metrics rely on a gold-standard and work on static contexts, which is problematic for two reasons. First, as the context is written by humans it does not reflect the behaviour of the dialogue system. Second, it cannot be used in deployed systems where no gold-standard is available. Dynamic context evaluation (Gandhe and Traum, 2016), on the other hand, usually requires human-computer interaction, which is costly, and puts an additional cognitive strain on the users if they are to rate live during the conversation (Schmitt and Ultes, 2015). Contribution. In this work we propose to automatically generate the dialogues relying on selftalk, which is derived from AlphaGo self-play (Silver et al., 2016). Dialogues are generated by two  instances of the same system conversing with each other. Then the automatically generated dialogues are rated by human judges. That is, the judges read the dialogues and rate it on the turn level. Based on these ratings, we train a regression model which learns to predict the ratings of the human judges. Our results show that this method, which we refer to as AutoJudge, achieves high correlation to human judgements. Thus, it can be applied to fully automatically assess the quality of a dialogue system without being dependent on gold standard responses. Applications. Since our approach is fully automatic and requires no humans in the loop, we want to go one step further and apply it to improve the dialogue system at hand. More precisely we attempt to apply the metric in two different ways: (i) response ranking similar to (Shalyminov et al., 2018; Hancock et al., 2019), and (ii) reward for reinforcement learning. It turns out that only the re-ranking shows promising results, whereas the metric is not useful as a reward function. This is very surprising, since the trained metric correlates well to human judgements, and it can discriminate between good and bad utterances. Why this happens, and how it can be resolved, is an open research question, which we discuss towards the end of this paper.  2  Experimental Setup  Our experimental pipeline follows three phases. First, the data generation phase, where we let the dialogue systems generate dialogues automatically. Second, the data annotation phase, where we rely on crowdsouring to rate the dialogues on the turn level. Third, the improvement phase, where we train an automated judgement model on the annotated data and apply this model to improve the dialogue system. 2.1  Dialogue Systems  For our experiments we relied on the following state-of-the-art dialogue systems (the training details are in Appendix A): Seq2Seq. The Sequence-to-Sequence model as proposed by (Vinyals and Le, 2015) consists of an encoder and a decoder. Both modules are based on Long Short-Term Memory cells (LSTM) (Hochreiter and Schmidhuber, 1997), where the encoder consumes the last utterance and produces a hidden  representation, which is passed as initial state to the decoder to condition the generation process. HRED. The Hierarchical Recurrent EncoderDecoder (HRED) model proposed by (Serban et al., 2016a) enhances the Seq2Seq model by a hierarchical encoding procedure. Here, the contextturns are encoded by first encoding each turn separately and then by applying a recurrent encoder over the hidden states of the turns. The decoding procedure is conditioned on the hidden state produced by the context encoder. VHRED. The Hierarchical Latent Variable Encoder-Dcoder model (VHRED) (Serban et al., 2017a) enhances the aforementioned HRED model by introducing a stochastic latent variable at the utterance level. This stochastic variable aims to inject variability at the utterance level, which in turn increases the variety of responses a model generates. MrRNN. The Multi-resolution Recurrent Neural Ntwork (MrRNN) (Serban et al., 2017b) enhances the HRED model by introducing an abstraction layer. More precisely, the dialogue is modelled by processing the inputs and outputs at various level of abstractions (e.g. at the level of meaning bearing words and the usual word-level). DE. The Dual Encoder (DE) (Lowe et al., 2015) is a selection based model, which differs from the generation based approaches of the aforementioned models. The DE encodes both the context and a candidate response (using the same encoder as the VHRED model) and then classifies if the candidate is a valid response to the given context. 2.2  Turn-Level Annotation  We apply self-talk to automatically generate dialogues. For this, we sample 100 different contexts randomly from a set of unseen contexts and let the dialogue system generate a dialogue starting from this context, which consist of 10 turns each. For the annotation process, we use Amazon Mechanical Turk (AMT) 1 and follow the procedure outlined by (Lowe et al., 2017), i.e. the judges rated the overall quality of each turn on a scale from 1 (low quality) to 5 (high quality). Each turn is annotated by three different judges. We required the AMT workers to be from an english speaking country (USA, UK, Ireland or Australia) 1  https://www.mturk.com/  in order to ensure that they are native speakers, since the generated messages are highly colloquial and make heavy usage of slang. For each annotation, we paid 15 cents, where we assumed that each annotation takes between 60 to 90 seconds. For the selection of the final turn-label, we apply the MACE procedure (Hovy et al., 2013), which learns confidence scores for the annotators. Our final dataset consists of a total of 500 annotated dialogues, which amounts to 5000 annotated pairs of contexts and responses. 2.3  AutoJudge  Similarly to the ADEM procedure proposed by (Lowe et al., 2017), we train a regression model on the annotated data. For this, we use the pre-trained context and response encoder from the VHRED model. Unlike ADEM, our dialogues are generated automatically, thus, we do not have access to a gold-standard response. For this reason, we use the following scoring function: score(c, r) = (cT M r−α)/β where M ∈ Rd×d is a learned similarity matrix, α, β are scalar constants, and c, r are the context and response embeddings respectively. The model is optimized to minimize the mean squared error between the predicted ratings and the human judgements. 2.4  Improving Dialogue Systems  Since AutoJudge is fully automated, we apply it to improve the existing dialogue systems. For this, we implemented the following two applications: as reward for reinforcement learning (RL), and as re-ranking candidate utterances. Re-Ranking. Given a list of responses from the five aforementioned dialogue systems for a given context, AutoJudge re-ranks them by their predicted score. In our experiments, we use the dialogue systems, which we trained for the self-talk experiment, i.e. we re-rank the outputs of the five aforementioned dialogue systems. Thus, the reranker serves as a meta-selection module. Reinforcement Learning Reward. We apply the predicted ratings as reward in the RL framework. For this, we apply the Policy Gradient formulation, as done in (Li et al.,P 2016), which is defined as follows: 5JRL (θ) = i 5 log p(ri |ci ) × P i R(ri , ci ) , where ri and ci are the response and context in the ith turn, R(rP i , ci ) is the predicted reward by AutoJudge, and i log p(ri |ci ) is the reconstruction error.  3  Results and Discussion  In our experiments we use the Twitter Dialogue Corpus (Ritter et al., 2011)2 . The Twitter Dialogue Corpus provides social interactions, which we believe to be a good basis for being annotated via crowdsouring. Data Aggregation. The turn-level ratings provide us with 5000 annotated pairs of context and responses. The distribution over the labels is balanced (i.e. each class is represented between 19% and 21% of the cases). However, the agreement scores among the human judges is rather low: the median pairwise Spearman correlation between two judges is only at 0.403. Furthermore, the MACE procedure reports on the confidence score (between 0 and 1) of single judges, which is used as basis for selecting the final label. The average confidence is at only 0.15. We assume that these problems stem from the high degree of subjectivity of the problem. C ONVO S PLIT S YSTEM S PLIT  Pearson Corr 0.573 0.544  Spearman’s Rho 0.577 0.53  MAE 0.928 0.984  Table 1: Average correlations between the judgements predicted by AutoJudge and the human judgement. C ONVO S PLIT denotes the cross-validation split according to the contexts and S YSTEM S PLIT denotes the cross-validation split according to the dialogue system.  AutoJudge. We train AutoJuge using k-fold cross validation. There are two ways of splitting the data into folds, in order to ensure that all turns of the same dialogue are in the same fold. First, we group the 100 contexts into 10 folds, thus, each fold consists of 50 dialogues (i.e. 10 contexts times the number of dialogue systems), this is denoted as C ONVO S PLIT. The second option is to split the data according to the system which created the conversation, which evaluates the performance of AutoJuge in rating dialogues of unseen dialogue systems. We denote this as S YSTEM S PLIT. In Table 1, we report the average Pearson correlation, Spearman’s rho and mean absolute error (MAE) over all folds for the conversation split and the system split. With moderate correlations of 0.573 on the dialogue level, we get results which are comparable to (Lowe et al., 2017), 2  We use the IDs provided et al., 2017a), which can be www.iulianserban.com/Files/TweetIDs.zip  by (Serban found here:  where ADEM achieves a Pearson correlation of 0.436. Note that we cannot directly compare our results to BLEU score and ADEM, since these base their predictions on gold standards, which we do not have in our setting. An interesting result is the System Split, i.e. that our approach is able to maintain a high correlation (0.544) with the ratings of a dialogue system when removing the data of that system from the training, which is not the case in (Lowe et al., 2017) where the correlation for a different system dropped significantly.  single universal response) tend to receive high scores, since the training data for AutoJudge does not include these kinds of responses. However, it is not clear how to stabilize AutoJudge to handle these cases. For instance, by artificially enhancing the training data for AutoJudge with negative examples, the Pearson correlation score drops to 0.50 without any impact on the reinforcement learning.  Answer Selection. In order to evaluate the improvements achieved by the re-ranking method, we sample a disjoint set of 100 new contexts and apply self-talk to generate conversations. Then, we use AMT to let humans judge the automatically generated conversations on the dialogue level (i.e. a rating for the entire dialogue as opposed to turn-based ratings). We compare the performance of the five base dialogue systems to the performance of the re-ranking strategy. Table 2 shows the average scores for each dialogue system. Our results show that the re-ranking approach works very well. It raises the score to 3.47, which is 0.16 points higher than the best base-system (i.e. S EQ 2S EQ).  Our results show that AutoJudge correlates well to human judgements and it is useful to measure the progress of a dialogue system, as it is able to discriminate among different strategies. Furthermore, it generalizes well to unseen strategies for the same domain. Since AutoJudge is independent of a gold-standard it can be applied to deployed systems where gold-standards are not available. Finally, it shows promising results when applied as answer selection module. As a next step, we intend to apply AutoJudge onto human-computer dialogues to measure the viability of AutoJudge in a real-world setting. In this work we tried to use AutoJudge as a reward for reinforcement learning, which resulted in suboptimal dialogues. The main reason seems to be that AutoJudge cannot properly handle the bad utterance that are generated during the initial phase of reinforcement learning. This is surprising, since AutoJudge is able to distinguish good and bad utterances of fully-trained systems. This seems to indicate that there are different types of ”bad” utterances, and we need to adapt the training mechanism of AutoJudge if we want to apply it not only to evaluation, but also to improving dialogue systems. Our results indicate that trained metrics suffer from instabilities, which might be caused by the size of the dataset. One major issue is that it is not clear which aspects AutoJudge captures. Although the correlation between the human judgements and the outputs of AutoJudge are high, we cannot make any statement about what aspects of the context or the response are relevant for the predicted rating. This is a fundamental problem with the evaluation of conversational dialogue systems, as there is no clear definition for ”adequate” responses. Thus, an important future work problem is the investigation into the definition of ”adequacy” for conversational dialogue systems.  Systems S EQ 2S EQ HRED VHRED M R RNN D UAL E NCODER R E -R ANKING  Dialogue Level Rating 3.31 2.78 3.20 2.37 2.02 3.47  Table 2: Human judgements on the dialogue level for each dialogue system. For this, a each dialogue system (the five base-systems and the re-ranking system) generate 100 dialogues using self-talk, which human judges rated on the dialogue level. Here we see the average ratings for each system.  Reinforcement Learning. When we apply AutoJudge as reward resulted in suboptimal dialogues. Although the return increases over time (from 21.74 to 37.41 over 80 episodes), the dialogues which the policy generates are often incoherent or completely useless. This seems counterintuitive when taking into account the aforementioned high correlation scores. We believe that the main reason for the suboptimal behaviour is that AutoJudge does not have enough coverage during training. Thus, very bad responses (e.g. empty responses, repeating responses, convergence to a  4  Conclusion  We conjecture that this might apply also to other automated metrics, thus, this is an important research question that needs to be addressed if we want to understand how to better train and optimize dialogue systems.  5  Acknowledgements  This paper has been partially funded by the LIHLITH project, supported by ERA-NET CHISTERA and Swiss National Science Foundation (20CH21 174237).  References Sudeep Gandhe and David Traum. 2016. A Semiautomated Evaluation Metric for Dialogue Model Coherence, pages 217–225. Springer International Publishing, Cham. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. 2019. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415. Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, pages 1735–1780. Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with mace. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1120–1130. Association for Computational Linguistics. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. 2016. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192– 1202. Association for Computational Linguistics. Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122–2132. Association for Computational Linguistics. Ryan Lowe, Michael Noseworthy, Iulian Vlad Serban, Nicolas Angelard-Gontier, Yoshua Bengio, and Joelle Pineau. 2017. Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses. In Proceedings of the 55th Annual Meeting of the  Association for Computational Linguistics (Volume 1: Long Papers), pages 1116–1126. Association for Computational Linguistics. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 285–294, Prague, Czech Republic. Association for Computational Linguistics. Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch, and Armand Joulin. 2018. Advances in pre-training distributed word representations. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018). Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven Response Generation in Social Media. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 583–593, Stroudsburg, PA, USA. Association for Computational Linguistics. Alexander Schmitt and Stefan Ultes. 2015. Interaction Quality: Assessing the quality of ongoing spoken dialog interaction by expertsAnd how it relates to user satisfaction. Speech Communication, 74:12 – 36. Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. 2016a. Building End-to-end Dialogue Systems Using Generative Hierarchical Neural Network Models. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16, pages 3776–3783. AAAI Press. Iulian V. Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017a. A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues. In Thirty-First AAAI Conference on Artificial Intelligence, page 1583. Iulian Vlad Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen Zhou, Yoshua Bengio, and Aaron C. Courville. 2017b. Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA., pages 3288–3294. Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, and Joelle Pineau. 2016b. Generative deep neural networks for dialogue: A short review. arXiv preprint arXiv:1611.06216. Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin, and Joelle Pineau. 2018. A Survey of Available Corpora for Building Data-Driven Dialogue Systems: The Journal Version. Dialogue & Discourse, 1(9).  Igor Shalyminov, Ondřej Dušek, and Oliver Lemon. 2018. Neural response ranking for social conversation: A data-efficient approach. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 1–8, Brussels, Belgium. Association for Computational Linguistics. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484. Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A Neural Network Approach to Context-Sensitive Generation of Conversational Responses. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 196–205. Association for Computational Linguistics. Oriol Vinyals and Quoc Le. 2015. A neural conversational model. arXiv preprint arXiv:1506.05869.  # 1  2  3  #Turn  Utterance  Pred.  Human  1  @user ur prolly tired now , arent u ? wanna sleep , don’t cha ? (:  4.67  5.00  2  @user atm jus chillin nd chattin nd listening to music :P haha xxx  3.70  2.00  3  @user x factor tickets came so im siva happy dancing :) might do some fanfic xx wbu ? xx  3.26  1.00  4  @user goood :) do u want to be in my 1d fanbook ? xx  3.07  2.00  5  @user : o gosh ppl these days :/ ... ok tell me everything 2moz p . s send me the link so i can send it to et :) xx dont stress it :)  2.89  1.00  6  @user yer im fine thanks xxx  3.18  3.00  7  @user okay , thanks xxx  3.42  1.00  8  @user :) you okay ? xxx  3.26  1.00  9  @user yeah im good thanks :) you ? xx  3.11  4.00  0  if i could i would hurt you ... a lot ! ! i’m so sick of you blaming me for everything that goes wrong ! !  4.86  5.00  1  @user awh ... is everything ok sweetie ?  4.72  5.00  2  @user thanks ... it is ... i just can’t wait to see how it is ...  3.72  3.00  3  @user wait , what ?  3.51  5.00  4  @user i have a new phone . i have to get it .  2.54  3.00  5  @user how much do you pay ?  2.20  3.00  6  @user pay attention  2.65  1.00  7  @user i would have to do that  1.82  1.00  8  @user you could of been in my head .  2.27  1.00  9  @user yeah , i don’t know .  3.03  1.00  0  # hashtag @user @user @user @user @user @user @user @user  4.47  5.00  1  @user thanks for the follow . do you have at bat on your phone yet ?  4.95  5.00  2  @user there’s a lot going on ... but i think is good to say what u think here and at the boards too . specially there !  3.55  2.00  3  @user pretty much just as well as the current macbook pros do ...  2.42  1.00  4  @user how well does it run photoshop / illustrator ?  2.67  2.00  5  @user this is like sixth sense . can i have your games consoles ?  3.00  1.00  2.15  1.00  2.09  1.00  7  @user probably , but i don’t know if it’d be appropriate in this case . i try only to use our funds for things i know i need to know . @user well we can scan thru a few joints i’m workin on , or take the traditional route n pen one down  8  @user i learned gordon’s rhythm patterning in methods classes , not much orff  2.21  1.00  9  @user its like a long story with sequence of events lmao  1.76  1.00  6  Table 3: Randomly sampled output. The conversation is sampled at random and AutoJudge rates each turn.  A  Training Details  40 35  AutoJudge Training We trained AutoJudge using the pre-trained VHRED model to encode the context and the response. During the training only the matrix M gets optimized. We also experimented with non-linear transformation on these encodings, which did not yield any improvements. Similar to (Lowe et al., 2017), we use α = 0.01 and β = 32. AutoJudge is optimized using Adam optimizer (Kingma and Ba, 2014), with a lr = 0.001 and a batch size of 512.  B  Reinforcement Learning  For reinforcement learning, we use the pre-trained HRED system as our initial policy. We apply Policy Gradient as described above. We experimented with various episode batch sizes (1, 10, 100, 1000), i.e. in sample n episodes at once to reduce variance. However, it had no impact on the performance. We also experimented with different formulations, i.e. using Advantage Actor Critics in order to reduce the variance. In Table 1, we show the rolling average return over the course of 100 episodes. We used a batch size of 100 and we used the standard Policy Gradient formulation. The reward oscillates, which is due to finding new local maxima. He maxi-  30 25  20  97  93  101  89  85  81  77  73  69  65  61  57  53  49  45  41  37  33  29  25  21  17  9  5  13  15  1  Model Training. For all models, we used a bidirectional LSTM to encode the turns, and a unidirectional LSTM for both the context encoder and decoder. We specify the number of units for the LSTMs to 500, 1000, 1000 for the turn-encoder, context-encoder and decoder respectively. We use the pretrained 300 dimensional FastText embeddings (Mikolov et al., 2018), which we refine during the training. In order to avoid too large vocabularies, we limit the vocabulary size to 20k distinct tokens. The generative models are trained to minimize the reconstruction error. For the VHRED and MrRNN, we refer to the original papers for the loss function formulation. The Dual Encoder is trained to minimize P a contrastive loss function logσ(cT rT ure ) + n∈N logσ(−cT rn ), where c is the context encoding, rT rue is the correct response encoding and N is a set of negative samples. For each training sample we sampled 10 negative examples uniformly at random from the training set. All models are optimized using the Adam optimizer (Kingma and Ba, 2014), with a lr = 0.001 and a batch size of 80.  Figure 1: Data distributions for both the overall data and for the systems.  mal observed reward is at 37 after 80 episodes. However, the generated dialogues are all empty, i.e. the dialogue system always returns the ”endof-sequence” token right away.  "
"arXiv:1909.12072v1 [cs.AI] 26 Sep 2019  Towards Explainable Artificial Intelligence Wojciech Samek1 and Klaus-Robert Müller2,3,4 1  Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany wojciech.samek@hhi.fraunhofer.de 2 Technische Universität Berlin, 10587 Berlin, Germany 3 Korea University, Anam-dong, Seongbuk-gu, Seoul 02841, Korea 4 Max Planck Institute for Informatics, Saarbrücken 66123, Germany klaus-robert.mueller@tu-berlin.de   Introduction  Today’s artificial intelligence (AI) systems based on machine learning excel in many fields. They not only outperform humans in complex visual tasks [16,53] or strategic games [56,83,61], but also became an indispensable part of our every day lives, e.g., as intelligent cell phone cameras which can recognize and track faces [71], as online services which can analyze and translate written texts [11] or as consumer devices which can understand speech and generate human-like answers [90]. Moreover, machine learning and artificial intelligence have become The final authenticated publication is available online at https://doi.org/10.1007/978-3-030-28954-6_1. In: W. Samek et al. (Eds.) Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol. 11700, pp. 5-22. Springer, Cham (2019).  2  W. Samek and K.-R. Müller  indispensable tools in the sciences for tasks such as prediction, simulation or exploration [78,15,89,92]. These immense successes of AI systems mainly became possible through improvements in deep learning methodology [48,47], the availability of large databases [17,34] and computational gains obtained with powerful GPU cards [52]. Despite the revolutionary character of this technology, challenges still exist which slow down or even hinder the prevailance of AI in some applications. Examplar challenges are (1) the large complexity and high energy demands of current deep learning models [29], which hinder their deployment in resource restricted environments and devices, (2) the lack of robustness to adversarial attacks [55], which pose a severe security risk in application such as autonomous driving5 , and (3) the lack of transparency and explainability [76,32,18], which reduces the trust in and the verifiability of the decisions made by an AI system. This paper focuses on the last challenge. It presents recent developments in the field of explainable artificial intelligence and aims to foster awareness for the advantages–and at times–also for the necessity of transparent decision making in practice. The historic second Go match between Lee Sedol and AlphaGo [82] nicely demonstrates the power of today’s AI technology, and hints at its enormous potential for generating new knowledge from data when being accessible for human interpretation. In this match AlphaGo played a move, which was classified as “not a human move” by a renowned Go expert, but which was the deciding move for AlphaGo to win the game. AlphaGo did not explain the move, but the later play unveiled the intention behind its decision. With explainable AI it may be possible to also identify such novel patterns and strategies in domains like health, drug development or material sciences, moreover, the explanations will ideally let us comprehend the reasoning of the system and understand why the system has decided e.g. to classify a patient in a specific manner or associate certain properties with a new drug or material. This opens up innumerable possibilities for future research and may lead to new scientific insights. The remainder of the paper is organized as follows. Section 2 discusses the need for transparency and trust in AI. Section 3 comments on the different types of explanations and their respective information content and use in practice. Recent techniques of explainable AI are briefly summarized in Section 4, including methods which rely on simple surrogate functions, frame explanation as an optimization problem, access the model’s gradient or make use of the model’s internal structure. The question of how to objectively evaluate the quality of explanations is addressed in Section 5. The paper concludes in Section 6 with a discussion on general challenges in the field of explainable AI.  5  The authors of [24] showed that deep models can be easily fooled by physical-world attacks. For instance, by putting specific stickers on a stop sign one can achieve that the stop sign is not recognized by the system anymore.  1. Towards Explainable Artificial Intelligence  2  3  Need for Transparency and Trust in AI  Black box AI systems have spread to many of today’s applications. For machine learning models used, e.g., in consumer electronics or online translation services, transparency and explainability are not a key requirement as long as the overall performance of these systems is good enough. But even if these systems fail, e.g., the cell phone camera does not recognize a person or the translation service produces grammatically wrong sentences, the consequences are rather unspectacular. Thus, the requirements for transparency and trust are rather low for these types of AI systems. In safety critical applications the situation is very different. Here, the intransparency of ML techniques may be a limiting or even disqualifying factor. Especially if single wrong decisions can result in danger to life and health of humans (e.g., autonomous driving, medical domain) or significant monetary losses (e.g., algorithmic trading), relying on a data-driven system whose reasoning is incomprehensible may not be an option. This intransparency is one reason why the adoption of machine learning to domains such as health is more cautious than the usage of these models in the consumer, e-commerce or entertainment industry. In the following we discuss why the ability to explain the decision making of an AI system helps to establish trust and is of utmost importance, not only in medical or safety critical applications. We refer the reader to [91] for a discussion of the challenges of transparency. 2.1  Explanations Help to Find “Clever Hans” Predictors  Clever Hans was a horse that could supposedly count and that was considered a scientific sensation in the years around 1900. As it turned out later, Hans did not master the math but in about 90 percent of the cases, he was able to derive the correct answer from the questioner’s reaction. Analogous behaviours have been recently observed in state-of-the-art AI systems [46]. Also here the algorithms have learned to use some spurious correlates in the training and test data and similarly to Hans predict right for the ‘wrong’ reason. For instance, the authors of [44,46] showed that the winning method of the PASCAL VOC competition [23] was often not detecting the object of interest, but was utilizing correlations or context in the data to correctly classify an image. It recognized boats by the presence of water and trains by the presence of rails in the image, moreover, it recognized horses by the presence of a copyright watermark6. The occurrence of the copyright tags in horse images is a clear artifact in the dataset, which had gone unnoticed to the organizers and participants of the challenge for many years. It can be assumed that nobody has systematically checked the thousands images in the dataset for this kind of artifacts (but even if someone did, such artifacts may be easily overlooked). Many other examples of “Clever Hans” predictors have been described in the 6  The PASCAL VOC images have been automatically crawled from flickr and especially the horse images were very often copyrighted with a watermark.  4  W. Samek and K.-R. Müller  literature. For instance, [73] show that current deep neural networks are distinguishing the classes “Wolf” and “Husky” mainly by the presence of snow in the image. The authors of [46] demonstrate that deep models overfit to padding artifacts when classifying airplanes, whereas [63] show that a model which was trained to distinguish between 1000 categories, has not learned dumbbells as an independent concept, but associates a dumbbell with the arm which lifts it. Such “Clever Hans” predictors perform well on their respective test sets, but will certainly fail if deployed to the real-world, where sailing boats may lie on a boat trailer, both wolves and huskies can be found in non-snow regions and horses do not have a copyright sign on them. However, if the AI system is a black box, it is very difficult to unmask such predictors. Explainability helps to detect these types of biases in the model or the data, moreover, it helps to understand the weaknesses of the AI system (even if it is not a “Clever Hans” predictor). In the extreme case, explanations allow to detect the classifier’s misbehaviour (e.g., the focus on the copyright tag) from a single test image7 . Since understanding the weaknesses of a system is the first step towards improving it, explanations are likely to become integral part of the training and validation process of future AI models. 2.2  Explanations Foster Trust and Verifiability  The ability to verify decisions of an AI system is very important to foster trust, both in situations where the AI system has a supportive role (e.g., medical diagnosis) and in situations where it practically takes the decisions (e.g., autonomous driving). In the former case, explanations provide extra information, which, e.g., help the medical expert to gain a comprehensive picture of the patient in order to take the best therapy decision. Similarly to a radiologist, who writes a detailed report explaining his findings, a supportive AI system should in detail explain its decisions rather than only providing the diagnosis to the medical expert. In cases where the AI system itself is deciding, it is even more critical to be able to comprehend the reasoning of the system in order to verify that it is not behaving like Clever Hans, but solves the problem in a robust and safe manner. Such verifications are required to build the necessary trust in every new technology. There is also a social dimension of explanations. Explaining the rationale behind one’s decisions is an important part of human interactions [30]. Explanations help to build trust in a relationship between humans, and should therefore be also part of human-machine interactions [3]. Explanations are not only an inevitable part of human learning and education (e.g., teacher explains solution to student), but also foster the acceptance of difficult decisions and are important for informed consent (e.g., doctor explaining therapy to patient). Thus, even if not providing additional information for verifying the decision, e.g., because the patient may have no medical knowledge, receiving explanations usually make us feel better as it integrates us into the decision-making process. An AI system which interacts with humans should therefore be explainable. 7  Traditional methods to evaluate classifier performance require large test datasets.  1. Towards Explainable Artificial Intelligence  2.3  5  Explanations are a Prerequisite for New Insights  AI systems have the potential to discover patterns in data, which are not accessible to the human expert. In the case of the Go game, these patterns can be new playing strategies [82]. In the case of scientific data, they can be unknown associations between genes and diseases [51], chemical compounds and material properties [68] or brain activations and cognitive states [49]. In the sciences, identifying these patterns, i.e., explaining and interpreting what features the AI system uses for predicting, is often more important than the prediction itself, because it unveils information about the biological, chemical or neural mechanisms and may lead to new scientific insights. This necessity to explain and interpret the results has led to a strong dominance of linear models in scientific communities in the past (e.g. [42,67]). Linear models are intrinsically interpretable and thus easily allow to extract the learned patterns. Only recently, it became possible to apply more powerful models such as deep neural networks without sacrificing interpretability. These explainable non-linear models have already attracted attention in domains such as neuroscience [87,89,20], health [33,14,40], autonomous driving [31], drug design [70] and physics [78,72] and it can be expected that they will play a pivotal role in future scientific research. 2.4  Explanations are Part of the Legislation  The infiltration of AI systems into our daily lives poses a new challenge for the legislation. Legal and ethical questions regarding the responsibility of AI systems and their level of autonomy have recently received increased attention [21,27]. But also anti-discrimination and fairness aspects have been widely discussed in the context of AI [28,19]. The EU’s General Data Protection Regulation (GDPR) has even added the right to explanation to the policy in Articles 13, 14 and 22, highlighting the importance of human-understandable interpretations derived from machine decisions. For instance, if a person is being rejected for a loan by the AI system of a bank, in principle, he or she has the right to know why the system has decided in this way, e.g., in order to make sure that the decision is compatible with the anti-discrimination law or other regulations. Although it is not yet clear how these legal requirements will be implemented in practice, one can be sure that transparency aspects will gain in importance as AI decisions will more and more affect our daily lives.  3  Different Facets of an Explanation  Recently proposed explanation techniques provide valuable information about the learned representations and the decision-making of an AI system. These explanations may differ in their information content, their recipient and their purpose. In the following we describe the different types of explanations and comment on their usefulness in practice.  6  3.1  W. Samek and K.-R. Müller  Recipient  Different recipients may require explanations with different level of detail and with different information content. For instance, for users of AI technology it may be sufficient to obtain coarse explanations, which are easy to interpret, whereas AI researchers and developers would certainly prefer explanations, which give them deeper insights into the functioning of the model. In the case of image classification such simple explanations could coarsely highlight image regions, which are regarded most relevant for the model. Several preprocessing steps, e.g., smoothing, filtering or contrast normalization, could be applied to further improve the visualization quality. Although discarding some information, such coarse explanations could help the ordinary user to foster trust in AI technology. On the other hand AI researchers and developers, who aim to improve the model, may require all the available information, including negative evidence, about the AI’s decision in the highest resolution (e.g., pixelwise explanations), because only this complete information gives detailed insights into the (mal)functioning of the model. One can easily identify further groups of recipients, which are interested in different types of explanations. For instance, when applying AI to the medical domain these groups could be patients, doctors and institutions. An AI system which analyzes patient data could provide simple explanations to the patients, e.g., indicating too high blood sugar, while providing more elaborate explanations to the medical personal, e.g., unusual relation between different blood parameters. Furthermore, institutions such as hospitals or the FDA might be less interested in understanding the AI’s decisions for individual patients, but would rather prefer to obtain global or aggregated explanations, i.e., patterns which the AI system has learned after analyzing many patients. 3.2  Information Content  Different types of explanation provide insights into different aspects of the model, ranging from information about the learned representations to the identification of distinct prediction strategies and the assessment of overall model behaviour. Depending on the recipient of the explanations and his or her intent, it may be advantageous to focus on one particular type of explanation. In the following we briefly describe four different types of explanations. 1. Explaining learned representations: This type of explanation aims to foster the understanding of the learned representations, e.g., neurons of a deep neural network. Recent work [12,38] investigates the role of single neurons or group of neurons in encoding certain concepts. Other methods [84,93,64,65] aim to interpret what the model has learned by building prototypes that are representative of the abstract learned concept. These methods, e.g., explain what the model has learned about the category “car” by generating a prototypical image of a car. Building such a prototype can be formulated within the activation maximization framework and has been  1. Towards Explainable Artificial Intelligence  7  shown to be an effective tool for studying the internal representation of a deep neural network. 2. Explaining individual predictions: Other types of explanations provide information about individual predictions, e.g., heatmaps visualizing which pixels have been most relevant for the model to arrive at its decision [60] or heatmaps highlighting the most sensitive parts of an input [84]. Such explanations help to verify the predictions and establish trust in the correct functioning on the system. Layer-wise Relevance Propagation (LRP) [9,58] provides a general framework for explaining individual predictions, i.e., it is applicable to various ML models, including neural networks [9], LSTMs [7], Fisher Vector classifiers [44] and Support Vector Machines [35]. Section 4 gives an overview over recently proposed methods for computing individual explanations. 3. Explaining model behaviour: This type of explanations go beyond the analysis of individual predictions towards a more general understanding of model behaviour, e.g., identification of distinct prediction strategies. The spectral relevance analysis (SpRAy) approach of [46] computes such meta explanations by clustering individual heatmaps. Each cluster then represents a particular prediction strategy learned by the model. For instance, the authors of [46] identify four clusters when classifying “horse” images with the Fisher Vector classifier [77] trained on the PASCAL VOC 2007 dataset [22], namely (1) detect the horse and rider, 2) detect a copyright tag in portrait oriented images, 3) detect wooden hurdles and other contextual elements of horseback riding, and 4) detect a copyright tag in landscape oriented images. Such explanations are useful for obtaining a global overview over the learned strategies and detecting “Clever Hans” predictors [46]. 4. Explaining with representative examples: Another class of methods interpret classifiers by identifying representative training examples [41,37]. This type of explanations can be useful for obtaining a better understanding of the training dataset and how it influences the model. Furthermore, these representative examples can potentially help to identify biases in the data and make the model more robust to variations of the training dataset. 3.3  Role  Besides the recipient and information content it is also important to consider the purpose of an explanation. Here we can distinguish two aspects, namely (1) the intent of the explanation method (what specific question does the explanation answer) and (2) our intent (what do we want to use the explanation for). Explanations are relative and it makes a huge difference whether their intent is to explain the prediction as is (even if it is incorrect), whether they aim to visualize what the model “thinks” about a specific class (e.g., the true class) or whether they explain the prediction relative to another alternative (“why is this image classified as car and not as truck”). Methods such as LRP allow to answer all these different questions, moreover, they also allow to adjust the amount of positive and negative evidence in the explanations, i.e., visualize what speaks  8  W. Samek and K.-R. Müller  for (positive evidence) and against (negative evidence) the prediction. Such finegrained explanations foster the understanding of the classifier and the problem at hand. Furthermore, there may be different goals for using the explanations beyond visualization and verification of the prediction. For instance, explanations can be potentially used to improve the model, e.g., by regularization [74]. Also since explanations provide information about the (relevant parts of the) model, they can be potentially used for model compression and pruning. Many other uses (certification of the model, legal use) of explanations can be thought of, but the details remain future work.  4  Methods of Explainable AI  This section gives an overview over different approaches to explainable AI, starting with techniques which are model-agnostic and rely on a simple surrogate function to explain the predictions. Then, we discuss methods which compute explanations by testing the model’s response to local perturbations (e.g., by utilizing gradient information or by optimization). Subsequently, we present very efficient propagation-based explanation techniques which leverage the model’s internal structure. Finally, we consider methods which go beyond individual explanations towards a meta-explanation of model behaviour. This section is not meant to be a complete survey of explanation methods, but it rather summarizes the most important developments in this field. Some approaches to explainable AI, e.g., methods which find influencial examples [37], are not discussed in this section. 4.1  Explaining with Surrogates  Simple classifiers such as linear models or shallow decision trees are intrinsically interpretable, so that explaining its predictions becomes a trivial task. Complex classifiers such as deep neural networks or recurrent models on the other hand contain several layers of non-linear transformations, which largely complicates the task of finding what exactly makes them arrive at their predictions. One approach to explain the predictions of complex models is to locally approximate them with a simple surrogate function, which is interpretable. A popular technique falling into this category is Local Interpretable Model-agnostic Explanations (LIME) [73]. This method samples in the neighborhood of the input of interest, evaluates the neural network at these points, and tries to fit the surrogate function such that it approximates the function of interest. If the input domain of the surrogate function is human-interpretable, then LIME can even explain decisions of a model which uses non-interpretable features. Since LIME is model agnostic, it can be applied to any classifier, even without knowing its internals, e.g., architecture or weights of a neural network classifier. One major drawback of LIME is its high computational complexity, e.g., for state-ofthe-art models such as GoogleNet it requires several minutes for computing the explanation of a single prediction [45].  1. Towards Explainable Artificial Intelligence  9  Similar to LIME which builds a model for locally approximating the function of interest, the SmoothGrad method [85] samples the neighborhood of the input to approximate the gradient. Also SmoothGrad does not leverage the internals of the model, however, it needs access to the gradients. Thus, it can also be regarded as a gradient-based explanation method.  4.2  Explaining with Local Perturbations  Another class of methods construct explanations by analyzing the model’s response to local changes. This includes methods which utilize the gradient information as well as perturbation- and optimization-based approaches. Explanation methods relying on the gradient of the function of interest [2] have a long history in machine learning. One example is the so-called Sensitivity Analysis (SA) [62,10,84]. Although being widely used as explanation methods, SA technically explains the change in prediction instead of the prediction itself. Furthermore, SA has been shown to suffer from fundamental problems such as gradient shattering and explanation discontinuities, and is therefore considered suboptimal for explanation of today’s AI models [60]. Variants of Sensitivity Analysis exist which tackle some of these problems by locally averaging the gradients [85] or integrating them along a specific path [88]. Perturbation-based explanation methods [94,97,25] explicitly test the model’s response to more general local perturbations. While the occlusion method of [94] measures the importance of input dimensions by masking parts of the input, the Prediction Difference Analysis (PDA) approach of [97] uses conditional sampling within the pixel neighborhood of an analyzed feature to effectively remove information. Both methods are model-agnostic, i.e., can be applied to any classifier, but are computationally not very efficient, because the function of interest (e.g., neural network) needs to be evaluated for all perturbations. The meaningful perturbation method of [25,26] is another model-agnostic technique to explaining with local perturbations. It regards explanation as a meta prediction task and applies optimization to synthesize the maximally informative explanations. The idea to formulate explanation as an optimization problem is also used by other methods. For instance, the methods [84,93,64] aim to interpret what the model has learned by building prototypes that are representative of the learned concept. These prototypes are computed within the activation maximization framework by searching for an input pattern that produces a maximum desired model response. Conceptually, activation maximization [64] is similar to the meaningful perturbation approach of [25]. While the latter finds a minimum perturbation of the data that makes f (x) low, activation maximization finds a minimum perturbation of the gray image that makes f (x) high. The costs of optimization can make these methods computationally very demanding.  10  4.3  W. Samek and K.-R. Müller  Propagation-Based Approaches (Leveraging Structure)  Propagation-based approaches to explanation are not oblivious to the model which they explain, but rather integrate the internal structure of the model into the explanation process. Layer-wise Relevance Propagation (LRP) [9,58] is a propagation-based explanation framework, which is applicable to general neural network structures, including deep neural networks [13], LSTMs [7,5], and Fisher Vector classifiers [44]. LRP explains individual decisions of a model by propagating the prediction from the output to the input using local redistribution rules. The propagation process can be theoretically embedded in the deep Taylor decomposition framework [59]. More recently, LRP was extended to a wider set of machine learning models, e.g., in clustering [36] or anomaly detection [35], by first transforming the model into a neural network (‘neuralization’) and then applying LRP to explain its predictions. The leveraging of the model structure together with the use of appropriate (theoretically-motivated) propagation rules, enables LRP to deliver good explanations at very low computational cost (one forward and one backward pass). Furthermore, the generality of the LRP framework allows also to express other recently proposed explanation techniques, e.g., [81,95]. Since LRP does not rely on gradients, it does not suffer from problems such as gradient shattering and explanation discontinuities [60]. Other popular explanation methods leveraging the model’s internal structure are Deconvolution [94] and Guided Backprogagation [86]. In contrast to LRP, these methods do not explain the prediction in the sense “how much did the input feature contribute to the prediction”, but rather identify patterns in input space, that relate to the analyzed network output. Many other explanation methods have been proposed in the literature which fall into the “leveraging structure” category. Some of these methods use heuristics to guide the redistribution process [79], others incorporate an optimization step into the propagation process [39]. The iNNvestigate toolbox [1] provides an efficient implementation for many of these propagation-based explanation methods. 4.4  Meta-Explanations  Finally, individual explanations can be aggregated and analyzed to identify general patterns of classifier behavior. A recently proposed method, spectral relevance analysis (SpRAy) [46], computes such meta explanations by clustering individual heatmaps. This approach allows to investigate the predictions strategies of the classifier on the whole dataset in a (semi-)automated manner and to systematically find weak points in models or training datasets. Another type of meta-explanation aims to better understand the learned representations and to provide interpretations in terms of human-friendly concepts. For instance, the network dissection approach of [12,96] evaluates the semantics of hidden units, i.e., quantify what concepts these neurons encode. Other recent work [38] provides explanations in terms of user-defined concepts and tests to which degree these concepts are important for the prediction.  1. Towards Explainable Artificial Intelligence  5  11  Evaluating Quality of Explanations  The objective assessment of the quality of explanations is an active field of research. Many efforts have been made to define quality measures for heatmaps which explain individual predictions of an AI model. This section gives an overview over the proposed approaches. A popular measure for heatmap quality is based on perturbation analysis [9,75,6]. The assumption of this evaluation metric is that the perturbation of relevant (according to the heatmap) input variables should lead to a steeper decline of the prediction score than the perturbation of input dimensions which are of lesser importance. Thus, the average decline of the prediction score after several rounds of perturbation (starting from the most relevant input variables) defines an objective measure of heatmap quality. If the explanation identifies the truly relevant input variables, then the decline should be large. The authors of [75] recommend to use untargeted perturbations (e.g., uniform noise) to allow fair comparison of different explanation methods. Although being very popular, it is clear that perturbation analysis can not be the only criterion to evaluate explanation quality, because one could easily design explanations techniques which would directly optimize this criterion. Examples are occlusion methods which were used in [94,50], however, they have been shown to be inferior (according to other quality criteria) to explanation techniques such as LRP [8]. Other studies use the ‘pointing game” [95] to evaluate the quality of a heatmap. The goal of this game is to evaluate the discriminativeness of the explanations for localizing target objects, i.e., it is compared if the most relevant point of the heatmap lies on the object of designated category. Thus, these measures assume that the AI model will focus most attention on the object of interest when classifying it, therefore this should be reflected in the explanation. However, this assumption may not always be true, e.g., “Clever Hans” predictors [46] may rather focus on context than of the object itself, irrespectively of the explanation method used. Thus, their explanations would be evaluated as poor quality according to this measure although they truly visualize the model’s prediction strategy. Task specific evaluation schemes have also been proposed in the literature. For example, [69] use the subject-verb agreement task to evaluate explanations of a NLP model. Here the model predicts a verb’s number and the explanations verify if the most relevant word is indeed the correct subject or a noun with the predicted number. Other approaches to evaluation rely on human judgment [73,66]. Such evaluation schemes relatively quickly become impractical if evaluating a larger number of explanations. A recent study [8] proposes to objectively evaluate explanation for sequential data using ground truth information in a toy task. The idea of this evaluation metric is to add or subtract two numbers within an input sequence and measure the correlation between the relevances assigned to the elements of the sequence and the two input numbe"
"Under review  S YMPLECTIC ODE-N ET: L EARNING H AMILTONIAN DYNAMICS WITH C ONTROL Yaofeng Desmond Zhong∗ Princeton University y.zhong@princeton.edu  Biswadip Dey Siemens Corporate Technology biswadip.dey@siemens.com  Amit Chakraborty Siemens Corporate Technology amit.chakraborty@siemens.com  arXiv:1909.12077v1 [cs.LG] 26 Sep 2019  I NTRODUCTION  In the recent years, deep neural networks (Goodfellow et al., 2016) have become very accurate and widely-used in many application domains, such as image recognition (He et al., 2016), language comprehension (Devlin et al., 2019), and sequential decision making (Silver et al., 2017). To learn underlying patterns from data and enable generalization beyond the training set, the learning approach incorporates appropriate inductive bias (Haussler, 1988; Baxter, 2000) by promoting representations which are simple in some sense. It typically manifests itself via a set of assumptions which in turn can guide a learning algorithm to pick one hypothesis over another. The success in predicting an outcome for previously unseen data then depends on how well the inductive bias captures the ground reality. Inductive bias can be introduced as the prior in a Bayesian model, or via the choice of computation graphs in a neural network. In a variety of settings, especially in physical systems, wherein laws of physics are primarily responsible for shaping the outcome, generalization in neural networks can be improved by leveraging underlying physics for designing the computation graphs. Here, by leveraging a generalization of the Hamiltonian dynamics, we develop a learning framework which captures the underlying physics in the associated computation graph. Our results show that incorporation of such physics-based inductive bias can provide knowledge about relevant physical properties (mass, potential energy) and laws (conservation of energy) of the system. These insights, in turn, enable more accurate prediction of future behavior and improvements in out-of-sample behavior. Furthermore, learning a physically-consistent model of the underlying dynamics can subsequently enable usage of model-based controllers which can provide performance guarantees for complex, nonlinear systems. In particular, insight about kinetic and potential energy of a physical system can be leveraged to design appropriate control strategies, such as the method of controlled Lagrangian (Bloch et al., 2001) and interconnection & damping assignment (Ortega et al., 2002) , which can reshape the closed-loop energy landscape to achieve a broad range of control objectives (regulation, tracking, etc.). R ELATED W ORK Physics-based Priors for Learning in Dynamical Systems: The last few years have witnessed a significant interest in incorporating physics-based priors into deep learning frameworks. Such approaches, in contrast to more rigid parametric system identification techniques (Söderström & Stoica, 1988), use neural networks to approximate the state-transition dynamics and therefore are more expressive. Sanchez-Gonzalez et al. (2018), by representing the causal relationships in a physical system as a directed graph, use a recurrent graph network to infer latent space dynamics of robotic systems. Lutter et al. (2019) and Gupta et al. (2019) leverage Lagrangian mechanics to learn dynamics of kinematic structures from time-series data of position, velocity and acceleration. A more recent (concurrent) work by Greydanus et al. (2019) uses Hamiltonian mechanics to learn dynamics of autonomous, energy-conserved mechanical systems from time-series data of position, momentum and their derivatives. A key difference between these approaches and the proposed one is that our framework does not require any information about higher order derivatives (e.g. acceleration) and can incorporate external control into the Hamiltonian formalism. ∗  This was work done during Desmond’s internship at Siemens Corporate Technology, Princeton, NJ 08540, USA.  1  Under review Neural Networks for Dynamics and Control Inferring underlying dynamics from time-series data plays a critical role towards controlling closed-loop response of dynamical systems, such as robotic manipulators (Lillicrap et al., 2015) and building HVAC systems (Wei et al., 2017). Although use of neural networks towards identification and control of dynamical systems dates back to more than three decades (Narendra & Parthasarathy, 1990), recent advances in deep neural networks have led to renewed interest in this domain. Watter et al. (2015) learns dynamics with control from high-dimensional observations (raw image sequences) using a variational approach and designs an iterative LQR controller to control physical systems by imposing a locally linear constraint. Karl et al. (2016) and Krishnan et al. (2017) adopt a variational approach and use recurrent architectures to learn state-space models from noisy observation. SE3-Nets (Byravan & Fox, 2017) learn SE(3) transformation of rigid bodies from point cloud data. Ayed et al. (2019) use partial information about the system state to learn a nonlinear state-space model. However, this body of work, while attempting to learn state-space models, does not take physics-based priors into consideration. C ONTRIBUTION The main contribution of this work is two-fold. First, we introduce a learning framework called Symplectic ODE-Net (SymODEN) which encodes a generalization of the Hamiltonian dynamics. This generalization, by adding an external control term to the standard Hamiltonian dynamics, allows us to learn the system dynamics which conforms to Hamiltonian dynamics with control. With the learnt structured dynamics, we are able to design controllers to control the system to track a reference point. Moreover, by encoding the structure, we can achieve better predictions with smaller network sizes. Second, we take one step forward in combining the physics-based prior and the data-driven approach. Previous approaches (Lutter et al., 2019; Greydanus et al., 2019) require data in the form of generalized coordinates and their derivatives up to the second order. However, a large number of physical systems accomodates generalized coordinates which are non-Euclidean (e.g. angles), and such angle data is often obtained in the embedded form, i.e., (cos q, sin q) instead of the coordinate (q) itself. The underlying reason is that an angular coordinate lies on S1 instead of R1 . In contrast to previous approaches which do not address this aspect, SymODEN has been designed to work with angle data in the embedded form. Additionally, we leverage differentiable ODE solvers to avoid the need for estimating second-order derivatives of generalized coordinates.  2 2.1  P RELIMINARY C ONCEPTS H AMILTONIAN DYNAMICS  Lagrangian dynamics and Hamiltonian dynamics are both reformulation of Newtonian dynamics and they provide new insights into the laws of mechanics. In these formulations, the configuration of a system is described by generalized coordinates q = (q1 , q2 , ..., qn ). With time, the configuration point of the system moves in the configuration space, tracing out a trajectory. Lagrangian dynamics describe the evolution of this trajectory, i.e. the equations of motion, in the configuration space. Hamiltonian dynamics, however, track the change of system states in the phase space – consisting of generalized coordinates q = (q1 , q2 , ..., qn ) and generalized momenta p = (p1 , p2 , ..., pn ). In other words, Hamiltonian dynamics treats q and p on a equal footing and this leads to not only symmetric equations of motion but a whole new approach to classical mechanics as well (Goldstein et al., 2002). Beyond classical mechanics, the Hamiltonian dynamics is also widely used in statistical and quantum mechanics. In Hamiltonian dynamics, the time-evolution of a system is described by the Hamiltonian H(q, p), a scalar function of generalized coordinates and momenta. Moreover, in almost all physical systems, the Hamiltonian is same as the total energy and hence can be expressed as 1 H(q, p) = pT M−1 (q)p + V (q), (1) 2 where the mass matrix M(q) is positive definite and V (q) represents potential energy of the system. Correspondingly, the time-evolution of the system is governed by ∂H ∂H q̇ = ṗ = − , (2) ∂p ∂q where we have dropped explicit dependence on q and p for brevity of notation. Moreover, since  ∂H T  ∂H T Ḣ = q̇ + ṗ = 0, (3) ∂q ∂p the total energy is conserved along a trajectory of the system. The RHS of Equation (2) is called the symplectic gradient (Rowe et al., 1980) of H. Equation (3) shows moving along the symplectic gradient keeps the Hamiltonian constant. In this work, we consider a generalization of the Hamiltonian dynamics which provides a means to incorporate external control (u), such as force and torque. As external control is usually affine and influences the change of generalized momenta, we consider the following dynamics   "" ∂H #   q̇ 0 = ∂p + u. (4) ṗ g(q) − ∂H ∂q 2  Under review When u = 0, the generalized dynamics reduce to the classical Hamiltonian dynamics (2) and the total energy is conserved; however, when u 6= 0, the system has a dissipation-free energy exchange with the environment. 2.2  C ONTROL VIA E NERGY S HAPING  Once the dynamics of a system have been learned, it can be used to synthesize a controller to maneuver the system to a reference configuration q? . As the proposed approach offers insight about the energy associated with a system, it is a natural choice to exploit this information for designing controllers via energy shaping (Ortega et al., 2001). As energy is a fundamental aspect of physical systems, reshaping the associated energy landscape enables us to specify a broad range of control objectives and design nonlinear controllers with provable performance guarantees. If rank(g(q)) = rank(q), the system is fully-actuated and we have control over any dimension of “acceleration” in ṗ. For such class of systems, a controller u(q, p) = β (q) + v(p) can be designed via potential energy shaping β (q) and damping injection v(p). We restate the procedure from Ortega et al. (2001) using our notation for completeness. As the name suggests, the goal of potential energy shaping is to design β (q) such that the closed-loop system behaves as if its time-evolution is governed by a desired Hamiltonian Hd , i.e. "" #   "" ∂H #   ∂Hd q̇ 0 ∂p ∂p + β (q) = (5) = d ṗ g(q) − ∂H − ∂H ∂q ∂q where the desired Hamiltonian differs from the original Hamiltonian by the potential energy Hd (q, p) =  1 T −1 p M (q)p + Vd (q). 2  (6)  In other words, β (q) shape the potential energy such that the desired Hamiltonian Hd (q, p) has a minimum at (q? , 0). Then, by substituting (1) and (6) into (5), we get  ∂V ∂Vd  β (q) = gT (ggT )−1 − . (7) ∂q ∂q With potential energy shaping, we ensure that the system has the lowest energy at the desired reference point and in general the system would oscillate around this point. To ensure that the trajectory actually converge to this point, we add some damping 1 v(p) = gT (ggT )−1 (−Kd p)  (8)  Remark If the desired potential energy is chosen to be a quadratic of the form 1 (q − q? )T Kp (q − q? ), 2  (9)  ∂V − Kp (q − q? ) − Kd p. ∂q  (10)  Vd (q) = the external forcing term can be expressed as g(q)u =  This is the familiar PD controller with an additional energy compensation term. However, for under-actuated systems, potential energy shaping alone is not sufficient to maneuver the system to a desired configuration. Kinetic energy shaping (Chang et al., 2002) is also needed to design the controller.  3  S YMPLECTIC ODE-N ET  In this section, we introduce the network architecture of Symplectic ODE-Net. In Subsection 3.1, we show how to learn an ordinary differential equation with a constant control term. In Subsection 3.2, we assume we have access to generalized coordinate and momentum data and derive the network architecture. In Subsection 3.3, we take one step further to propose a data-driven approach to deal with data of embedded angle coordinates. In Subsection 3.4, we put together the line of reasoning introduced in the previous two subsections to propose SymODEN for learning dynamics on the hybrid space Rn × Tm . 3.1  N EURAL ODE WITH C ONSTANT F ORCING  Now we focus on the problem of learning the ordinary differential equation (ODE) from time series data. Consider an ODE: ẋ = f (x). Assume we don’t know the analytical expression of the right hand side (RHS) and we approximate it with a neural network. If we have time series data X = (xt0 , xt1 , ..., xtn ), how could we learn f (x) from the data? 1  if we have access to q̇ instead of p, we use q̇ instead in Equation (8)  3  Under review Chen et al. (2018) introduced Neural ODE, differentiable ODE solvers with O(1)-memory backpropagation. With Neural ODE, we make predictions by approximating the RHS function using a neural network fθ and put it into a ODE solver x̂t1 , x̂t2 , ..., x̂tn = ODESolve(xt0 , fθ , t1 , t2 , ..., tn ) We can then construct the loss function L = kX − X̂k22 and update the weights θ by backpropagating through the ODE solver. In theory, we can learn fθ in this way. In practice, however, the neural net is hard to train if n is large. If we have a bad initial estimate of the fθ , the prediction error would in general be large. Although |xt1 − x̂t1 | might be small, x̂tN would be far from xtN as error accumulates, which makes the neural network hard to train. In fact, the prediction error of x̂tN is not as important as x̂t1 . In other words, we should weight data points in a short time horizon more than the rest of the data points. In order to address this and better utilize the data, we introduce the time horizon τ as a hyperparameter and predict xti+1 , xti+2 , ..., xti+τ from initial condition xti , where i = 0, ..., n − τ . One challenge of leveraging Neural ODE to learn state-space models is how to learn the dynamics with the control term. Equation 4 has the form ẋ = f (x, u) with x = (q, p). A function like this cannot be put into Neural ODE directly. In general, if our data consists of trajectories of (x, u)t0 ,...,tn and u remains the same in a trajectory. We can approximate the augmented dynamics     ẋ fθ (x, u) = = f̃θ (x, u) (11) u̇ 0 Here, the input and output of f̃θ have the same dimension, which can be put into Neural ODE. The problem is then how to design the network architecture of f̃θ , or equivalently fθ such that we can learn the dynamics in an efficient way. 3.2  L EARNING FROM G ENERALIZED C OORDINATE AND M OMENTUM  Suppose we have trajectory data consisting of (q, p, u)t0 ,...,tn , where u remains constant in a trajectory. If we have the prior knowledge that the unforced dynamics of q and p is governed by Hamiltonian dynamics, we can use three neural nets – M−1 θ1 (q), Vθ2 (q) and gθ3 (q) – as function approximators to represent the inverse of mass matrix, potential energy and the control coefficient. Thus, "" ∂H #   θ1 ,θ2 0 ∂p (12) fθ (q, p, u) = ∂Hθ1 ,θ2 + g (q) u θ3 − ∂q where Hθ1 ,θ2 (q, p) =  1 T −1 p Mθ1 (q)p + Vθ2 (q) 2  (13)  The partial derivative in the expression can be taken care of by automatic differentiation. by putting the designed fθ (q, p, u) into Neural ODE, we obtain a systematic way of adding the prior knowledge of Hamiltonian dynamics into end-to-end learning. 3.3  L EARNING FROM E MBEDDED A NGLE DATA  In the previous subsection, we assume (q, p, u)t0 ,...,tn . In a lot of physical system models, the state variables involve angles which reside in the interval [−π, π). In other words, each angle resides on the manifold S1 . From a data-driven perspective, the data that respects the geometry is a 2 dimensional embedding (cos q, sin q). Furthermore, the generalized momentum data is usually not available. Instead, the velocity is often available. For example, in OpenAI Gym (Brockman et al., 2016) Pendulum-v0 task, the observation is (cos q, sin q, q̇). From a theoretic perspective, however, the angle itself instead of the 2D embedding is often used. The reason is that both the Lagrangian and Hamiltonian formulation are derived using generalized coordinates. Using a set of independent generalized coordinate makes the solution of the equations of motion easier. In this subsection, we take the data-driven standpoint. We assume all the generalized coordinates are angles and the data comes in the form of (x1 (q), x2 (q), x3 (q̇))t0 ,...,tn = (cos q, sin q, q̇)t0 ,...,tn . We aim to put our theoretical prior – Hamiltonian dynamics – into the data-driven approach. The goal is to learn the dynamics of x1 , x2 and x3 . Noticing p = M(x1 , x2 )q̇, we can write down the derivative of x1 , x2 and x3 , ẋ1 = − sin q ◦ q̇ = −x2 ◦ q̇ ẋ2 = cos q ◦ q̇ = x1 ◦ q̇ d d ẋ3 = (M−1 (x1 , x2 )p) = (M−1 (x1 , x2 ))p + M−1 (x1 , x2 )ṗ dt dt 4  (14)  Under review where “◦” represents the elementwise product (Hadamard product). We assume q and p evolves with the generalized Hamiltonian dynamics Equation 4. Here the Hamiltonian H(x1 , x2 , p) is a function of x1 , x2 and p instead of q and p. ∂H ∂p ∂H ∂x1 ∂H ∂x2 ∂H ṗ = − + g(q)u = − − + g(q)u ∂q ∂q ∂x1 ∂q ∂x2 ∂H ∂H ∂H ∂H = sin q ◦ − cos q ◦ + g(q)u = x2 ◦ − x1 ◦ + g(q)u ∂x1 ∂x2 ∂x1 ∂x2 q̇ =  (15)  (16)  Then the right hand side of Equation (14) can be expressed as a function of state variables and control (x1 , x2 , x3 , u). Thus, it can be put into the Neural ODE. We use three neural nets – M−1 θ1 (x1 , x2 ), Vθ2 (x1 , x2 ) and gθ3 (x1 , x2 ) – as function approximators. Substitute Equation (15) and (16) into (14), then the RHS serves as fθ (x1 , x2 , x3 , u). 2   ∂Hθ1 ,θ2 −x2 ◦ ∂p   ∂Hθ1 ,θ2  x1 ◦ ∂p (17) fθ (x1 , x2 , x3 , u) =     ∂Hθ1,θ2 ∂Hθ1,θ2 −1 −1 d −x ◦ +g (x ,x )u (M (x ,x ))p+M (x ,x ) x ◦ 1 θ3 1 2 1 2 1 2 2 θ1 θ1 dt ∂x1 ∂x2 where 1 T −1 p Mθ1 (x1 , x2 )p + Vθ2 (x1 , x2 ) 2 p = Mθ1 (x1 , x2 )x3  Hθ1 ,θ2 (x1 , x2 , p) =  (18) (19)  L EARNING ON H YBRID S PACES Rn × Tm  3.4  In Subsection 3.2, we treated the generalized coordinates as translational coordinate. In Subsection 3.3, we developed a method to better deal with embedded angle data. In most of physical systems, these two types of coordinates coexist. For example, robotics systems are usually modelled as interconnected rigid bodies. The positions of joints or center of mass are translational coordinates and the orientations of each rigid body are angular coordinates. In other words, the generalized coordinates lie on Rn × Tm , where Tm denotes the m-torus, with T1 = S1 and T2 = S1 × S1 . In this subsection, we put together the architecture of the previous two subsections. We assume the generalized coordinates are q = (r, φ ) ∈ Rn × Tm and the data comes in the φ)t0 ,...,tn . With similar line of reasoning, we use three neural nets – form of (x1 , x2 , x3 , x4 , x5 )t0 ,...,tn = (r, cos φ , sin φ , ṙ, φ̇ M−1 (x , x , x ), V (x , x , x ) and g (x , x , x ) – as function approximators. We have 1 2 3 θ 1 2 3 θ 1 2 3 2 3 θ1   x p = Mθ1 (x1 , x2 , , x3 ) 4 (20) x5 1 (21) Hθ1 ,θ2 (x1 , x2 , x3 , p) = pT M−1 θ1 (x1 , x2 , x3 )p + Vθ2 (x1 , x2 , x3 ) 2 with Hamiltonian dynamics, we have q̇ =    ∂Hθ1 ,θ2 ṙ = φ φ̇ ∂p "" ∂H  (22) #  θ1 ,θ2  ṗ =  x3 ◦  ∂Hθ1 ,θ2 ∂x2  ∂x1  − x2 ◦  ∂Hθ1 ,θ2 ∂x3  + gθ3 (x1 , x2 , x3 )u  (23)  Then     ẋ1 ṙ ẋ2    φ −x3φ̇     = fθ (x1 , x2 , x3 , x4 , x5 , u) ẋ3  =   φ x2φ̇ ẋ  4 −1 −1 d ẋ5 dt (Mθ1 (x1 , x2 , x3 ))p + Mθ1 (x1 , x2 , x3 )ṗ  (24)  φ come from Equation (22). Now we obtain a fθ which can be put into Neural ODE. Figure 1 shows the flow where the ṙ and φ̇ of the computation graph based on Equation (20)-(24). 2  In Equation (17), the derivative of M−1 θ1 (x1 , x2 ) can be expanded using chain rule and expressed as a function of the states.  5  Under review  Figure 1: The computation graph of SymODEN. Blue arrows indicate neural network parametrization. Red arrows indicate automatic differentiation. For a given (x, u), the computation graph outputs a fθ (x, u) which follows Hamiltonian dynamics with control. The function itself is an input to the Neural ODE to generate estimation of states at each time step. Since all the operations are differentiable, weights of the neural networks can be updated by backpropagation. 3.5  P OSITIVE D EFINITENESS OF M ASS MATRIX  In real physical systems, the mass matrix M is positive definite, which ensures a positive kinetic energy with a non-zero velocity. The positive definiteness of M implies the positive definiteness of M−1 θ1 . Thus, we impose this constraint in the −1 T network architecture by Mθ1 = Lθ1 Lθ1 , where Lθ1 is a lower-triangular matrix. The positive definiteness is ensured if the diagonal elements of Mθ−1 is positive. In practice, this can be done by adding a small constant  to the diagonal elements of 1 Mθ1 . It not only makes Mθ1 invertible, but also stabilize the training.  4  E XPERIMENTS  4.1  E XPERIMENTAL S ETUP  We evaluate our model on four tasks: Task 1: a pendulum with generalized coordinate and momentum data (learning on R1 ); Task 2: a pendulum with embedded angle data (learning on S1 ); Task 3: a cart-pole system (learning on R1 × S1 ) and Task 4: an acrobot (learning on T2 ). Model Variants. Besides the Symplectic ODE-Net model derived above, we consider a variant by approximating the Hamiltonian using a fully connected neural net Hθ1 ,θ2 . We call it Unstructured Symplectic ODE-Net (Unstructured SymODEN) since here we are not exploiting the structure of the Hamiltonian. Baseline Models. In order to show that we can learn the dynamics better with less parameters by leveraging prior knowledge, we set up baseline models for all four experiments. For the pendulum with generalized coordinate and momentum data, the naive baseline model approximate Equation (12) – fθ (x, u) – by a fully connected neural net. For all the other experiments, which involves embedded angle data, we set up two different baseline models: naive baseline approximate fθ (x, u) by a fully connected neural net. It doesn’t respect the fact that the coordinate pair, cos φ and sin φ , lie on Tm . Thus, we set up the geometric baseline model which approximate q̇ and ṗ with a fully connected neural net. This ensures that the angle data evolves on Tm . 3 Data Generation. For all tasks, we randomly generated initial conditions of states and combine them with 5 values of constant control, i.e., u = −2.0, −1.0, 0.0, 1.0, 2.0 to construct the initial conditions of simulation. The initial conditions are then put into simulators to integrate for 20 time steps to generate trajectory data. These trajectory data serve as training set. The simulators for different tasks are different. For Task 1, we integrate the true generalized Hamiltonian dynamics with a time interval of 0.05 seconds to generate trajectories . All the other tasks deal with embedded angle data and velocity directly so we leverage Open AI Gym (Brockman et al., 2016) simulators to generate trajectory data. One caveat of using Open AI Gym is that not all environments use the Runge-Kutta method (RK4) for simulation. Gym favors other numerical schemes over RK4 because of speed, but it is harder to learn the dynamics with inaccurate data. For example, if we plot the total energy as a function of time from data generated by Pendulum-v0 environment with zero action, we see that the total energy oscillates 3  For more information on model details, please refer to Appendix A.  6  Under review around a constant by a significant amount, even though the total energy should be conserved. Thus, for Task 1 and Task 2, we leverage Pendulum-v0 and CartPole-v1 and replace the numerical integrator of the environments to RK4. For Task 3, we leverage the Acrobot-v1 environment which is already using RK4. We also change the action space of Pendulum-v0, CartPole-v1 and Acrobot-v1 to a continuous space with a large enough bound. Model training. In all the tasks, we train our model using Adam optimizer (Kingma & Ba (2014)) with 1000 epochs. We set a time horizon τ = 3, and choose “RK4” as the numerical integration scheme in Neural ODE. We vary the size of training set by doubling from 16 state initial conditions to 1024 state initial conditions. Each state initial condition is combined with five constant control u = −2.0, −1.0, 0.0, 1.0, 2.0 to construct initial condition for simulation. Each trajectory is generated by putting the initial condition into the simulator and integrate 20 time steps forward. We set the size of mini-batches to be the number of state initial conditions. We logged the training error per trajectory and the prediction error per trajectory in each case for all the tasks. The training loss per trajectory is the mean squared error (MSE) between the estimation and the ground truth of 20 time steps. To evaluate the performance of each model in terms of long time prediction, we construct the metric of prediction error per trajectory by using the same state initial condition in the training set with a constant control of u = 0.0, integrating 40 time steps forward, and calculating the MSE of 40 time steps The reason of using only the unforced trajectories is that a constant nonzero control might cause the velocity to keep increasing or decreasing over time and large absolute values of velocity are of little interest in designing controller. 4.2  R ESULTS  Figure 2: Training error per trajectory and prediction error per trajectory for all 4 tasks with different number of training trajectories. Horizontal axis shows number of state initial condition of 16, 32, 64, 128, 256, 512, 1024 in the training set. Both the horizontal axis and vertical axis are in log scale. Figure 2 shows the variation in training error and prediction error with changes in the number of state initial conditions in the training set. We can see that SymODEN yields better generalization in all the tasks. In Task 3, although the Geometric Baseline Model beats the other ones in terms of training error, SymODEN generates more accurate predictions, indicating overfitting in the Geometric Baseline Model. By incorporating the physics-based prior of Hamiltonian dynamics, SymODEN learns dynamics that obey physical law and thus performs better in prediction. In most cases, SymODEN trained with less training data beats other models with more training data in terms of training error and prediction error, indicating that better generalization can be achieved with fewer training samples. Figure 3 shows how the MSE and the total energy evolves along a trajectory with a previously unseen initial condition. For all the tasks, the MSE of the baseline models diverge faster than SymODEN. The Unstructured SymODEN works well in Task 1, Task 2 and Task 4 but not so well in Task 3. As for the total energy, in the two pendulum tasks, SymODEN and Unstructured SymODEN conserve total energy by oscillating around a constant value. In these models, the Hamiltonian itself is learnt and the prediction of the future states stay around a level set of the Hamiltonian. Baseline models, however, fail to find the conservation and the estimation of future states drift away from the initial Hamiltonian level set. 4.3  TASK 1: P ENDULUM WITH G ENERALIZED C OORDINATE AND M OMENTUM DATA  In this task, the dynamics has the following form q̇ = 3p,  ṗ = −5 sin q + u  2  with Hamiltonian H(q, p) = 1.5p + 5(1 − cos q). In other words M (q) = 3, V (q) = 5(1 − cos q) and g(q) = 1. 7  (25)  Under review  Figure 3: Mean square error and total energy of test trajectories. SymODEN works the best in terms of both MSE and total energy. SymODEN predicts trajectories that match the ground truth since it has learnt the Hamiltonian and discovered the conservation from data. The ground truth of energy in all four tasks stay constant.  In Figure 4, The ground truth is an unforced trajectory which is energy-conserved. The prediction trajectory of the baseline model does not conserve energy while both the SymODEN and its unstructured variant predict energyconserved trajectories. For SymODEN, the learnt gθ3 (q) and Mθ−1 (q) matches the ground 1 truth well. Vθ2 (q) differs from the ground truth with a constant. This is acceptable since the potential energy is a relative notion. Only the derivative of Vθ2 (q) plays a role in the dynamics. In this task, we are treating q as a variable in R1 and our training set contains initial condition of q ∈ [−π, 3π]. The learnt functions do not extrapolate well outside this range, as we can see Figure 4: Sample trajectories and learnt functions of Task 1. (q) and from the left part in the figures of Mθ−1 1 Vθ2 (q). We address this issue by working directly with embedded angle data, which lead to the next subsection. 4.4  TASK 2: P ENDULUM WITH E MBEDDED DATA  The dynamics of this task are the same as Equation (25) but the training data are generated by the OpenAI Gym simulator. Here we do not have access to the true generalized momentum data, and the learnt function matches the ground truth with a scaling β, as shown in Figure 5. To explain the scaling, let us look at the following dynamics Figure 5: Without true generalized momentum data, the learnt functions match the ground truth with a scaling. Here β = 0.357 q̇ = p/α, ṗ = −15α sin q + 3αu (26) with Hamiltonian H = p2 /(2α) + 15α(1 − cos q). If we only look at the dynamics of q, we have q̈ = −15 sin q + 3u, which is independent of α. If we don’t have access to the generalized momentum p, our trained neural network may converge to a Hamiltonian with a αe which is different from the true value, αt = 1/3, in this task. By a scaling β = αt /αe = 0.357, the learnt functions match the ground truth. Even we are not learning the true αt , we can still perform prediction and control since we are learning the dynamics of q correctly. We let Vd = −Vθ2 (q), then the desired Hamiltonian has minimum energy when the pendulum rests at the upward position. For the damping injection, we let Kd = 3. Then from Equation (7) and (8), the 8  Under review controller we design is   ∂V (cos q, sin q) θ2 − 3 q̇ u(cos q, sin q, q̇) = gθ−1 (cos q, sin q) 2 3 ∂q    ∂V ∂Vθ2 θ2 = gθ−1 (cos q, sin q) 2 − sin q + cos q − 3 q̇ 3 ∂ cos q ∂ sin q  (27)  Only SymODEN out of all models we consider provides the learnt potential energy which is required to construct the controller. Figure 6 shows how the states evolve when the controller is fed into the OpenAI Gym simulator. We can successfully control the pendulum into the inverted position using the controller based Figure 6: Time-evolution of the state variables (cos q, sin q, q̇) when the on learnt model even though the absolute maxi- closed-loop control input u(cos q, sin q, q̇) is governed by Equation (27). mum control u, 7.5, is more than three times larger than the absolute maximum u in the training set, which is 2.0. This shows SymODEN extrapolates well. 4.5  TASK 3: C ART P OLE S YSTEM  The CartPole system is an underactuated system and to design a controller to balance the pole from arbitrary initial condition requires trajectory optimization or kinetic energy shaping. Here we following the setup in the OpenAI Gym CartPole-v1 environment: “In CartPole-v1, the pendulum starts upright and the goal is to prevent it from falling over. The episode ends when the pole is more than 15 degrees from vertical or the cart moves more than 2.4 units from the center” (Car). Since the initial condition is close to the goal, after learning the dynamics, we are able to design a PD controller based on the learnt dynamics and feed the controller back to the OpenAI Gym simulator. u = Kp sin q + Kd q̇  (28)  Figure 7 shows the results of control with Kp = 70 and Kd = 0.9. In 8 seconds, the pole remains within 15 degrees from vertical and cart remains within 0.3 units from the center.  Figure 7: Time-evolution of the state variables when the closed-loop control input is governed by Equation (28).  5  C ONCLUSION  Here we have introduced Symplectic ODE-Net which provides a systematic way to incorporate the prior knowledge of Hamiltonian dynamics with control into a deep learning framework. We show that SymODEN achieves better extrapolation with fewer training samples by learning an interpretable, physically-consistent state-space model. In future works, a broader class of physics-based prior such as port-Hamiltonian system can be introduced to model a larger class of physical systems. SymODEN can work with embedded angle data or when we only have access to velocity instead of generalized momentum. Future works would explore, other types of embedding, such as embedded 3D orientations.  R EFERENCES Cartpole-v1. https://gym.openai.com/envs/CartPole-v1/. Accessed"
"GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue ‡  Jun Quan‡∗, Deyi Xiong‡†, Bonnie Webber§ and Changjian Hu¶ School of Computer Science and Technology, Soochow University, Suzhou, China § University of Edinburgh, Edinburgh, UK ¶ Lenovo Research AI Lab, Beijing, China terryqj0107@gmail.com, dyxiong@suda.edu.cn, bonnie.webber@ed.ac.uk, hucj1@lenovo.com  arXiv:1909.12086v1 [cs.CL] 26 Sep 2019   1 Introduction Due to the rhetorical principle of saving words and avoiding repetitions, ellipsis and co-reference occur frequently in multi-turn dialogues leaving utterances paragmatically incomplete if they are separate from context. Humans can easily understand utterances with anaphorically referenced or absent ∗  Work performed during an internship at Lenovo Research AI Lab. † Corresponding author  information (e.g., Q2 and Q3 in Table 1) based on the dialogue context while dialogue systems often fail to understand such utterances correctly, which may result in false or incoherent responses. If user utterances can be automatically supplemented with information that is left out or substituted by anaphora according to the dialogue context as humans do (e.g., Q2 : I want cheap Italian restaurants. Q3 : Yes, I would like the phone number please.), dialogue models may understand user requests correctly and would not generate wrong responses caused by ellipsis and co-reference phenomena. Especially in task-oriented dialogue systems, explicitly providing such information to the models can effectively improve the success rate of task completion. In order to achieve this goal, we propose an endto-end generative ellipsis and co-reference resolution model (GECOR) for task-oriented dialogue in this paper. The essential idea behind GECOR is that we treat the resolution of ellipsis and co-reference in user utterances as a generation task: transforming a user utterance with ellipsis or anaphora into a new utterance where the left-out or referred expressions are automatically generated from the dialogue context. We refer to the new utterance as the complete version of the original utterance. We use an endto-end sequence-to-sequence model with two encoders for this transformation task, where one encoder reads the user utterance and the other the dialogue context and the decoder generates the complete utterance. Since most omitted expressions or antecedents can be found in the dialogue context, we resort to the attention and copy mechanism to detect such fragments in previous context and copy them into the generated complete utterance. We then incorporate GECOR into an end-toend task-oriented dialogue system in a multi-task  learning framework. The entire model contains two encoders (one for user utterance and the other for the dialogue context) and three decoders: one decoder for predicting dialogue states, the second decoder for generating complete user utterances and the third decoder for generating system responses. The three decoders are jointly trained. In order to train GECOR with the task-oriented dialogue model, we manually annotate the public task-oriented dialogue dataset CamRest676 with omitted expressions and substitute anaphora in the dataset with corresponding antecedents. The new dataset can be used either to train a standalone ellipsis or co-reference resolution model or to jointly train a task-oriented dialogue model equipped with the ellipsis / co-reference resolution model. We conduct a series of experiments and analyses, demonstrating that the proposed method can significantly outperform a strong baseline model. Our contributions are threefold: • We propose an end-to-end generative resolution model that attempts to solve the ellipsis and co-reference reolution in a single unified framework, significantly different from previous end-to-end co-reference resolution network with two phases of detection and candidate ranking. • To the best of our knowledge, this is the first attempt to combine the task of ellipsis and coreference resolution with the multi-turn taskoriented dialogue. The success rate of task completion is significantly improved with the assistance of the ellipsis and co-reference resolution. • We construct a new dataset based on CamRest676 for ellipsis and co-reference resolution in the context of task-oriented dialogue.1  2 Related Work Ellipsis recovery: The earliest work on ellipsis as far as we know is the PUNDIT system (Palmer et al., 1986) which discusses the communication between the syntactic, semantic and pragmatic modules that is necessary for making implicit linguistic information explicit. Dalrymple et al. (1991) and Shieber et al. (1996) 1  The new dataset and the code our proposed system are available https://multinlp.github.io/GECOR/  of at  establish a set of linguistic theories in the ellipsis recovery of English verb phrases. Nielsen (2003) first proposes an end-to-end computable system to perform English verb phrase ellipsis recovery on the original input text. Liu et al. (2016) propose to decompose the resolution of the verb phrase ellipsis into three sub-tasks: target detection, antecedent head resolution, and antecedent boundary detection. Co-reference resolution: Co-reference resolution is mainly concerned with two sub-tasks, referring expressions (i.e., mentions) detection, and entity candidate ranking. Uryupina and Moschitti (2013) propose a rule-based approach for coreference detection which employs parse tree features with an SVM model. Peng et al. (2015) improve the performance of mention detection by applying a binary classififier on their feature set. In recent years, applying deep neural networks to the co-reference resolution has gained great success. Clark and Manning (2016) apply reinforcement learning on mention-ranking coreference resolution. Lee et al. (2017) introduce the first end-to-end co-reference resolution model. Lee et al. (2018) present a high-order co-reference resolution model with coarse-to-fine inference. Ellipsis and co-reference resolution in QA and Dialogue: The methods mentioned above do not generalize well to dialogues because they normally require a large amount of well-annotated contextual data with syntactic norms and candidate antecedents. In recent years, a few studies try to solve ellipsis / co-reference resolution tailored for dialogue or QA tasks. Kumar and Joshi (2016) train a semantic sequence model to learn semantic patterns and a syntactic sequence model to learn linguistic patterns to tackle with the non-sentential (incomplete) questions in a question answering system. Zheng et al. (2018) builds a seq2seq neural network model for short texts to identify and recover ellipsis. However, these methods are still limited to short texts or one-shot dialogues. Our work is the first attempt to provide both solution and dataset for ellipsis and co-reference resolution in multi-turn dialogues. End-to-end task-oriented dialogue: Taskoriented dialogue systems have evolved from traditional modularized pipeline architectures (Rudnicky et al., 1999; Zue et al., 2000; Zue and Glass, 2000) to recent end-to-end neural frameworks (Eric and Manning, 2017a,b;  Turn Q1 A1 Q2 A2  Dialogue I would like an Italian restaurant. What price range do you have in mind? I want cheap ones. Pizza Hut Cherry Hinton serves Italian food in the south part of town. Would you like their phone number? Q3 Yes, please. User utterances after resolution Q2 I want cheap Italian restaurants. Q3 Yes, I would like the phone number please.  Table 1: Examples of ellipsis and co-reference resolution  Lei et al., 2018; Jin et al., 2018). Our work is an innovative combination of ellipsis and co-reference resolution and the end-to-end task-oriented dialogue.  3 The GECOR Model In this section, we reformulate the ellipsis and coreference resolution task in the context of multiturn dialogue and detail the proposed GECOR model. 3.1  Ellipsis and Co-Reference Resolution Reformulation  Our task is to reconstruct a pragmatically complete utterance from a user utterance where the ellipsis and/or co-reference phenomena are present according to the dialogue context. Table 1 provides examples of reconstructed utterances in which the omitted information is recovered or the anaphor is substituted with referred expressions. We attempt to solve the resolution of ellipsis and co-reference in a unified framework because in essence both ellipsis and co-reference can be understood from contextual clues. We consider these two problems in multi-turn dialogue and reformulate the resolution of them as a generation problem: generating the omitted or referred expressions. In this way, the modeling of ellipsis and co-reference is in line with response generation in dialogue modeling. Unlike previous methods that combine detection and ranking models, our generation-based formulation is not constrained by the syntactic forms of ellipsis or co-reference in sentences. They can be either words (e.g., noun, verb) or phrases or even clauses. Furthermore, the formulation does not need to provide a set of candidate antecedents  to be resolved. Previous studies usually need to traverse the text when there are multiple ellipsis or anaphora to be resolved, which leads to a high computational complexity. In this reformulation, we assume that the dialogue context is composed of all utterances from the beginning of the dialogue to the current user utterance. Both the context and the user utterance in question are input to the GECOR model to generate the complete version of the user utterance. 3.2 Model Structure The GECOR model is shown in Figure 1. The model essentially contains an embedding module, a user utterance encoder, a dialogue context encoder and a decoder with either copy (Gu et al., 2016) or gated copy mechanism (modified from See et al. (2017)). Both the generation probability over the entire vocabulary and the copy probability over all words from the dialogue context are taken into account for predicting the complete user utterance. Embedding Layer In GECOR, we first tokenize the input user utterance and the dialogue context. We then use GloVe (Pennington et al., 2014) (the pre-trained 50-dimensional word vectors) in the embedding layer to obtain word embeddings. Let U = {u1 , ..., um }, C = {c1 , ..., cn } be representations of the tokenized utterance and context sequence. Utterance and Context Encoder We use a single-layer bidirectional GRU to construct both encoders. The forward and backward hidden states over the input embeddings from the embedding layer are concatenated to form the hidden states of the two encoders. Decoder The decoder is a single-layer unidirectional GRU. In the decoder, the attention distribution at is calculated as in Bahdanau et al. (2015): eti = v T tanh(Wh hi + Ws st−1 + battn ) (1) at = sof tmax(et )  (2)  where v, Wh , Ws and battn are learnable parameters, hi is the hidden state for word ui from the sequence produced by the utterance encoder. The attention distribution at is used to produce a weighted sum of the encoder hidden states, known as the context vector h∗t : h∗t =  X  ati hi  (3)  i  It is fed into the single-layer unidirectional GRU together with the previous decoder state st and the  Figure 1: The end-to-end generative model for ellipsis and co-reference resolution (GECOR).  word embedding yt−1 of the previously generated word to obtain the decoder state st . The updated st−1 is then concatenated with the context vector h∗t to produce the generation probability distribution over the vocabulary V as follows: 1 (4) P g (yt ) = eψg (yt ) , yt ∈ V Z ψg (yt = vi ) = vi T (Wgh h∗t + Wgs st + bg )(5) st = GRU ([yt−1 ; h∗t ], st−1 )  and σ is a non-linear activation function. ψc is the score function for the copy-mode and Z is the normalization term shared by equation (4) and (7). Both probabilities from the two modes contribute to the final probability distribution over the extended vocabulary (the vocabulary plus the words from the dialogue context) which is calculated as follows:  (6)  P (yt ) = P g (yt ) + P c (yt ), yt ∈ V ∪ C (9)  where Whg , Wsg and bg are learnable parameters and vi is the one-hot indicator vector for word vi ∈ V. ψg is the score function for the generation-mode and Z is the normalization term shared by the generation-mode and copy-mode. Copy Network The copy network (Gu et al., 2016) is used to calculate the probabilities for words copied from the dialogue context. These words are parts of the omitted or referred expressions to be predicted. We build the copy network on the top of the context encoder. The probability for copying each word from the dialogue context is computed as follows: |C| 1 X ψc (ci ) c e , yt ∈ C (7) P (yt ) = Z i:c =y  which is used to predict the final output word. Gated Copy An alternative to the copy network is the gated copy mechanism that use a gate to regulate the contributions of the generation and copy mode to the final prediction. The gate pgen is calculated as follows:  i  t  ψc (yt = ci ) = σ(Wc hci + bc )st  (8)  where Wc and bc are learnable parameters, hci is the output for word ci from the context encoder,  pgen = σ(Wh h∗t +Ws st +Wy yt−1 +bt )(10) P (yt ) = pgen P g (yt )+(1−pgen )P c (yt )(11) where Wh , Ws , Wy and bt are learnable parameters and σ is the sigmoid function. Training The standard cross-entropy loss is adopted as the loss function to train the GECOR model.  4 Task-Oriented Dialogue with GECOR We integrate the proposed GECOR into an end-toend task-oriented dialogue system TSCP proposed  Figure 2: The architecture of the end-to-end task-oriented dialogue enhanced with the GECOR model. Decoder 1: BSpan decoder. Decoder 2: completed user utterance decoder. Decoder 3: machine response decoder.  by Lei et al. (2018) in a multi-task learning framework, which is shown in Figure 2. The GECORequipped TSCP model contains the embedding layer, the utterance and context encoders, and three decoders: decoder 1 for generating belief spans (BSpan) defined in (Lei et al., 2018) which are text spans for tracking dialogue states (e.g., hinf iItalian, cheaph/inf i; hreqiphoneh/reqi), decoder 2 for complete user utterances and decoder 3 for machine responses. The embedding layer and encoders are the same as described in section 3. BSpan Decoder Unlike Lei et al. (2018), we do not concatenate current user utterance with previously generated machine response. At each turn of dialogue, the user utterance and the previous BSpan (the dialogue states updated to the previous turn) are used as the inputs to the user utterance encoder. The outputs of this encoder are then fed into the BSpan decoder for predicting the new BSpan for the current turn and a cross-entropy loss L1 is calculated. The user utterance encoder hidden states, the last hidden state and the output of the BSpan decoder are input into the other two decoders. Complete User Utterance Decoder The basic structure of this decoder is the same as the decoder described in the last section.We pass the last hidden state of the BSpan decoder to the initial state of this decoder. In addition to the inputs from the user utterance encoder and the dialogue context  encoder, we also input the output of the BSpan decoder into this decoder. The generation probability Pgt , copy probability Ptc1 for copying tokens in BSpan, and copy probability Ptc2 for copying words in the dialogue context are calculated with a shared normalization term and combined for the final probability computation: Pt = Ptg + Ptc1 + Ptc2  (12)  Pt is then used to decode the words in the complete user utterance. For this decoder, the second cross-entropy loss L2 is calculated. Machine Response Decoder Similar to the previous two decoders, the machine response decoder is also a single-layer unidirectional GRU, the initial state of which is set to the last hidden state of the complete user utterance decoder. In this decoder, we compute three context vectors for each decoder state st . The first context vector h∗t1 is calculated over the user utterance encoder hidden states while the other two context vectors h∗t2 , h∗t3 are calculated over the BSpan decoder hidden states and the complete user utterance decoder hidden states, respectively. The concatenation of st , h∗t1 , h∗t2 , h∗t3 and the Knowledge Base (KB) matching vector Kt (a one-hot representation of the retrieval results in KB according to the constraints in the corresponding BSpan) is used to generate the output and update the decoder state. The generated output is then concatenated with the three context vectors to feed into a layer to produce the gener-  Turn Q1 A1 Q2 Q2 (Complete) Q2 (Ellipsis) Q2 (Co-reference)  Dialogue I would like a traditional food restaurant. What price range do you have in mind? I don’t care. I don’t care about the price range. I don’t care. I don’t care about it.  Table 2: An example of the ellipsis / co-reference annotation  ation probability distribution over the vocabulary. Similar to the complete user utterance decoder, we also use the copy mechanism in the machine response decoder. The third cross-entropy loss L3 is then calculated. Training The final loss for the multi-task learning framework is estimated as follows: L = L1 + L2 + L3  (13)  We learn parameters to minimize the final loss.  5 Data Annotation for Ellipsis and Co-Reference Rosultion in Dialogue Since there are no publicly available labeled data for the resolution of ellipsis and co-reference in dialogue, we manually annotate such a new dataset based on the public dataset CamRest676 (Wen et al., 2016a,b) from the restaurant domain. Annotation Specification Annotation cases for user utterances can be summarized into the following three conventions: • As shown in Table 2, if a user utterance contains an ellipsis or anaphor, we manually resolve the ambiguity of ellipsis or anaphor and supplement the user utterance with a correct expression by checking the dialogue context. In doing so, we create a pragmatically complete version for the utterance. If the utterance only contains an ellipsis and the ellipsis can be replaced with an anaphor, we create a co-reference version for it. Similarly, if the utterance only contains an anaphor and the anaphor can be omitted, we create an ellipsis version for the utterance. • If the user utterance itself is pragmatically complete, without any ellipsis or anaphora, we create an anaphor and ellipsis version for it if such a creation is appropriate. • If the utterance itself is complete and it is not suitable to create an ellipsis or anaphor version, we just do nothing.  With the annotation convention described above, for each user utterance in the dataset, we can label it as l ∈ {ellipsis, co-reference, complete} or create two other versions for it if appropriate. Please notice that these labels are used only for dataset statistics or for designing experiments, not for training our models. Dataset statistics The CamRest676 dataset contains 676 dialogues, with 2,744 user utterances. After annotation, 1,174 ellipsis versions and 1,209 co-reference versions are created from the 2,744 user utterances. 1,331 incomplete utterances are created that they are an either ellipsis or co-reference version. 1,413 of the 2,744 user utterances are complete and not amenable to change. No new versions are created from these 1,413 utterances. Dataset Split for Experiments We split the new dataset into a training set (accounting for 80%) and validation set (accounting for 20%) which can be used for the stand-alone ellipsis/coreference resolution task and the multi-task learning of both the ellipsis/co-reference resolution and end-to-end task-oriented dialogue.  6 Experiments In this section we conducted experiments on the new dataset to examine the generative ellipsis/coreference resolution model and its integration into the end-to-end task-oriented dialogue. 6.1 Evaluation Metrics As far as we know, there is no end-to-end generative ellipsis and co-reference resolution model applied to multi-turn dialogues. Therefore there are no off-the-shelf metrics tailored to this evaluation. Since we deal with two tasks: the task of ellipsis/co-reference resolution (resolution task for short) and the task-oriented dialogue with integrated ellipsis/co-reference resolution (hereafter dialogue task), we use two sets of evaluation metrics. For the resolution task, we use the exact match rate (EM) that measures whether the generated utterances exatly match the gold utterances. BLEU (Papineni et al., 2002) and F1 score (a balance between word-level precision and recall) are also used for the resolution task to evaluate the quality of generated utterances at the n-gram and word level. We use the success F1 which is defined as the F1 score of requested slots correctly answered in dialogues to evaluate task comple-  Data  Model  Ellipsis  Co-reference  Mixed  Baseline GECOR 1 GECOR 2 Baseline GECOR 1 GECOR 2 Baseline GECOR 1 GECOR 2  EM(%) 49.99 67.56 67.75 55.64 71.35 71.18 50.38 68.52 66.22  EM 1(%) 68.88 92.07 91.38 76.03 91.67 93.80 70.89 92.03 91.64  EM 2(%) 27.31 37.18 38.46 33.60 47.68 44.92 28.57 42.04 37.45  Resolution Task BLEU(%) F1 (%) 73.26 90.89 83.69 96.25 82.94 96.58 78.12 92.58 85.89 96.49 85.93 97.09 74.11 90.93 83.91 95.88 82.98 96.47  Prec.(%) 92.14 98.28 98.48 93.28 98.19 98.46 91.72 98.12 98.41  Rec.(%) 89.67 94.30 94.76 91.89 94.86 95.76 90.15 93.74 94.60  Reso.F1 (%) 44.47 70.48 70.85 44.24 64.93 71.26 44.10 66.06 66.16  Table 3: Results of the resolution task on the dataset. GECOR 1/2: the GECOR model with the copy/gated copy mechanism. EM 1 and EM 2 respectively indicate the situation that the input utterance is complete or incomplete while EM is the comprehensive evaluation of the two situations. Reso.F1 : Resolution F1  tion rate for the dialogue task, similar to Lei et al. (2018). 6.2  Parameter Settings  For all our models, both the size of hidden states and word embeddings were set to 50. The vocabulary size |V| was set to 800 and the batch size was set to 32. We trained our models via the Adam optimizer (Kingma and Ba, 2015), with a learning rate of 0.003 and a decay parameter of 0.5. Early stopping and dropout were used to prevent overfitting, and the dropout rate was set to 0.5. 6.3  Baselines and Comparisons  For the resolution task, we compared our GECOR model with the baseline model proposed by Zheng et al. (2018) which is a seq2seq neural network model that identifies and recovers ellipsis for short texts. For the dialogue task, we compared our multitask learning framework with the baseline model TSCP proposed by Lei et al. (2018) which is a seq2seq model enhanced with reinforcement learning. We ran the source code2 on our dataset to get the baseline results for comparison. For the resolution task, we also performed a comparison study to examine the impacts of the gate mechanism incorporated into the copy network on the GECOR model and on the multi-task learning dialogue model. 6.4  The GECOR Model  Our generative resolution model was trained on three types of data: the ellipsis data where only ellipsis version utterances from the annotated dataset were used, the co-reference data where 2  https://github.com/WING-NUS/sequicity  only co-reference version utterances from the annotated dataset were used, and the mixed data where we randomly selected a version for each user utterance from {ellipsis, co-reference, complete}. In the mixed data, 633 turns are with ellipsis user utterances, 698 turns are with co-reference user utterances, and the rest are with complete user utterances. The experimental results of the GECOR and baseline model (Zheng et al., 2018) on the three different datasets are shown in Table 3. Overall results From the third column of the table, we find that the GECOR model with the copy mechanism (GECOR 1) improves the exact match rate (EM) by more than 17 points on the ellipsis version data, more than 15 points on the co-reference data, and more than 18 points on the mixed data. We further define a metric we term as Resolution F1 that is an F1 score calculated by comparing machine-generated words with ground truth words for only the ellipsis / co-reference part of user utterances. The GECOR model achieves consistent and significant improvements over the baseline in terms of BLEU, F1 and Resolution F1 in addition to the EM metric . The major difference between the GECOR and the baseline is that the former tries to copy words from the dialogue context. The improvements, especially the improvements on the ellipsis resolution (higher than those on the co-reference resolution) indicate that the copy mechanism is crucial for the recovery of ellipsis and co-reference. Effect of the two copy mechanisms Comparing the GECOR 1 against the GECOR 2 (with the gated copy mechanism), we can find that the gating between copy and generation is helpful in terms of the word-level quality (F1 and Resolution F1 score) but not in terms of the fragment  Data  Model  Complete  TSCP TSCP Our Model TSCP Our Model TSCP Our Model  Ellipsis Co-reference Mixed  EM(%) 60.83 68.56 66.47  Resolution Task BLEU(%) F1(%) Prec.(%) 78.89 95.64 97.79 83.98 96.61 98.09 83.63 96.26 98.16  Rec.(%) 93.58 95.18 94.44  Dialogue Task Succ.F1(%) Prec.(%) 86.30 89.60 84.56 87.25 85.33 88.69 82.17 88.91 86.00 90.46 83.25 86.91 85.97 87.98  Rec.(%) 83.23 82.02 82.21 76.38 81.95 79.89 84.05  Table 4: Results of the multi-task learning model. This table is split into two parts: performance of resolution for the integrated GECOR on the left side and performance of dialogue task on the right side.  or sequence-based metrics (i.e., BLEU and EM). Therefore, we only integrate the GECOR model with the copy mechanism into the dialogue system. Incomplete vs. complete utterances In multiturn dialogues, user utterances may be incomplete or complete. A robust resolution model needs to be able to accurately identify whether the input utterance is complete or not. The model needs to keep it unchanged when it is complete and to predict the corresponding complete version when it is incomplete. For these cases, we tested our models and made statistical analysis on the three versions of data as shown in column 3, 4 and 5 of Table 3 (EM, EM 1, EM 2). We can find that the GECOR model beats the baseline model in all respects. However, the GECOR model needs further improvement when the input utterances are incomplete, compared with its good performance on complete utterances. Analysis on GECOR results for complete utterances We then analyzed the experimental results of the GECOR 1 on the mixed data in detail. When the input user utterances are complete, the GECOR model can amazingly generate 92.03% utterances that exactly match the input utterances. Only 7.97% do not match perfectly. Most unmatched cases, as we found, are with: (1) missed words (e.g., User: Can I get a Korean restaurant in the town centre? GECOR: Can I get a Korean restaurant in the town?) (2) Repetition (e.g., User: OK, thank you. That is all for today then. GECOR: OK, thank you. That is all for today for today then.) Analysis on GECOR results for incomplete utterances For incomplete input user utterances, GECOR can generate 42.04% exactly matched cases. Among the 57.96% cases that do not exactly match ground truth utterances, only 6.3% are not complete, which still contains unresolved el-  lipsis or co-reference, while 93.7% of these cases are complete with GECOR-generated words that do not match ground truth words. An in-depth analysis on these show that they can be clustered into 4 classes. (1) Paraphrases. We found that the majority of the unmatched complete utterances generated by GECOR are actually paraphrases to the ground truth complete utterances (e.g., User: Any will be fine. GECOR: Any food type will be fine. Reference: Any type of restaurant will be fine.). This is also confirmed by the high scores of the word-level evaluation metrics in Table 3. (2) Partial resolution. When a pronoun refers to more than one items, GECOR sometimes generate a partial resolution for the pronoun (e.g., User: I do not care about them. GECOR: I do not care about the price range. Reference: I do not care about the price range or location.). (3) Minor errors. In a few cases, the resolution part is correct while there are some errors elsewhere. (e.g., User: How about Chinese food? Prediction: How about international food on the south side of town? Reference: How about Chinese food on the south side of town?) (4) Repetition. Some cases contain repeatedly generated words. We think that although not exactly matched, paraphrased complete utterances generated by GECOR are acceptable. These utterances are helpful for the downstream dialogue task. For other errors, such as partial resolution or repetition, it may be necessary to enhance the attention or copy mechanism further in GECOR. 6.5 The Multi-Task Learning Model We further conducted experiments to extrinsically evaluate the GECOR model in task-oriented dialogue with the success F1 metric. This is also to evaluate our multi-task learning framework in integrating the GECOR model into the end-to-end dialogue model. In addition to training the base-  line TSCP model on the ellipsis, co-reference and mixed dataset, we also trained it on the dataset with only complete user utterances. This is to examine the ability of the baseline model in using correct contextual information presented in user utterances. The experimental results are shown in Table 4. Overall results In comparison to the baseline, we can see that our model improves the success F1 score by nearly 4 points on the co-reference dataset, which is close to the score obtained by the baseline trained with the complete user utterances. On the mixed and ellipsis dataset, our model also achieves 2.7 points and 0.8 points of success F1 score improvements, respectively. Resolution performance of the integrated GECOR We also provide the performance of the integrated GECOR on the resolution task in Table 4. The performance is slightly lower than when the GECOR is trained independently as a standalone system. This suggests that the GECOR is able to perform well when integrated into a dialogue system. The overall results demonstrate that the proposed multi-task learning framework for the end-to-end dialogue is able to improve the task completion rate by incorporating an auxiliary ellipsis/co-reference resolution task. Since the BSpan decoder is also used in the baseline system to capture contextual information and track dialogue states, we believe that our multi-task learning model with the integrated GECOR will play a more important role in end-toend dialgoue models that do not use state tracking modules, e.g., neural open-domain conversation models (Vinyals and Le, 2015; Li et al., 2016).  7 Conclusion and Future Work In this paper, we have extensively investigated the ellipsis and co-reference resolution in the context of multi-turn task-oriented dialogues. We have presented the GECOR, a unified end-to-end generative model for both ellipsis and co-reference resolution in multi-turn dialogues. A multi-task learning framework is further proposed to integrate the GECOR into the end-to-end task-oriented dialogue. In order to train and test the proposed model and framework, we manually created a new dataset with annotated ellipsis and co-reference information based on the publicly available CamRest676 dataset. Experiments on the resolution task show that the GECOR is able to significantly  improve the performance in terms of the exact match rate, BLEU and word-level F1 score. Experiments on the dialogue task demonstrate that the task completion rate of the task-oriented dialogue system is significantly improved with the aid of ellipsis and co-reference resolution. Our work could be extended to end-to-end open-domain multi-turn dialogue. We will further improve our model by incorporating syntactic and location information. We would also like to adapt the proposed methods to document-level neural machine translation in the future.  Acknowledgments The present research was supported by the National Natural Science Foundation of China (Grant No.61861130364) and the Royal Society (London) (NAF\R1\180122). We would like to thank the anonymous reviewers for their insightful comments.  References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015). Kevin Clark and Christopher D Manning. 2016. Deep reinforcement learning for mention-ranking coreferen"
"arXiv:1909.12104v1 [cs.AI] 26 Sep 2019  Action Selection for MDPs: Anytime AO* vs. UCT Blai Bonet  Hector Geffner  Universidad Simón Bolı́var Caracas, Venezuela bonet@ldc.usb.ve  ICREA & Universitat Pompeu Fabra 08003 Barcelona, SPAIN hector.geffner@upf.edu  Introduction One of the natural approaches for selecting actions in very large state spaces is by performing a limited amount of lookahead. In the contexts of discounted MDPs, Kearns, Mansour, and Ng have shown that near to optimal actions can be selected by considering a sampled lookahead tree that is sufficiently sparse, whose size depends on the discount factor and the suboptimality bound but not on the number of problem states (Kearns, Mansour, and Ng 1999). The UCT algorithm (Kocsis and Szepesvári 2006) is a version of this form of Monte Carlo planning, where the lookahead trees are not grown depth-first but ‘best-first’, following a selection criterion that balances ‘exploration’ and ‘exploitation’ borrowed from the UCB algorithm for multi-armed bandit problems (Auer, Cesa-Bianchi, and Fischer 2002). While UCT does not inherit the theoretical properties of either Sparse Sampling or UCB, UCT is an anytime optimal algorithm for discounted or finite horizon MDPs that eventually picks up the optimal actions when given sufficient time. The popularity of UCT follows from its success in the game Copyright c 2012, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  of Go where it outperformed all other approaches (Gelly and Silver 2007); a success that has been replicated in other models and tasks such as Real-Time Strategy Games (Balla and Fern 2009), General Game Playing (Finnsson and Björnsson 2008), and POMDPs (Silver and Veness 2010). An original motivation for the work reported in this paper was to get a better understanding of the success of UCT and related Monte-Carlo Tree Search (MCTS) methods (Chaslot et al. 2008).1 It has been argued that this success is the result of adaptive sampling methods: sampling methods that achieve a good exploration-exploitation tradeoff. Yet, adaptive sampling methods like Real-Time Dynamic Programming (RTDP) (Barto, Bradtke, and Singh 1995) have been used before in planning. For us, another important reason for the success of MCTS methods is that they address a slightly different problem; a problem that can be characterized as: 1. anytime action selection over MDPs (and related models) given a time window, resulting in good selection when the window is short, and near to optimal selection when window is sufficiently large, along with 2. non-exhaustive search combined with the ability to use informed base policies for improved performance. From this perspective, an algorithm like RTDP fails on two grounds: first, RTDP does not appear to make best use of short time windows in large state spaces; second, and more importantly, RTDP can use admissible heuristics but not informed base policies. On the other hand, algorithms like Policy Iteration (Howard 1971), deliver all of these features except one: they are exhaustive, and thus even to get started, they need vectors with the size of the state space. At the same time, while there are non-exhaustive versions of (asynchronous) Value Iteration such as RTDP, there are no similar ‘focused’ versions of Policy Iteration ensuring anytime optimality. Rollouts and nested rollouts are two practical and focused versions of Policy Iteration over large spaces (Bertsekas, Tsitsiklis, and Wu 1997; Yan et al. 2005), yet neither one aims at optimality.2 In this work, we introduce a new, simple heuristic search algorithm designed to address points 1 and 2 above, and 1  For a different attempt at understanding the success of UCT, see (Ramanujan, Sabharwal, and Selman 2010). 2 Nested rollouts can deliver optimal action selection but for impracticable large levels of nesting.  compare it with UCT. We call the new algorithm Anytime AO*, because it is a very simple variation of the classical AO* algorithm for AND/OR graphs (Nilsson 1980) that is optimal even in the presence of non-admissible heuristics. Anytime AO* is related to recent anytime and on-line heuristic search algorithms for OR graphs (Likhachev, Gordon, and Thrun 2003; Hansen and Zhou 2007; Koenig and Sun 2009; Thayer and Ruml 2010). It is well known that A* and other best-first algorithms can be easily converted into anytime optimal algorithms in the presence of nonadmissible heuristics by simply continuing the search after the first solution is found. The same trick, however, does not work for best-first algorithms over AND/OR graphs that must be able to expand leaf nodes of the explicit graph that are not part of the best partial solution. Thus, Anytime AO* differs from AO* in two main points: first, with probability p, it expands leaf nodes that are not part of the best partial solution; second, the search finishes when time is up or there are no more leaf nodes to expand at all (not just in the best solution graph). Anytime AO* delivers an optimal policy eventually and can also use random heuristics that are not admissible and can be sampled, as when the heuristics result from rollouts of a given base policy.  MDPs Markov Decision Processes are fully observable, stochastic state models. In the discounted reward formulation, an MDP is given by a set S of states, sets A(s) of actions applicable in each state s, transition probabilities Pa (s0 |s) of moving from s into s0 when the action a ∈ A(s) is applied, real rewards r(a, s) for doing action a in the state s, and a discount factor γ, 0 < γ < 1. A solution is a policy π selecting an action π(s) ∈ A(s) in each state s ∈ S. A policy is optimal if it maximizes the expected accumulated discounted reward. Undiscounted finite horizon MDPs replace the discount factor γ by a positive integer horizon H. The policies for such MDPs are functions mapping nodes (s, d) into actions a ∈ A(s), where s is a state and d is the horizon to go, 0 < d ≤ H. Optimal policies maximize the total reward that can be accumulated in H steps. Finite horizon MDPs are acyclic as all actions decrement the horizon to go by 1. Undiscounted infinite horizon MDPs are like discounted MDPs but with discount γ = 1. For such MDPs to have well-defined solutions, it is common to assume that rewards are negative, and thus represent positive costs, except in certain goal states that are cost-free and absorbing. If these goal states are reachable from all the other states with positive probability, the set of optimal policies is well defined. The MDPs above are reward-based. AO* is used normally in a cost setting, where rewards r(a, s) are replaced by costs c(a, s), and maximization is replaced by minimization. The two points of view are equivalent, and we will use one or the other when most convenient. We are interested in the problem of selecting the action to do in the current state of a given infinite horizon MDPs, whether discounted or undiscounted. This will be achieved by a lookahead that uses a limited time window to run an anytime optimal algorithm over the version of the MDP that results from fixing the horizon.  AO* A finite horizon MDP defines an implicit AND/OR graph that can be solved by the well-known AO* algorithm (Nilsson 1980). The root node of this graph is the pair (s0 , H) where s0 is the initial state and H is the horizon, while the terminal nodes are of the form (s, d) where s is a state and d is 0, or s is a terminal state (goal or dead-end). The children of a non-terminal node (s, d) are the triplets (a, s, d) where a is an action in A(s), while the children of a node (a, s, d) are the nodes (s0 , d−1) where s0 is a possible successor state of a in s; i.e., Pa (s0 |s) > 0. Non-terminal nodes of the form (s, d) are OR-nodes where an action needs to be selected, while nodes (a, s, d) are non-terminal AND-nodes. The solutions graphs to these AND/OR graphs are defined in the standard way; they include the root node (s0 , H), and recursively, one child of every OR-node and all children of every AND-node. The value of a solution is defined recursively: the leaves have value equal to 0, the OR-nodes (s, d) have value equal to the value of the selected child, and the AND-nodes (a, s, d) have value equal to the cost of the action a in s plus the sum of the values for the children (s0 , d−1) weighted by their probabilities Pa (s0 |s). The optimal solution graph yields a minimum value to the root node (cost setting), and can be computed as part of the evaluation procedure, marking the best child (min cost child) for each OR-node. This procedure is called backward induction. The algorithm AO* computes an optimal solution graph in a more selective and incremental manner, using a heuristic function h over the nodes of the graph that is admissible or optimistic (does not overestimate in the cost setting). AO* maintains a graph G that explicates part of the implicit AND/OR graph, the explicit graph, and a second graph G∗ , the best partial solution, that represents an optimal solution of G under the assumption that the tips n of G are the terminal nodes with values given by the heuristic h(n). Initially, G contains the root node of the implicit graph only, and G∗ is G. Then, iteratively, a non-terminal leaf node is selected from the best partial solution G∗ , and the children of this node in the implicit graph are explicated in G. The best partial solution of G is then revised by applying an incremental form of backward induction, setting the values of the leaves in G to their heuristic values. The procedure finishes when there are no leaf nodes in the best partial graph G∗ . If the heuristic values are optimistic, the best partial solution G∗ is an optimal solution to the implicit AND/OR graph, which is partially explicated in G. In the best case, G ends up containing no more nodes than those in the best solution graph; in the worst case, G ends up explicating all the nodes in the implicit graph. Code for AO* is shown in Fig. 1. The choice of which non-terminal leaf node in the best partial graph to expand is important for performance but is not part of the algorithm and it does not affect its optimality.  UCT UCT has some of the flavour of AO* and it is often presented as a best-first search algorithm too.3 Indeed, UCT 3 This is technically wrong though, as a best-first algorithm just expands nodes in the best partial solution in correspondence to  AO*: G is explicit graph, initially empty; h is heuristic function. Initialization 1. Insert node (s, H) in G where s is the initial state 2. Initialize V (s, H) := h(s, H) 3. Initialize best partial graph to G Loop 4. Select non-terminal tip node (s, d) in best partial graph. If there is no such node, Exit. 5. Expand node (s, d): for each a ∈ A(s), add node (a, s, d) as child of (s, d), and for each s0 with Pa (s0 |s) > 0, add node (s0 , d − 1) as child of (a, s, d). Initialize values V (s0 , d − 1) for new nodes (s0 , d − 1) as 0 if d − 1 = 0, or h(s0 , d − 1) otherwise. 6. Update ancestors AND and OR nodes of (s, d) in G, bottom-up as: P Q(a, s, d) := c(a, s) + γ s0 Pa (s0 |s)V (s0 , d − 1), V (s, d) := mina∈A(s) Q(a, s, d). 7. Mark best action in ancestor OR-nodes (s, d) to any action a such that V (s, d) = Q(a, s, d), maintaining marked action if still best. 8. Recompute best partial graph by following marked actions.  Figure 1: AO* for Finite Horizon Cost-based MDPs. maintains an explicit partial graph that is expanded incrementally like the graph G in AO*. The main differences are in how leaf nodes of this graph are selected and expanded, and how heuristic values for leafs are obtained and propagated. The code for UCT, which is more naturally described as a recursive algorithm, is shown in Fig. 2. UCT consists of a sequence of stochastic simulations, like RTDP, that start at the root node. However, while the choice of successor states is stochastic, the choice of the actions is not greedy on the action Q-values as in RTDP, but greedy p on the sum of the action Q-values and a bonus term C 2 log N (s, d)/N (a, s, d) that ensures that all applicable actions are tried in all states infinitely often at suitable rates. Here, C is an exploration constant, and N (s, d) and N (a, s, d) are counters that track the number of simulations that had passed through the node (s, d) and the number of times that action a has been selected at such node. The counters N (a, d) and N (a, s, d) are maintained for the nodes in the explicit graph that is extended incrementally, starting like in AO*, with the single root node (s, H). The simulations start at the root and terminate at a terminal node or at the first node (s, d) that is not in the graph. In between, UCT selects an action a that is greedy on the stored value Q(a, s, d) plus the bonus term, samples the next state s0 with probability Pa (s0 |s), increments the counters N (s, d) and N (a, s, d), and generates the node (s0 , d − 1). When a node (s, d) is generated that is not in the explicit graph, the node is added to the explicit graph, the registers N (s, d), N (a, s, d), and Q(a, s, d) are allocated and initialized to 0, and a total discounted reward r(π, s, d) is sampled, by simulating a base policy π for d steps starting at s, and propagated upward along the nodes in the simulated path. These values are not propagated using full Bellman backwhat is called ‘exploitation’. Yet UCT as the other MCTS algorithms, and Anytime AO* below, do ‘exploration’ as well.  UCT(s, d): G is explicit graph, initially empty; π is base policy; C is exploration constant. 1. If d = 0 or s is terminal, Return 0 2. If node (s, d) is not in explicit graph G, then Add node (s, d) to explicit graph G Initialize N (s, d) := 0 and N (a, s, d) := 0 for all a ∈ A(s) Initialize Q(a, s, d) := 0 for all a ∈ A(s) Obtain sampled accumulated discounted reward r(π, s, d) by simulating base policy π for d steps starting at s – Return r(π, s, d)  – – – –  3. If node (s, d) is in explicit graph G, p – Let Bonus(a) = C 2 log N (s, d)/N (a, s, d) if N (a, s, d) > 0, else ∞, for each a ∈ A(s) – Select action a = argmaxa∈A(s) [Q(a, s, d) + Bonus(a)] – Sample state s0 with probability Pa (s0 |s) – Let nv = r(s, a) + γUCT(s0 , d − 1) – Increment N (s, d) and N (a, s, d) – Set Q(a, s, d) := Q(a, s, d) + [nv − Q(a, s, d)]/N (a, s, d) – Return nv  Figure 2: UCT for Finite-Horizon Reward-based MDPs. ups as in AO* or Value Iteration, but through Monte-Carlo backups that extend the current average with a new sampled value (Sutton and Barto 1998); see Fig. 2 for details. It can be shown that UCT eventually explicates the whole finite horizon MDP graph, and that it converges to the optimal policy asymptotically. Unlike AO*, however, UCT does not have a termination condition, and moreover, UCT does not necessarily add a new node to the graph in every iteration, even when there are such nodes to explicate. Indeed, while in the worst case, AO* converges in a number of iterations that is bounded by the number of nodes in the implicit graph, UCT may require an exponential number of iterations (Munos and Coquelin 2007; Walsh, Goschin, and Littman 2010). On the other hand, AO* is a model-based approach to planning that assumes that the probabilities and costs are known, while UCT is a simulation-based approach that just requires a simulator that can be reset to the current state. The differences between UCT and AO* are in the leafs of the graphs selected for expansion, the way they are expanded, the values to which they are initialized, and how the values are propagated. These dimensions are the ones used to define a family of Monte Carlo Tree Search (MCTS) methods that includes UCT as the best known member (Chaslot et al. 2008). The feature that is common to this family of methods is the use of Monte Carlo simulations to evaluate the leafs of the graph. The resulting values are heuristic, although not necessarily optimistic as required for the optimality of AO*. One way to bridge the gap between AO* and MCTS methods is by modifying AO* to accommodate nonadmissible heuristics, and moreover, random heuristics that can be sampled such as the cost of a base policy.  Anytime AO* Anytime AO* involves two small changes from AO*. The first, shown in Fig. 3, is for handling non-admissible heuris-  Anytime AO* with possibly non-admissible heuristic h: same code as AO* except for extra parameter p, 0 ≤ p ≤ 1, and line 4 in Loop for tip selection replaced by 4.1 and 4.2 below. 4.1. Choose IN with probability 1 − p, else Choose OUT: • IN: Select non-terminal tip node (s, d) from explicit graph G that is IN the best partial graph. • OUT: Select non-terminal tip node (s, d) from explicit graph G that is NOT in the best partial graph. 4.2. If there is no node (s, d) satisfying the choice condition, make the other choice. If there is no node satisfying either choice condition, Exit the loop.  Figure 3: Anytime AO*. If the heuristic h(s, d) is random, as when representing the cost of a given base policy; see text. tics: rather than always selecting a non-terminal tip node from the explicit graph that is IN the best partial graph, Anytime AO* selects with probability p a non-terminal tip node from the explicit graph that is OUT of the best partial graph. The probability p is a given parameter between 0 and 1, by default 1/2. Of course, if a choice for an IN node is decided (a tip in the best partial graph) but there is no such node, then an OUT choice is forced, and vice versa. Anytime AO* terminates when neither IN or OUT choices are possible; i.e., when no tip nodes in the explicit graph are left, or when the time is up. It is easy to see that Anytime AO* is optimal, whether the heuristic is admissible or not, because it terminates when the implicit graph has been fully explicated. In the worst case, the complexity of Anytime AO* is not worse than AO*, as AO* expands the complete graph in the worst case too. The second change in Anytime AO*, not shown in the figure, is for dealing with random heuristics h. Basically, when the value V (s, d) of a tip node (s, d) is set to a heuristic h(s, d) that is a random variable, such as the reward obtained by following a base policy d steps from s, Anytime AO* uses samples of h(s, d) until the node (s, d) is expanded. Until then, a ‘fetch for value’ V (s, d), which occurs each time that a parent node of (s, d) is updated, results in a new sample of h(s, d) which is averaged with the previous ones. This is implemented in standard fashion by incrementally updating the value V (s, d) using a counter N (s, d) and the new sample. These counters are no longer needed when the node (s, d) is expanded, as then the value of the node is given by the value of its children in the graph.  Choice of Tip Nodes in Anytime AO* AO* and Anytime AO* leave open the criterion for selecting the tip node to expand. For the experiments, we use a selection criterion aimed at selecting the tip nodes that can have the biggest potential impact in the best partial graph. For this, a function ∆(n) is introduced that measures the change in the value of the node n that is needed in order to produce a change in the best partial graph. The function ∆ is defined top-down over the explicit graph as: 1. For the root node, ∆(s, H) = ∞. 2. For children na = (a, s, d) of node (s, d) in the best solution graph, ∆(na ) is [V (n)−Q(na )] if a is not the best  action in n, else is minb [∆(n), Q(nb )−V (n)], for b 6= a. 3. For children na = (a, s, d) of node (s, d) that is not in the best solution graph, ∆(na ) is ∆(n) + V (n) − Q(na ). 4. For children ns0 = (s0 , d − 1) of node na = (a, s, d), ∆(ns0 ) is ∆(na )/γPa (s0 |s). The tip nodes (s, d) that are chosen for expansion are the ones that minimize the values |∆(s, d)|. Since this computation is expensive, as it involves a complete traversal of the explicit graph G, we select N ≥ 1 tip nodes for expansion at a time. This selection is implemented by using two priority queues during the graph traversal: one for selecting the best N tips in the solution graph (IN), and one for selecting the best N tips OUT. The first selected tip is the one with min |∆(s, d)| in the IN queue with probability 1 − p and in the OUT queue otherwise. Once selected, the tip is removed from the queue, and the process is repeated N times.  Experimental Results We have evaluated Anytime AO* (abbreviated AOT) vs. UCT as an action selection mechanism over a number of MDPs. In each case, we run the planning algorithm for a number of iterations from the current state s, apply the best action according to the resulting Q(a, s, H) values, where H is the planning horizon, and repeat the loop from the state that results until the goal is reached. The quality profile of the algorithms is given by the average cost to the goal as a function of the time window for action selection. Each data point in the profiles (and table) is the average over 1,000 sampled episodes that finish when the goal is reached or after 100 steps. For the profiles, the x-axis stands for the average time per action selected, and the y-axis for the average cost to the goal. AOT was run with parameter p = 0.5 (i.e., equal probability for IN and OUT choices), and UCT with the exploration ‘constant’ C set to current Q(a, s, d) value of the node; a choice that appears to be standard (Balla and Fern 2009; Eyerich, Keller, and Helmert 2010). The N parameter for the computation of the ∆’s was set as a fraction k of the number of iterations, k = 1/10.4 The actual codes for UCT and AOT differ from the ones shown in that they deal with graphs instead of trees; i.e., there are duplicates (s, d) with the same state s and depth d. The same applies to the computation of the ∆’s. All experiments were run on Xeon ’Woodcrest’ computers of 2.33 GHz and 8 Gb of RAM. CTP. The Canadian Traveller Problem is a path finding problem over a graph whose edges (roads) may be blocked (Papadimitriou and Yannakakis 1991). Each edge is blocked with a given prior probability, and the status of an edge (blocked or free) can be sensed noise-free from either end of the edge. The problem is an acyclic POMDP that results in a belief MDP whose states are given by the agent position, and edge beliefs that can take 3 values: prior, known to be blocked, and known to be free. If the number of nodes 4 The number of tip nodes in UCT is bounded by the number of iterations (rollouts). In AOT, the number of tips is bounded in the worst case by the number of iterations (expansions) multiplied by |A||S|, where |A| and |S| are the number of actions and states.  br. factor prob. P (bad)  avg max  UCT–CTP  random base policy  optimistic base policy  UCTB  UCTO  direct  UCT  AOT  direct  UCT  AOT  10-1 10-2 10-3 10-4 10-5 10-6 10-7 10-8 10-9 10-10 total  19.9 45.6 21.9 1.4 22.7 24.9 4.1 14.1 28.1 31.1  8.5 6.5 8.9 11.4 7.2 7.1 11.0 8.0 12.7 9.4  32 64 16 32 16 32 32 32 32 32  114.8 ± 3 102.5 ± 2 127.2 ± 3 58.0 ± 2 89.3 ± 2 111.5 ± 3 83.3 ± 2 87.0 ± 1 72.0 ± 3 76.7 ± 2 922.3  101.3 ± 3 99.9 ± 2 115.2 ± 3 53.4 ± 2 86.1 ± 2 92.6 ± 2 66.7 ± 1 74.6 ± 1 66.5 ± 2 75.4 ± 2 831.7  324.7 ± 5 254.8 ± 4 313.2 ± 4 276.4 ± 5 224.4 ± 3 225.4 ± 3 244.8 ± 4 230.9 ± 3 177.6 ± 4 200.0 ± 4 2472.2  108.4 ± 1 101.1 ± 1 127.1 ± 1 55.9 ± 1 91.3 ± 1 121.4 ± 1 83.9 ± 1 107.1 ± 1 71.5 ± 1 79.5 ± 1 947.2  103.1 ± 1 98.3 ± 1 125.3 ± 1 54.4 ± 1 90.0 ± 1 102.9 ± 1 69.9 ± 1 78.3 ± 0 66.7 ± 1 74.5 ± 1 863.4  102.8 ± 1 145.9 ± 2 125.0 ± 1 53.0 ± 1 86.7 ± 1 105.0 ± 1 118.4 ± 2 75.0 ± 0 63.7 ± 1 76.9 ± 1 952.4  106.1 ± 1 97.9 ± 1 118.7 ± 1 53.6 ± 1 89.3 ± 1 105.4 ± 1 74.5 ± 0 81.8 ± 0 68.1 ± 1 75.2 ± 1 870.6  102.2 ± 1 99.1 ± 1 116.5 ± 1 52.5 ± 1 94.4 ± 1 96.0 ± 1 69.0 ± 0 76.1 ± 0 66.7 ± 1 82.7 ± 1 855.2  20-1 20-2 20-3 20-4 20-5 20-6 20-7 20-8 20-9 20-10 total  17.9 9.5 14.3 78.6 20.4 14.4 8.4 23.3 33.0 12.1  13.5 15.7 15.2 11.4 15.0 13.9 14.3 15.0 14.6 15.3  128 64 128 64 64 64 128 64 128 64  210.7 ± 7 176.4 ± 4 150.7 ± 7 264.8 ± 9 123.2 ± 7 165.4 ± 6 191.6 ± 6 160.1 ± 7 235.2 ± 6 180.8 ± 7 1858.9  169.0 ± 6 148.9 ± 3 132.5 ± 6 235.2 ± 7 111.3 ± 5 133.1 ± 3 148.2 ± 4 134.5 ± 5 173.9 ± 4 167.0 ± 5 1553.6  1000.3 ± 1 676.6 ± 1 571.7 ± 1 861.4 ± 1 586.2 ± 1 670.7 ± 1 862.9 ± 1 704.0 ± 1 783.4 ± 1 825.7 ± 1 7542.9  216.4 ± 3 178.5 ± 2 169.7 ± 4 264.1 ± 4 139.8 ± 4 178.0 ± 3 211.8 ± 3 218.5 ± 4 251.9 ± 3 185.7 ± 3 2014.4  187.4 ± 3 167.4 ± 2 140.7 ± 3 261.0 ± 4 128.3 ± 3 160.0 ± 2 170.2 ± 2 154.6 ± 3 213.7 ± 2 180.5 ± 3 1763.8  191.8 ± 0 202.7 ± 0 142.1 ± 0 267.9 ± 0 163.1 ± 0 193.5 ± 0 171.3 ± 0 167.9 ± 0 212.8 ± 0 173.2 ± 0 1886.3  180.7 ± 3 160.8 ± 2 144.3 ± 3 238.3 ± 3 123.9 ± 3 167.8 ± 2 174.1 ± 2 152.3 ± 3 185.2 ± 2 178.5 ± 3 1705.9  163.8 ± 2 156.4 ± 1 133.8 ± 2 233.4 ± 3 109.4 ± 2 135.5 ± 1 145.1 ± 1 135.9 ± 2 173.3 ± 1 166.4 ± 2 1553.0  Table 1: CTP instances with 10 and 20 nodes. P (bad) is probability (in percentage) of the instance not being solvable, and max branching  factor is 2m where m + 1 is the number of edges with a common node in CTP graph. UCTB and UCTO are two domain-dependent UCT implementations (Eyerich, Keller, and Helmert 2010), π refers to the base policy in our domain-independent UCT and AOT algorithms, whose base performance is shown as well. UCT run for 10,000 iterations and AOT for 1,000. Boldface figures are best in whole table; gray cells show best among domain-independent implementations.  is n and the number of edges in the graph is m, the number of MDP states is n × 3m . The problem has been addressed recently using a domain-specific implementation of UCT (Eyerich, Keller, and Helmert 2010). We consider instances with 20 nodes from that paper and instances with 10 nodes obtained from the authors. Following Eyerich, Keller and Helmert, the actions are identified with moves to any node in the frontier of the known graph. The horizon H can then be set to the number of nodes in the graph. Quality profiles are shown in Fig. 4 for instances 10-7 and 20-7. On each panel, results on the left are for UCT and AOT using the base random policy, while results on the right are for the base optimistic policy that assumes that all edges of unknown status are traversable. The points for UCT correspond to running 10, 50, 100, 500, 1k, 5k, 10k and 50k iterations (rollouts), resulting in the times shown. The points for AOT, correspond to running 10, 50, 100, 500, 1k, 5k, 10k iterations (expansions). Points that time out are not displayed. The curves show that AOT performs better than UCT except up to the time window of 1 second in the instance 20-7 for the random base policy. We have computed the curves for the 20 instances and the pattern is similar (but not shown for lack of space). A comprehensive view of the results is shown in Table 1. As a reference, we include also the results for two specialized implementations of UCT, UCTB and UCTO (Eyerich, Keller, and Helmert 2010), that take advantage of the specific MDP structure of the CTP, use a more informed base policy, and actually solve a slightly simpler version of the problem where the given CTP is solvable. While all the algorithms are evaluated over solvable instances, AOT and  our UCT solve the harder problem of getting to the target location if the problem is solvable, and determining otherwise that the problem is unsolvable. In some instances, the edge priors are such that the probability of an instance not being solvable is high. This is shown as the probability of ‘bad weather’ in the second column of the table. In spite of these differences, AOT improves upon the domain-specific implementation of UCT in several cases, and practically dominates the domain-independent version. Since UCTO is the state of the art in CTP, the results show that our domainindependent implementations of both AOT and UCT are good enough, and that AOT in particular appears to be competitive with the state of the art in this domain. Sailing and Racetrack. The Sailing domain is similar to the one in the original UCT paper (Kocsis and Szepesvári 2006). The profile for a 100 × 100 instance with 80,000 states is shown in the left panel of Fig. 5 for a random base policy. The problem has a discount γ = 0.95 and the optimal value is 26.08. UCT and AOT are run with horizon H = 50. Profiles for other instances show the same pattern: AOT is slower to get started because of the more expensive expansions, but then learns faster. The profile for the instance ’barto-big’ of the Racetrack domain (Barto, Bradtke, and Singh 1995) is shown in the right panel of Fig. 5. In this case, AOT converges much faster than UCT. Base Policies, Heuristics, and RTDP. We also performed experiments comparing the use of heuristics h vs. base policies π for initializing the value of tip nodes in AOT. We refer to AOT using π as a base policy as AOT(π), and to AOT us-  0.1  1  10  100  1e3  1e4  ● ●  ●  1  10  100  1e3  1e4  0.1  avg. time per decision (milliseconds)  400 300  400 300  ●  100 0.1  avg. time per decision (milliseconds)  ●  200  ● ●  ●  UCT AOT  ●  avg. accumulated cost  ● ●  ●  20−7 with optimistic base policy  ●  ● ● ● ●  ● ●  ●  100  ●  ●  200  ●  UCT AOT  ●  avg. accumulated cost  100 110 120  ●  20−7 with random base policy  60  70  80  ● ●  90  ● ●  80  90  ●  UCT AOT  ●  70  ●  10−7 with optimistic base policy  avg. accumulated cost  100 110 120  UCT AOT  ●  ●  60  avg. accumulated cost  10−7 with random base policy  1  10  100  1e3  1e4  1e5  0.1  avg. time per decision (milliseconds)  (a) 10-7  1  10  100  1e3  1e4  1e5  avg. time per decision (milliseconds)  (b) 20-7  Figure 4: Quality profiles for CTP instances 10-7 and 20-7.  0.1  1e3  avg. time per decision (milliseconds)  avg. accumulated cost  ing the heuristic h as AOT(h). Clearly, the overhead per iteration is smaller in AOT(h) than in AOT(π) that must do a full rollout of π to evaluate a tip node. Results of AOT(π) vs. AOT(h) on the instances 20-1 and 20-4 of CTP are shown in Fig. 6 with both the zero heuristic and the min-min heuristic (Bonet and Geffner 2003). The base policy π is the policy πh that is greedy with respect to h with πh being the random policy when h = 0. The curves also show the performance of LRTDP (Bonet and Geffner 2003), used as an anytime action selection mechanism that is run over the same finite-horizon MDP as AOT and with the same heuristic h. Interestingly, when LRTDP is used in this form, as opposed to an off-line planning algorithm, LRTDP does rather well on CTP too, where there is no clear dominance between AOT(πh ), AOT(h), and LRTDP(h). The curves for the instance 20-1 are rather typical of the CTP instances. For the zero heuristic, AOT(h) does better than LRTDP(h) which does better than AOT(πh ). On the other hand, for the minmin heuristic, the ranking is reversed but with differences in performance being smaller. There are some exceptions to this pattern where AOT(πh ) does better, and even much better than both AOT(h) and LRTDP(h) for the two heuristics. One such instance, 20-4, is shown in the bottom part of Fig. 6. Figure 7 shows the same comparison for a Racetrack instance and three variations hd of the min-min heuristic hmin , hd = d × hmin for d = 2, 1, 1/2. Notice that multiplying an heuristic h by a constant d has no effect on the policy πh that is greedy with respect to h, and hence no effect on AOT(πhd ). On the other hand, these changes affect both AOT(hd ) and LRTDP(hd ). As expected the per-  10  100  1e3  1e4  avg. accumulated cost  150 200 250 300 350 400 450  ● ●  1e5  ●  0.1  1  10  ● ●  100  ● ●  1e3  ● ●  1e4  1e5  avg. time per decision (milliseconds)  avg. time per decision (milliseconds)  20−4 with zero heuristic  20−4 with min−min heuristic  450  Figure 5: Quality profiles for Sailing and Racetrack.  1  ●  AOT(pi_h) AOT(h) LRTDP(h)  ●  ●  ● ●  ● ● ● ●  0.1  1  10  100  1e3  1e4  1e5  avg. time per decision (milliseconds)  450  100  ●  AOT(pi_h) AOT(h) LRTDP(h)  ●  400  10  ●  350  avg. time per decision (milliseconds)  1  ●  AOT(pi_h) AOT(h) LRTDP(h)  ●  300  1e3  UCT AOT  ●  avg. accumulated cost  100  avg. accumulated cost  ●  ●  150 200 250 300 350 400 450  100 60  80  ●  20−1 with min−min heuristic  AOT(pi_h) AOT(h) LRTDP(h)  ●  250  10  ●  400  1  ●  350  ●  ●  300  ●  ●  ●  20  ●  ●  250  ●  30  35  ●  20−1 with zero heuristic  barto−big with random base policy  40  UCT AOT  40  ●  avg. accumulated cost  ●  25  avg. accumulated cost  45  100x100 with random base policy  ● ●  0.1  1  10  100  ●  ● ●  1e3  ● ●  1e4  1e5  avg. time per decision (milliseconds)  Figure 6: AOT(πh ), AOT(h), and LRTDP(h) on two CTP instances for h = 0 and hmin . Policy πh is greedy in h.  formance of LRTDP(hd ) deteriorates for d < 1, but some"
"ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL 1  Two-stage Image Classification Supervised by a Single Teacher Single Student Model †  Jianhang Zhou , Shaoning Zeng mb85405@um.edu.mo, zsn@outlook.com Bob Zhang* bobzhang@um.edu.mo  †  PAMI Research Group Department of Computer and Information Science University of Macau († Both authors contributed equally  Introduction Image classification is one of the most crucial techniques in computer vision. While one-step classification might not be credibly adequate, two-stage image classification has been successful in many tasks, i.e., face recognition [1-3] and object recognition [4]. Realworld recognition tasks often contain a lot of complicated data and conditions. For this reason, the discriminative ability of one single classifier is likely to fail in picking the best result. Thus, it is reasonable to use two-stage classification to perform coarse and fine classification. Since the final result is determined by a subset of the classes, the complexity of the distribution in data is reduced [4]. In addition, according to the probability estimation, it is more effortless to choose multiple candidate classes containing the right class than to find out if one single class is the right class. For instance, the Top-1 accuracy of ImageNet is hardly equal to the Top-5 accuracy in any condition. Therefore, two-stage classification, in various implementations, is more promising than others. Two-stage methods have been popular for a long time [5-7]. For example, Xu et al. proposed a two-phase test sample representation (TPTSR) method [5], which used all training samples to represent a test sample in order to exploit its nearest neighbours in the first stage, before organizing these nearest neighbours to represent the test sample. The  2 ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL WSRC (weighted sparse representation for classification) [2] exploited weights of the representation to seek a sparser representation, which is a form of the sparse representation method implemented as a two-stage method. The RAMUSA [8] algorithm performed multi-task learning by using a multi-stage method, which is similar to the idea of two-stage methods. In addition, the idea of two-stage classification is helpful when implemented to other problems, such as coarse-to-fine frameworks [9-10], and face recognition [11]. From the above discussion, we believe that all of these two stage classification methods only paid attention to the two classifiers, without considering the relationship of the classification criteria, or scores, between the two stages. In contrast to this, a Teacher-Student model has a much clearer role of definitions for the two classifiers. Recently, You et al. proposed g-SVM for solving the single-teacher multi-students problem [12]. The multi-teacher single-student problem was solved by a multi-teacher networks model [13]. Zheng et al. combined GAN with the teacher-student problem and achieved effective results [14]. In our opinion, the teacher-student model is a special case of the two-stage classification. However, the teacher classifier cannot reduce the computation load of students, which is different from the first classifier in conventional two-stage methods. What is more, no such findings are available to solve the single-teacher single-student problem by using a score-based prediction mechanism. Both the Two-Stage and Teacher-Student classification methods have various implementations. Among them, linear methods show promising performances, i.e., Sparse Representation (SR) and SVM. SVM proposed in 1995 [15] is a powerful classifier in different classification tasks [16-17]. Currently, there are several variants and applications. The sparse representation classifier (SRC) [18] shows effective performances and robustness in image classification as well by taking advantage of the role of sparsity. Another powerful representation method is the collaborative representation classifier (CRC) [19]. By putting emphasis on collaborative representation, CRC improves the efficiency of SRC. There are fusion works of SRC and CRC [20-21], trying to keep a balance between sparsity and collaborative representation. Nevertheless, no such work can fuse them in a two-stage classification framework. In this paper, we propose a novel framework for image classification using a two-stage representation method and formulate the two-stage classification problem to a singleteacher single-student problem. We name it Two-Stage image classification supervised by a Single Teacher Single Student model (TS-STSS) and utilize sparse representation to implement the algorithm. In the first stage of classification, the teacher classifier makes classification and seeks the nearest classes to the test sample, which is denoted as ‘candidate classes’. Then, a ‘candidate set’ containing the training samples can be organized according to ‘candidate classes’. In the second stage, we represent the test sample and perform classification using the candidate set. Next, we use the single-teacher single-student model to make a decision based on scores generated from the teacher and student classifier. Generally speaking, our proposed two-stage representation method is supervised by a teacher classifier in image classification. The contributions of our work can be summarized in four aspects as follows: 1) We propose a novel two-stage representation method for image classification. 2) We formulate the decision-making problem between results of both stages to a single-teacher single-student problem, and solve it using a score-based mechanism. 3) We implement TS-STSS via L1-minimization (sparse representation) and L2minimization (collaborative representation).  ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL 3 The remainder of this paper will be organized as follows. In Section 2, we first describe the two-stage test representation method and the single-teacher single-student strategy, before proposing our classification framework. In Section 3, experimental and comparison results on image datasets will be demonstrated to show the effectiveness and performance of our proposed method. Section 4 concludes this paper.  2. The Method Our proposed method TS-STSS is a novel Two-Stage classification supervised by the Single-Teacher Single-Student model, where the implementation is based on sparse supervised representation [22]. In the first stage, a sparse representation-based classifier via L1-minimization is learned as the teacher classifier, which computes the distances (or scores) to select a set of candidate classes. Then, in the second stage, one single student classifier based on the faster L2-minization, is trained using the samples of all candidate classes. Meanwhile, the scores of each class are generated. With the supervision via scoring of the teacher classifier, the student classifier in the second stage is capable of generating the final result. The detailed process of TS-STSS is depicted in the following sub-sections.  2.1 Two-Stage (TS) Representation The representation procedure is performed in two stages. Specifically, in the first stage, all training samples are used to represent the test sample in a linear combination: y  1 x1   2 x2  ...   m xm (1) where y is the test sample, and the  i is the coefficient of the i th instance in the linear combination, xi   s1 is the column vector of the i th instance, and m is the number of instances in training set. For each class, we calculate its deviation with test sample by: m  V j  y   j ,i x j ,i  2  i 1  (2)  where V j denotes the deviation of the j th class,  j ,i and x j ,i are the i th coefficient and i th sample of the j th class. According to equation (2), we pick N nearest neighbors and append their corresponding class label to the candidate classes set C . We denote a sample set gathering samples from C as ‘candidate set’ G . In the second stage, each test sample will be represented by samples in G using a linear combination: y  ˆ1 x1  ˆ2 x2  ...  ˆn xn (3) Where n denotes number of instances in G , and the ˆ is the coefficient of i th instance in i  the linear combination.  2.2 Single-Teacher Single-Student (STSS) Model To take results from the first stage and second stage into consideration when classifying, we design a single-teacher single-student model, and solve it using a score-based mechanism. We define the classifier in the first stage as the teacher classifier, and classifier in the second stage as the student classifier. Then, we calculate the ‘gate value’ of the  4 ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL teacher and student classifiers respectively in order to make a comparison to decide the final result. In our strategy, we take the highest value of the teacher classifier and student classifier as the ‘gate value’. We denote this solution as a single-teacher single-student model (STSS). Firstly, we utilize a strong multi-class classifier T as a teacher classifier. Then, we apply a faster classifier as the student classifier ST . Next, we use the teacher classifier T to perform multi-class classification and obtain a score vector S K 1 for each class: Sj   X j (4)     where S j is the j th instance of a score vector, X j denotes the training set of the j th class, and    is a score evaluation function to evaluate the score of the sample vector. Classes with the highest score will be selected as the classification result by the teacher classifier, and its corresponding score will be taken as the gate value g : g  max  S   (5)  The gate value g can also be regarded as the confidence of teacher ( T ). When the classification results of the student and the teacher are different, the final decision should be made between them. In this scenario, if the student ( ST ’s) learned highest score is higher than the gate value g , the ST ’s classification result will determine the final result. Otherwise, the classification result of T will be taken as the final result. The final result z is determined as follows:  L  SST  , if SST  ST  (6) z   L  ST  , otherwise where L   denotes the function mapping of a score to its corresponding class label, and S ST , ST denote highest scores learned by a student ST and a teacher T , respectively.  Figure 1: Two-stage image classification supervised by a teacher classifier.  ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL 5 It is obvious that the single-teacher single-student strategy is a two-fold learning strategy, which motivates us to combine it with a two-stage representation method, as both of the two learning strategies require two steps to perform classification.  2.3 Supervision of Teacher Classifier Based on the two-stage representation method (TSR) and the single-teacher single-student model (STSS), we propose our two-stage image classification framework, named twostage image classification supervised by a single-teacher single-student model (TS-STSS) for image classification. Figure 1 depicts the general idea of our method intuitively. First of all, in the initial stage, we apply the sparse representation classifier (SRC) [23] as teacher classifier using all training samples for representation: (7) ˆ  arg min  1 s.t. y  X  2     where  is the coefficient vector in linear combination, and  is the noise in y . The candidate class set C  C1 , C2 ,..., Cm  is built by appending the class associated      with the M lowest deviations, and candidate set G  X C1 , X C2 , X C3 ,... X Cm . Next, the gate value can be described as follows: 1 n g j   y  i X i n i 1  2   yjX j  2  (8)  g*  max  g   where i  0,...,0, i ,1 , i ,2 ,..., i , k ,0,...,0 , X i denotes training set of the i th class, and n represents the number of all classes. Following this, in the second stage, we apply the collaborative representation classifier (CRC) whose classification speed is higher than SRC [19] as the student classifier using samples from the candidate set for representation: (9) ˆ  arg min y  X  s.t.  q   2    where  is the coefficient vector in linear combination,  is the noise in y , and q can be    1 or 0. The solution of equation (9) is ˆ = Xˆ  X +  I    -1  XT .  The score learned by the student can be described as follows: 1 k sj   y i Xi  y  j X j 2 k i 1     2  (10)  s*  max s j  where X i is the i th instance in G , and k is the number of instances in G . If the classification results of the student and teacher are different from each other, we compare the score of the class identified by the student and teacher respectively before utilizing the decision-making method discussed in section 3.2:  6 ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL        L s* , if s*  g *  zˆ   (11) *  L g , otherwise where L  denotes the function mapping score s and the gate value g to its corresponding class label. We summarize our proposed TS-STSS framework in Algorithm 1: Algorithm 1 TS-STSS classification framework Input: Training set X , test sample y Output: identity I 1: In the first stage, use SRC as the teacher classifier according to (7) to obtain a candidate set C and classification result Rt . 2: According to (8), calculate the gate value g . 3: Perform the second phase classification using CRC as the student according to (9) and obtain the result of student Rs . 4: Calculate score s learned by the student according to (10). 5: if Rt  Rs then   L  s  , if s  g . I    L  g  , otherwise 6: else I  Rt . 7: 8: end if 10: return I  3. Experiments To verify the effectiveness of our proposed method in different classification tasks, we performed several experiments on five datasets. Specifically, we tested our method on the FEI, MUCT, and YouTube facial datasets, the COIL-100 object dataset and the MNIST handwriting datasets, respectively. In addition, we conducted bench-mark experiments to other popular classifiers. The recognition rate was evaluated using a hand-out method, and we set different configurations for the different datasets. On COIL-100, MUCT and FEI, we increased the number of training samples in each class for every iteration and took the remaining samples of each class as test sample. Then, we calculated the average accuracy and maximum accuracy respectively. On MNIST and YouTube, we directly used the predivided training set and testing set. The experiments were executed using MATLAB R2018b on a PC with one 3.40GHz CPU and 16.0 GB RAMs.  3.1 Dataset description As shown in Figure 2, we used five image datasets in total to evaluate our proposed method, including COIL-100 [24], MNIST handwriting digits [25], MUCT [26], FEI [27] and YouTubeFace [28], respectively. The details of each database are summarized in Table 1. There are three facial datasets used in the experiments. The MUCT face database contains 3,755 face images from 276 people. The resolution of each image is 640*480 pixels. All images were captured by a CCD camera and stored in 24-bit RGB format. The FEI dataset is a face dataset containing 2,800 images from 200 people (14 images per  ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL 7 person). The resolution of each original image is 640*480 pixels. In our experiments, we used the 24*96 pixels version. FEI and MUCT are relatively small, therefore, we wish to demonstrate our proposed TS-STSS works well in small datasets.  Figure 2: Image Datasets: (a) COIL-100, (b) MNIST, (c) MUCT, (d) FEI, (e) YouTubeFace. The last face dataset is YouTubeFace, which is a large-scale dataset. It is designed for studying unconstrained face recognition problems in video. In this dataset, 3,425 videos from 1,595 different people were collected from the YouTube website and labeled according to the LFW image collection method [29]. The resolution of each image is 32*32 pixels. In our experiments, we chose 1,283 classes with over 100 samples, and randomly selected 100 samples per class, 128,300 samples in total. Hence, we can evaluate the performance of TS-STSS for large-scale recognition. The COIL-100 dataset (Columbia Object Image Library) is the object dataset, which collected 100 objects and contains 7,200 images in total with a black background. Each object has 72 images captured in different degrees by a CCD color camera. The resolution of each image is 32*32 pixels. The MNIST hand-writing digits database is a hand-writing digits database built by LeCun et.al [25], containing 60,000 examples for a training set and 10,000 examples for a test set. The resolution of each image is 20*20 pixels. Each image was acquired from the center of 28*28 pixels from the original image and processed by a normalization algorithm. Database Classes Samples Size Dimension FEI 200 2,800 24*96 2-D MUCT 276 3,755 640*480 2-D YouTubeFace 1,595 620,951 32*32 2-D COIL-100 100 7,200 32*32 2-D MNIST 10 7,0000 28*28 2-D Table 1: Configurations of the image databases in the experiments.  3.2 Face Recognition We used the MUCT, FEI and YouTubeFace datasets to perform experiments on face recognition. From the results shown in Table 2, we notice that TS-STSS achieves the highest accuracy on the MUCT dataset, which is 92.06%, indicating its effectiveness on face recognition. Besides this, TS-STSS outperforms most of the classifiers in face recognition both in maximum and average accuracy (MUCT: 92.06%, 80.03%; FEI: 90.5% and 62.45%). As for the YouTubeFace dataset, the proposed method achieved a recognition rate of 90.94%, which is only 0.1% off the best result from [12]. Figures 3(a)  8 ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL and 3(b) show the accuracies generated by SRC, CRC and TS-STSS on the MUCT and FEI datasets respectively. In Figure 3(a), it is obvious that TS-STSS produced a better recognition performance no matter the training samples. In Figure 3(b), the accuracy of TS-STSS and SRC is quite competitive in beginning, while TS-STSS surpasses SRC after using seven training samples. Object Methods SRC CRC K-SVD SVM KNN TPSTR[5] STMS[12] TS-STSS  Handwriting  Face  COIL-100 MNIST MUCT FEI YouTubeFace MAX AVG ACC MAX AVG MAX AVG ACC 76.97 73.77 95.96 90.15 79.01 89.80 57.95 84.91 70.84 65.23 82.83 85.83 76.87 74.75 49.26 72.12 61.58 58.62 82.87 77.09 70.26 65.88 40.95 53.26 58.94 53.54 28.44 24.78 57.13 40.95 78.90 98.60 74.56 70.02 95.00 67.94 57.97 69.63 48.55 90.00 76.89 72.41 87.27 88.54 66.48 89.17 61.88 78.04 77.19 72.43 95.59 89.64 75.26 89.66 61.32 91.04 96.19 90.94 78.84 75.03 92.06 80.03 90.50 62.45 Table 2: Recognition rate comparisons to popular classifiers. (Note: Unit of data is %, and bold figures indicate the best results.)  3.3 Object Recognition We used the COIL-100 dataset to perform experiments on object recognition. As can be seen in Table 2, the highest accuracy achieved by TS-STSS is 78.84%, which is higher than SRC (76.97%) and CRC (70.84%), respectively. Noticeably, the highest improvement for average accuracy compared with SRC and CRC is 1.26% and 9.8% correspondingly, showing that the proposed method has a significant effect on object recognition. Figure 3(c) shows the accuracy generated by SRC, CRC and TS-STSS. We can observe directly that the gap between TS-STSS and SRC, CRC is larger when the size of the training set is increasing. As more training samples are used for representation, the difference between each class becomes larger. Therefore, the teacher classifier is able to supervise the student more accurately.  Figure 3: Recognition rate vs. increasing the number of training samples: (a) MUCT, (b) FEI, (c) COIL-100  ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL 9  3.4 Hand-writing Recognition We applied the MNIST dataset to perform experiments on hand-writing recognition, where the results are displayed in Table 2. Here, the accuracy of SRC and CRC is 95.96% and 82.83% respectively. We can see that TS-STSS improves the recognition rate with the highest accuracy of 96.19%, which is higher than SRC and CRC by 0.23% and 13.36, respectively. Moreover, the proposed TS-STSS also achieves a better performance than TPSTR and STMS by 8.92% and 0.6%, correspondingly. Compared with other popular classifiers like K-SVD (82.87%), KNN (95.00%), and SVM (98.60%) TS-STSS is competitive as well.  3.5 Discussion The experiments cover a variety of conditions and tasks in image classification, including face, object and hand-writing recognition, as well as different dataset sizes. The promising performances of our proposed method has been well proved. In addition, we can obtain the following inferences. (1) Enlarging the training set is helpful in TS-STSS, since the implementation is based on sparse representation. As shown in Figure 3, the MUCT, FEI and COIL-100 datasets have no pre-split training and test sets, hence different training samples were utilized to train the classifiers. The accuracy keeps increasing as the training set becomes larger. (2) The supervision of the teacher classifier is the key to improve the Two-Stage classification. As shown in Table 2, TS-STSS outperforms both TPSTR [5] and STMS [12] all but once (where for YouTubeFace the different with [12] is only 0.1%). This confirms our expectation that applying a consistent scoring criteria in two stages is beneficial to classification, while the conventional two-stage classifiers lack this. (3) Compared to other linear methods, TS-STSS is very promising. Table 2 shows the recognition results of other popular linear classifiers, where TS-STSS consistently produces the highest accuracy in most cases. Furthermore, TS-STSS introduces only one additional parameter, the number of candidate class k , which can be set to an empirical value of C / 2 . (4) Compared with SRC and CRC, TS-STSS outperforms both of them on all experiments, indicating that our proposed framework successfully integrates two weaker classifiers to form a stronger classifier in image classification.  4. Conclusion In this paper, we propose a novel image classification framework named Two-Stage image classification Supervised by a Single-Teacher Single-Student model (TS-STSS). In the first stage, a candidate set of classes are chosen and the classification score vector is built using the L1-based SRC classifier (Teacher). Then, the L2-based CRC classifier (Student) represents the test sample using the candidate set in the second stage, under the supervision of the teacher classifier. In order to make a more precise score, we formulate it to the Single-Teacher Single-Student (STSS) problem. This image classification framework is able to combine two different weaker classifiers to form a stronger classifier. The experiments on five popular image datasets proved its effectiveness and promising capability on image classification, outperforming many other popular methods. Currently, we have only implemented the proposed framework with linear sparse methods, SRC and CRC. It will be interesting to consider using other linear models as the teacher and student classifiers, i.e., dictionary learning [30], SVM, KNN, etc. Some nonlinear classifiers, like the ones based on deep neural networks, are potentially good  10ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL choices as well. Despite our method utilizing image data, using deep features [20] ought to be helpful in real-world applications. We will continue to observe and explore these options in the future. Acknowledgements. This work was supported by the University of Macau (MYRG2018-00053-FST).  References [1] X. Dong, H. Zhang, J. Sun and W. Wan, A two-stage learning approach to face recognition. Journal of Visual Communication Image Representation, 43:21-29, 2017. [2] Z. Fan, M. Ni, Q. Zhu and E. Liu. Weighted sparse representation for face recognition. Neurocomputing, 151(1):304-309, 2015. [3] Y. Lei, Y. Guo, M. Hayat, M. Bennamoun and X. Zhou. A Two-Phase Weighted Collaborative Representation for 3D partial face recognition with single sample. Pattern Recognition, 52:218237, 2016. [4] J. Li, J. Cao, K. Lu. Improve the two-phase test samples representation method for palmprint recognition. Optik, 124(24):6651-6656, 2013. [5] Y. Xu, D. Zhang, J. Yang and J. Yang. A two-phase test sample sparse representation method for use with face recognition. IEEE Transactions on Circuits Systems for Video Technology, 21(9):1255-1262, 2011. [6] Y. Xu, W. Zuo, and Z. Fan. Supervised sparse representation method with a heuristic strat-egy and face recognition experiments. Neurocomputing, 79:125-131, 2012. [7] T. Wong and C. Hsu. Two-stage classification methods for microarray data. Expert Systems with Applications, 34(1):375-383, 2008. [8] L. Han, and Y. Zhang. Multi-stage multi-task learning with reduced rank. AAAI Conference on Artificial Intelligence, pages 1638-1644, Phoenix, Arizona, 2016. [9] S. Zeng, X. Yang and J. Gou. Using kernel sparse representation to perform coarse-to-fine recognition of face images. Optik, 140: 528-535, 2017. [10] Y. Xu, Q. Zhu, Z. Fan and D. Zhang, J. Mi and Z. Lai. Using the idea of the sparse representation to perform coarse-to-fine face recognition. Information Sciences, 238: 138-148, 2013. [11] Z. Liu, J. Pu, M. Xu and Y. Qiu. Face recognition via weighted two-phase test sample sparse representation. Neural Processing Letters, 41(1):43-53, 2015. [12] S. You, C. Xu, C. Xu and D. Tao. Learning from Multiple Teacher Networks. Thirty-Second AAAI Conference on Artificial Intelligence, pages 1285-1294, 2017. [13] S. You, C. Yu, C. Xu, D. Tao, Learning from Multiple Teacher Networks, 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1285-1294, 2017. [14] X. Zheng, Y. Hsu, and J. Huang. Training student networks for acceleration with conditional adversarial networks. BMVC2018, 2018. [15] C. Corinna and V. Vladimir. k Support-vector networks. Machine Learning, 20(3):273-297, 1995. [16] Q., Liming L., L. Zhen, and L. Jing. A novel projection nonparallel support vector machine for pattern classification. Engineering Applications of Artificial Intelligence, 75: 64-75, 2018.  ZHOU ET AL.: CLASSIFICATION SUPERVISED BY TEACHER STUDENT MODEL11 [17] L. Bai, Y. Wang, Z. Li and C. Na. Clustering by twin support vector machine and least square twin support vector classifier with uniform output coding. Knowledge-Based Systems, 163(1):227-240, 2019. [18] J. Wright, A. Yang, A. Ganesh, S. S. Sastry and Y. Ma. Robust face recognition via sparse representation. IEEE transactions on pattern analysis machine intelligence, 31(2): 210-227, 2009. [19] L. Zhang, M. Yang, X. Feng. Sparse representation or collaborative representation: Which helps face recognition? 2011 International conference on computer vision, pages 471-478, 2011. [20] S. Zeng, B. Zhang, Y. Zhang and J. Gou. Collaboratively Weighting Deep and Classic Representation via L2 Regularization for Image Classification. Asian Conference on Machine Learning, pages 502-517, 2018. [21] Z. Chen, W. Zuo, Q. Hu, L. Lin, Kernel sparse representation for time series classification. Information Sciences, 292(20):15-26, 2015. [22] T. Shu, B. Zhang and Y. Y. Tang. Sparse Supervised Representation-Based Classifier for Uncontrolled and Imbalanced Classification. IEEE Transactions on Neural Networks and Learning Systems, 1 - 10, 2018. [23] J. Wright, Y. Ma, J. Mairal, M. Sapiro, T. Huang and S. Yan. Sparse representation for computer vision and pattern recognition. Proceedings of the IEEE, 98:1031-1044, 2010. [24] S. A. Nene and S. K. Nayar and H. Murase. Columbia object image library (coil 100). Department of Computer Science Columbia University, 1996. [25] D. Li, The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29:141-142, 2012. [26] S. Milborrow and J. Morkel and F. Nicolls. The MUCT Landmarked Face Database. Pattern Recognition Association of South Africa, 2010. [27] Thomaz, C. E. Giraldi, G. Antonio, A new ranking method for principal components analysis and its application to face image analysis. Image Vision Computing, 28(6): 902-913, 2010. [28] L. Wolf, T. Hassner and I. Maoz. Face recognition in unconstrained videos with matched background similarity. CVPR2011, 2011. [29] E. Learned-Miller, G. Huang, A. RoyChowdhury, H. Li and G. Hua. Labeled faces in the wild: A survey. Advances in face detection and facial image analysis, pages 189-248, 2016. [30] J. Mairal, F. Bach, J. Ponce and G. Sapiro. Online dictionary learning for sparse coding. 26th Annual International Conference on Machine Learning, pages 689-696, 2009.  "
"arXiv:1909.12114v1 [cs.LG] 25 Sep 2019  Explaining and Interpreting LSTMs Leila Arras?1 , José Arjona-Medina?2 , Michael Widrich2 , Grégoire Montavon3 , Michael Gillhofer2 , Klaus-Robert Müller3,4,5 , Sepp Hochreiter2 , and Wojciech Samek1( ) 1  Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany {leila.arras,wojciech.samek}@hhi.fraunhofer.de 2 Johannes Kepler University Linz, 4040 Linz, Austria {arjona,widrich,gillhofer,hochreit}@ml.jku.at 3 Technische Universität Berlin, 10587 Berlin, Germany {gregoire.montavon,klaus-robert.mueller}@tu-berlin.de 4 Korea University, Anam-dong, Seongbuk-gu, Seoul 02841, Korea 5 Max Planck Institute for Informatics, 66123 Saarbrücken, Germany   Introduction  In practical applications, building high-performing AI systems is not always the sole objective, and interpretability may also be an important issue [16]. Most of the recent research on interpretable AI has focused on feedforward neural networks, especially the deep rectifier networks and variants used for image recognition [68,79]. Layer-wise relevance propagation (LRP) [6,51] was shown in this setting to provide for state-of-the-art models such as VGG-16, explanations that are both informative and fast to compute, and that could be embedded in the framework of deep Taylor decomposition [52]. However, in the presence of sequential data, one may need to incorporate temporal structure in the neural network model, e.g. to make forecasts about ?  L. Arras and J. Arjona-Medina contributed equally to this work. The final authenticated publication is available online at https://doi.org/10.1007/ 978-3-030-28954-6 11. In: W. Samek et al. (Eds.) Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Lecture Notes in Computer Science, vol 11700, pp. 211-238. Springer, Cham (2019)  2  Arras et al.  future time steps. In this setting it is key to be able to learn the underlying dynamical system, e.g. with a recurrent neural network, so that it can then be simulated forward. Learning dynamical systems with long-term dependencies using recurrent neural networks presents a number of challenges. The backpropagation through time learning signal tends to either blow up or vanish [30,10]. To reduce this difficulty, special neural network architectures have been proposed, in particular, the Long Short-Term Memory (LSTM) [30,35,37], which makes use of special accumulators and gating functions. The multiple architectural changes and the unique nature of the sequential prediction task make a direct application of the LRP-type explanation technique non-straightforward. To be able to deliver accurate explanations, one needs to carefully inspect the structure of the LSTM blocks forming the model and their interaction. In this chapter, we explore multiple dimensions of the interface between the LRP technique and the LSTM. First, we analyze how the LRP propagation mechanism can be adapted to accumulators and gated interactions in the LSTM. Our new propagation scheme is embedded in the deep Taylor decomposition framework [52], and validated empirically on sentiment analysis and on a toy numeric task. Further, we investigate how modifications of the LSTM architecture, in particular, on the cell input activation, the forget and output gates and on the network connections, make explanations more straightforward, and we apply these changes in a reinforcement learning showcase. The present chapter elaborates on our previous work [2,5].  2 2.1  Background Long Short-Term Memory (LSTM)  Recently, Long Short-Term Memory (LSTM; [30,35,37]) networks have emerged as the best-performing technique in speech and language processing. LSTM networks have been overwhelmingly successful in different speech and language applications, including handwriting recognition [24], generation of writings [23], language modeling and identification [22,78], automatic language translation [73], speech recognition [63,17], analysis of audio data [49], analysis, annotation, and description of video data [15,76,70]. LSTM has facilitated recent benchmark records in TIMIT phoneme recognition, optical character recognition, text-tospeech synthesis, language identification, large vocabulary speech recognition, English-to-French translation, audio onset detection, social signal classification, image caption generation, video-to-text description, end-to-end speech recognition, and semantic representations. The key idea of LSTM is the use of memory cells that allow for constant error flow during training. Thereby, LSTM avoids the vanishing gradient problem, that is, the phenomenon that training errors are decaying when they are back-propagated through time [30,33]. The vanishing gradient problem severely impedes credit assignment in recurrent neural networks, i.e. the correct identification of relevant events whose effects are not immediate, but observed with  Explaining and Interpreting LSTMs  3  possibly long delays. LSTM, by its constant error flow, avoids vanishing gradients and, hence, allows for uniform credit assignment, i.e. all input signals obtain a similar error signal. Other recurrent neural networks are not able to assign the same credit to all input signals and therefore, are very limited concerning the solutions they will find. Uniform credit assignment enables LSTM networks to excel in speech and language tasks: if a sentence is analyzed, then the first word can be as important as the last word. Via uniform credit assignment, LSTM networks regard all words of a sentence equally. Uniform credit assignment enables to consider all input information at each phase of learning, no matter where it is located in the input sequence. Therefore, uniform credit assignment reveals many more solutions to the learning algorithm, which would otherwise remain hidden. LSTM in a Nutshell. The central processing and storage unit for LSTM recurrent networks is the memory cell. As already mentioned, it avoids vanishing gradients and allows for uniform credit assignment. The most commonly used  ...  ...  cell output  y  LSTM cell  recurrent  Legend  recurrent ...  output  o  feedforward data flow  + output gate  recurrent data flow ...  feedforward weights  input  h  recurrent weights  recurrent ...  branching point  f  +  recurrent  +  ...  ...  c  forget gate  i  input  z  multiplication  +  + input gate  gate activation function (usually sigmoid)  ...  input  g  g  input activation function (usually tanh or sigmoid)  h  output activation function (usually tanh or sigmoid)  ...  ...  + input  recurrent  sum over all inputs  cell input  Fig. 1. LSTM memory cell without peepholes. z is the vector of cell input activations, i is the vector of input gate activations, f is the vector of forget gate activations, c is the vector of memory cell states, o is the vector of output gate activations, and y is the vector of cell output activations. The activation functions are g for the cell input, h for the cell state, and σ for the gates. Data flow is either “feed-forward” without delay or “recurrent” with a one-step delay. “Input” connections are from the external input to the LSTM network, while “recurrent” connections take inputs from other memory cell outputs y in the LSTM network with a delay of one time step, accordingly to Equations 1-6. The cell state c also has a recurrent connection with one time step delay to himself via a multiplication with the forget gate f , and gets accumulated through a sum with the current input.  4  Arras et al.  LSTM memory cell architecture in the literature [25,66] contains forget gates [19,20] and peephole connections [18]. In our previous work [38,34], we found that peephole connections are only useful for modeling time series, but not for language, meta-learning, or biological sequences. That peephole connections can be removed without performance decrease, was recently confirmed in a large assessment, where different LSTM architectures have been tested [26]. While LSTM networks are highly successful in various applications, the central memory cell architecture was not modified since 2000 [66]. A memory cell architecture without peepholes is depicted in Figure 1. In our definition of an LSTM network, all units of one kind are pooled to a vector: z is the vector of cell input activations, i is the vector of input gate activations, f is the vector of forget gate activations, c is the vector of memory cell states, o is the vector of output gate activations, and y is the vector of cell output activations. We assume to have an input sequence, where the input vector at time t is xt . The matrices Wz , Wi , Wf , and Wo correspond to the weights of the connections between inputs and cell input, input gate, forget gate, and output gate, respectively. The matrices Uz , Ui , Uf , and Uo correspond to the weights of the connections between the cell output activations with one-step delay and cell input, input gate, forget gate, and output gate, respectively. The vectors bz , bi , bf , and bo are the bias vectors of cell input, input gate, forget gate, and output gate, respectively. The activation functions are g for the cell input, h for the cell state, and σ for the gates, where these functions are evaluated in a component-wise manner if they are applied to vectors. Typically, either the 1 sigmoid 1+exp(−x) or tanh are used as activation functions. denotes the pointwise multiplication of two vectors. Without peepholes, the LSTM memory cell forward pass rules are (see Figure 1): zt = g (Wz xt + Uz yt−1 + bz )  cell input  (1)  it = σ (Wi xt + Ui yt−1 + bi )  input gate  (2)  ft = σ (Wf xt + Uf yt−1 + bf )  forget gate  (3)  cell state  (4)  output gate  (5)  cell output  (6)  c t = it  zt + ft  ct−1  ot = σ (Wo xt + Uo yt−1 + bo ) yt = ot  h (ct )  Long-Term Dependencies vs. Uniform Credit Assignment. The LSTM network has been proposed with the aim to learn long-term dependencies in sequences which span over long intervals [37,36,31,32]. However, besides extracting longterm dependencies, LSTM memory cells have another, even more important, advantage in sequence learning: as already described in the early 1990s, LSTM memory cells allow for uniform credit assignment, that is, the propagation of errors back to inputs without scaling them [30]. For uniform credit assignment of current LSTM architectures, the forget gate f must be one or close to one. A memory cell without an input gate i just sums up all the squashed inputs it receives during scanning the input sequence. Thus, such a memory cell is  Explaining and Interpreting LSTMs  5  equivalent to a unit that sees all sequence elements at the same time, as has been shown via the “Ersatzschaltbild” (engl. equivalent circuit diagram) [30]. If an output error occurs only at the end of the sequence, such a memory cell, via backpropagation, supplies the same delta error at the cell input unit z at every time step. Thus, all inputs obtain the same credit for producing the correct output and are treated on an equal level and, consequently, the incoming weights to a memory cell are adjusted by using the same delta error at the input unit z. In contrast to LSTM memory cells, standard recurrent networks scale the delta error and assign different credit to different inputs. The more recent the input, the more credit it obtains. The first inputs of the sequence are hidden from the final states of the recurrent network. In many learning tasks, however, important information is distributed over the entire length of the sequence and can even occur at the very beginning. For example, in language- and text-related tasks, the first words are often important for the meaning of a sentence. If the credit assignment is not uniform along the input sequence, then learning is very limited. Learning would start by trying to improve the prediction solely by using the most recent inputs. Therefore, the solutions that can be found are restricted to those that can be constructed if the last inputs are considered first. Thus, only those solutions are found that are accessible by gradient descent from regions in the parameter space that only use the most recent input information. In general, these limitations lead to suboptimal solutions, since learning gets trapped in local optima. Typically, these local optima correspond to solutions which efficiently exploit the most recent information in the input sequence, while information way back in the past is neglected. 2.2  Layer-Wise Relevance Propagation (LRP)  Layer-wise relevance propagation (LRP) [6] (cf. [51] for an overview) is a technique to explain individual predictions of deep neural networks in terms of input variables. For a given input and the neural network’s prediction, it assigns a score to each of the input variables indicating to which extent they contributed to the prediction. LRP works by reverse-propagating the prediction through the network by means of heuristic propagation rules that apply to each layer of a neural network [6]. In terms of computational cost the LRP method is very efficient, as it can be computed in one forward and backward pass through the network. In various applications LRP was shown to produce faithful explanations, even for highly complex and nonlinear networks used in computer vision [6,64]. Besides it was able to detect biases in models and datasets used for training [44], e.g. the presence of a copyright tag that spuriously correlated to the class ‘horse’ in the Pascal VOC 2012 dataset. Further, it was used to get new insights in scientific and medical applications [71,39,77], to interpret clustering [40], to analyze audio data [75,9], and to compare text classifiers for topic categorization [3]. Conservative Propagation. LRP explains by redistributing the neural network output progressively from layer to layer until the input layer is reached. Similar to other works such as [41,80,67], the propagation procedure implemented by LRP  6  Arras et al.  is based on a local conservation principle: the net quantity, or relevance, received by any higher layer neuron is redistributed in the same amount to neurons of the layer below. In this way the relevance’s flow is analog to the Kirchhoffs first law for the conservation of electric charge, or to the continuity equation in physics for transportation in general form. Concretely, if j and k are indices for neurons in two consecutive layers, and denoting by Rj←k the relevance flowing between two neurons, we have the equations: P  Rj←k = Rk  P  Rj←k .  j  Rj =  k  This local enforcement of conservation induces conservation atPcoarser P P scales, in particular, conservation between consecutive layers R = j j j k Rj←k = P P P R = R , and ultimately, conservation at the level of the whole j←k k k j k deep P neural network, i.e. given an input x = (xi )i and its prediction f (x), we have i Ri = f (x)1 . This global conservation property allows to interpret the result as the share by which each input variable has contributed to the prediction. LRP in Deep Neural Networks. LRP has been most commonly applied to deep rectifier networks. In these networks, the activations at the current layer can be computed from activations in the previous layer as: ak = max 0,  P  0,j  aj wjk    A general family of propagation rules for such types of layer is given by [51]: Rj =  X k  +  a · ρ(wjk ) Pj Rk 0,j aj · ρ(wjk )  Specific propagation rules such as LRP-, LRP-α1 β0 and LRP-γ fall under this umbrella. They are easy to implement [42,51] and can be interpreted as the result of a deep Taylor decomposition of the neural network function [52]. On convolutional neural networks for computer vision, composite strategies making use of different rules at different layers have shown to work well in practice [43,51]. An alternative default strategy in computer vision is to uniformly employ the LRP-α1 β0 in every hidden layer [53], the latter has the advantage of having no free parameter, and delivers positive explanations. On convolutional neural networks for text, LRP- with a small  value was found to work well [3,57], it provides a signed explanation. While LRP was described in the context of a layered feed-forward neural network, the principle is general enough to apply to arbitrary directed acyclic graphs, including recurrent neural networks unfolded in time such as LSTMs. 1  The global conservation is exact up to the relevance absorbed by some stabilizing term, and by the biases, see details later in Section 3.1.  Explaining and Interpreting LSTMs  3  7  Extending LRP for LSTMs  We address the question of how to explain the LSTM model’s output by expanding the previously described LRP technique to “standard” LSTM architectures, in the form they are most commonly used in the literature [26], i.e. following the recurrence Equations 1-6 and Figure 1 introduced in Section 2.1, and usually containing the tanh nonlinearity as an activation function for the cell input and the cell state. For this, we first need to identify an appropriate structure of computation in these models, and introduce some notation. Let s, g be the neurons representing the signal and the gate, let p be the neuron representing the product of these two quantities. Let f be the neuron corresponding to the forget gate. Let k be the neuron on which the signal is being accumulated. Let k − 1, p − 1, . . . be the same neurons at previous time steps. We can recompose the LSTM forward pass in terms of the following three elementary types of computation: P P 1. linear mappings zs = 0,j aj wjs , zg = 0,j aj wjg   2. gated interactions ap = tanh zs · sigm zg ak = af · ak−1 + ap  3. accumulation  These three types of computation and the way they are typically interconnected are shown graphically in Figure 2.  ap–1 ak–1  zg  af  ap ak  aj  accumulation  linear mappings  zs gated interactions  Fig. 2. Three elementary computations performed by the LSTM from the perspective of LRP.  Linear mappings form the input of the gated interactions. The output of some of the gated interactions enter into the accumulation function. 3.1  Linear Mappings  Each output of this computation is a weighted sum over a large number of input variables. Here, one strategy is to redistribute the relevance in proportion to  8  Arras et al.  the weighted activations aj wjs , as they occur in the linear projection formulas above. One way of implementing this strategy is the epsilon-rule (LRP-) given by [6]: X a w Pj js Rj = Rs  + s 0,j aj wjs s  P where s =  · sign 0,j aj wjs is a small stabilizer that pushes the denominator away from zero by some constant factor, and has the effect of absorbing some relevance when the weighted activations are weak or contradictory. This type of propagation rule was employed by previous works with recurrent neural networks [2,4,14,57,77]. A large value for  tends to keep only the most salient factors of explanation. Note that, in our notation, neuron biases are taken into account via a constant neuron a0 = 1 whose connection weight is the corresponding bias. This neuron also gets assigned a share of relevance. However its relevance will not be propagated further and will get trapped in that neuron, since the “bias neuron” has no lower-layer connections. 3.2  Gated Interactions  These layers do not have a simple summing structure as the linear mappings. Their multiplicative nonlinearity makes them intrinsically more difficult to handle. Recently, three works extended the LRP propagation technique to recurrent neural networks, such as LSTMs [37] and GRUs [12], by proposing a rule to propagate the relevance through such product layers [2,4,14]. These LRP extensions were tested in the context of sentiment analysis, machine translation and reinforcement learning respectively. Arras et al. [4], in particular, proposed the signal-take-all redistribution rule (Rg , Rs ) = (0, Rp ) referred as “LRP-all” in our experiments. This redistribution strategy can be motivated in a similar way the gates were initially introduced in the LSTM model [37]: the gate units are intended to control the flow of information in the LSTM, but not to be information themselves. In the following, we provide further justification of this rule based on Deep Taylor Decomposition (DTD) [52], a mathematical framework for analyzing the relevance propagation process in a deep network. DTD expresses the relevance obtained at a given layer as a function of the activations in the lower-layer, and determines how the relevance should be redistributed based on a Taylor expansion of this function. Consider the relevance function Rp (zg , zs ) mapping the input z = (zg , zs ) of the gated interaction to the relevance received by the output of that module. We then write its Taylor expansion: Rp (zg , zs ) = Rp (e zg , zes ) +  ∂Rp ∂zg  z e  · (zg − zeg ) +  ∂Rp ∂zs  z e  · (zs − zes ) + . . .  where ze = (e zg , zes ) is a root point of the function, and where the first-order terms can be used to determine on which lower-layer neurons (g or s the relevance  Explaining and Interpreting LSTMs  9  should be propagated). In practice, a root point and its gradient are difficult to compute analytically. However, we can consider instead a relevance model [52] which is easier to analyze, in our case, of the form: bp (zg , zs ) = sigm(zg ) · tanh(zs ) · cp . R bp (zg , zs ) locally. This The variable cp is constant and set such that Rp (zg , zs ) = R model is a reasonable approximation when Rp results from a propagation rule where the activation term naturally factors out (cf. [52]). The relevance model for the gated interaction of the standard LSTM is depicted in Figure 3 (left). sigm(zg ) · tanh(zs ) · cp  sigm(zg ) · max(0, zs ) · cp 2  z z e  0  −1  −2  −2  −1  −1  1  2  zg  −2  0  −1  −2  −1  0  zg  z  1  z e  0  −2 0  z  1  zs  zs  1  2  zs  2  sigm(2 zg ) · sigm(2 zs ) · cp  1  2  −2  −1  0  1  2  zg  Fig. 3. DTD relevance models for different choices of nonlinear functions with nearest root point (white dot). The left model is the standard LSTM. Positive contours are drawn as continuous lines, negative contours as dashed lines, and the dark line represents the zero-valued contour.  Having built the relevance model, we would like to perform a Taylor expansion of it at some root point in the vicinity of the observed point z = (zg , zs ). The nearest root point of the relevance model is found at (e zg , zes ) = (zg , 0), and more generally any root point satisfies zes = 0. A Taylor expansion of this simplified relevance model gives: bp (zg , zs ) = R bp (e R zg , zes )  + sigm0 (e zg ) · tanh(e zs ) · cp · (zg − zeg )  ( = Rg )  + sigm(e zg ) · tanh (e zs ) · cp · (zs − zes )  ( = Rs )  0  + ... Clearly, the first linear term Rg is zero for the nearest root point, thus, no relevance will be redistributed to the gate, however, the saturation effect of the hyperbolic tangent can create a mismatch between the first-order term, and the function value to redistribute. However, if replacing in the LSTM the hyperbolic tangent by the identity or the ReLU nonlinearity (as this was done, for example, in [59]), then we get an exact decomposition of the relevance model with bp ), since the Taylor remainder is exactly zero in this case. This (Rg , Rs ) = (0, R corresponds to the LRP-all redistribution rule.  10  Arras et al.  This section has justified the signal-take-all strategy for standard LSTMs. In Section 4 modified LSTM variants that are tuned for further interpretability will benefit from a different propagation strategy. For example, using sigmoids both for the gate and the signal (cf. Figure 3 right) suggests a different propagation strategy. A more complete set of propagation rules that have been used in practice [2,4,14,57,59,77], and that we consider in our experiments, is given in Table 1. In addition to the definitions provided in Table 1, in order to avoid near zero division, one may add a stabilizing term into the denominator of the LRPprop and LRP-abs variants, similarly to the  epsilon-rule stabilization for linear mappings. It has the form  · sign zg + zs in the first case, and simply  in the other case, where  is a small positive number.  Table 1. Overview of LRP propagation rules for gated interactions, and whether they derive from a deep Taylor decomposition. LRP-all stands for “signal-take-all”, LRPprop stands for “proportional”, LRP-abs is similar to LRP-prop but with absolute values instead, and LRP-half corresponds to equal redistribution. Name  Proposed in Received by gate  Received by signal  LRP-all LRP-prop  [4] [14,2]  Rs = Rp s Rs = zgz+z Rp s  X ×  s| Rs = |zg|z R |+|zs | p Rs = 0.5 · Rp  × ×  LRP-abs LRP-half  3.3  Rg = 0 zg Rp Rg = zg +z s |z |  [2]  g Rg = |zg |+|z Rp s| Rg = 0.5 · Rp  DTD  Accumulation  The last type of module one needs to consider is the accumulation module that discounts the LSTM memory state with a “forget” factor, and adds a small additive term based on current observations: ak = af · ak−1 + ap . Consider the relevance Rk of the accumulator neuron ak for the final time step. Define Rk = ak · ck . Through the accumulation module, we get the following redistribution: Rp = ap · ck Rk−1 = af · ak−1 · ck , where we have used the signal-take-all strategy in the product, and the epsilonrule (with no stabilizer) in the sum. Iterating this redistribution process on  Explaining and Interpreting LSTMs  11  previous time steps, we obtain: Rp−1 = af · ap−1 · ck Rp−2 = (af · af −1 ) · ap−2 · ck .. . Q  T Rp−T = t=1 af −t+1 · ap−T · ck . Note that, here, we assume a simplified recurrence structure over the standard LSTM presented in Figure 1, in particular we assume that neurons ap do not redistribute relevance to past time steps via zs (i.e. zs is connected only to the current input and not to previous recurrent states), to simplify the present analysis. Now we inspect the structure of the relevance scores Rp , . . . , Rp−T at each time step, as given above. We can see that the relevance terms can be divided into three parts: 1. A product of forget gates: This term tends to decrease exponentially with every further redistribution step, unless the forget gate is equal to one. In other words, only the few most recent time steps will receive relevance. 2. The value of the product neuron ap at the current time step. In other words, the relevance at a given time step is directly influenced by the activation of its representative neuron, which can be either positive or negative. 3. A term that does not change with the time steps, and relates to the amount of relevance available for redistribution. These observations on the structure of the relevance over time provide a further justification for the LRP explanation procedure, which we will be validating empirically in Section 5. They also serve as a starting point to propose new variants of the LSTM for which the relevance redistribution satisfies further constraints, as proposed in the following Section 4.  4  LSTM Architectures Motivated by LRP  A “standard” LSTM network with fully connected LSTM blocks, as presented in Figure 1, is a very powerful network capable of modelling extremely complex sequential tasks. However, in many cases, an LSTM network with a reduced complexity is able to solve the same problems with a similar prediction performance. With the further goal of increasing the model’s interpretability, we propose some modifications which simplify the LSTM network and make the resulting model easier to explain with LRP. 4.1  LSTM for LRP Backward Analysis: Nondecreasing Memory Cells  The LRP backward propagation procedure is made simpler if memory cell states ct are nondecreasing, this way the contribution of each input to each memory  12  Arras et al.  cell is well-defined, and the problem that a negative and a positive contribution cancel each other is avoided. For nondecreasing memory cells and backward analysis with LRP, we make the following assumptions over the LSTM network from Figure 1 and Equations 1-6:  (A1) ft = 1 for all t. That is, the forget gate is always 1 and nothing is forgotten. This ensures uniform credit assignment over time, and alleviates the problem identified earlier in Section 3.3 that the relevance redistributed via the accumulation module decreases over time. (A2) g > 0, that is, the cell input activation function g is positive. For example 1 we can use a sigmoid σ(x) = 1+exp(−x) : g(x) = ag σ(x), with ag ∈ {2, 3, 4}. Indeed methods like LRP and the epsilon-rule for linear mappings (cf. Section 3.1) face numerical stability issues when negative contributions cancel with positive contributions [53]. With a positive g all contributions are positive, and the redistribution in the LSTM accumulation module is made more stable. Further, we assume that the cell input z has a negative bias, that is, bz < 0. This is important to avoid the drift effect. The drift effect is that the memory content only gets positive contributions, which leads to an increase of c over time. Typical values are bz ∈ {−1, −2, −3, −4, −5}. (A3) We want to ensure that for the cell state activation it holds h(0) = 0, such that, if the memory content is zero, then nothing is transferred to the next layer. Therefore we set h = ah tanh, with ah ∈ {1, 2, 4}. (A4) The cell input z is only connected to the input, and is not connected to other LSTM memory cell outputs. Which means Uz is zero. This ensures that LRP assigns relevance z to the input and z is not disturbed by redistributing relevance to the network. (A5) The input gate i has only connections to other memory cell outputs, and is not connected to the input. That is, Wi is zero. This ensures that LRP assigns relevance only via z to the input. (A6) The output gate o has only connections to other memory cell outputs, and is not connected to the input. That is, Wo is zero. This ensures that LRP assigns relevance only via z to the input. (A7) The input gate i has a negative bias, that is, bi < 0. Like with the cell input the negative bias avoids the drift effect. Typical values are bi ∈ {−1, −2, −3, −4}. (A8) The output gate o may also have a negative bias, that is, bo < 0. This allows to bring in different memory cells at different time points. It is related to resource allocation. (A9) The memory cell state content is initialized with zero at time t = 0, that is, c0 = 0. Lastly, the memory cell content ct is non-negative ct ≥ 0 for all t, since zt ≥ 0 and it ≥ 0.  Explaining and Interpreting LSTMs  13  The resulting LSTM forward pass rules for LRP are: zt = ag σ (Wz xt + bz ) it = σ (Ui yt−1 + bi ) c t = it  (7)  input gate  (8)  cell state  (9)  zt + ct−1  ot = σ (Uo yt−1 + bo ) yt = ot  cell input  output gate  (10)  cell output  (11)  ah tanh (ct )  See Figure 4a which depicts these forward pass rules for LRP.  o  +  h  ...  z  z  cell input input  sum over all inputs  sigmoid activation  +  h  cell activation (tanh)  ...  b  multiplication  +  input gate  ...  ...  a  branching point  c +  +  cell input  feedforward weights recurrent weights  1.0  + input  h  recurrent  i  input gate  cell input  feedforward data flow recurrent data flow  +  +  Legend  y  LSTM cell  c  recurrent  + input  ...  1.0  c +  z  cell output  h  recurrent  i  LSTM cell  ...  1.0  y  output  ...  output gate  recurrent  ...  LSTM y cell  cell output output  recurrent  ...  ...  ...  cell output output  c  Fig. 4. LSTM memory cell used for Layer-Wise Relevance Propagation (LRP). (a) z is the vector of cell input activations, i is the vector of input gate activations, c is the vector of memory cell states, o is the vector of output gate activations, and y is the vector of cell output activations. (b) The memory cell is nondecreasing and guarantees the Markov property. (a and b) Data flow is either “feed-forward” without delay or “recurrent” with a one-step delay. External input reaches the LSTM network only via the cell input z. All gates only receive recurrent input, that is, from other memory cell outputs. (c) LSTM memory cell without gates. External input is stored in the memory cell via the input z.  4.2  LSTM for LRP Backward Analysis: Keeping the Markov Property  Forget gates can modify the memory cells’ information at some future time step, i.e. they could completely erase the cells’ state content. Output gates can hide the cells’ information and deliver it in the future, i.e. output gates can be closed and open at some future time, masking all information stored by the cell. Thus, in order to guarantee the Markov property, the forget and output gates must be disconnected.  14  Arras et al.  The resulting LSTM forward pass rules for Markov memory cells are: zt = ag σ (Wz xt + bz ) it = σ (Ui yt"
"O PTIMAL T RANSPORT, C YCLE GAN, AND P ENALIZED LS FOR U NSUPERVISED L EARNING IN I NVERSE P ROB LEMS Byeongsu Sim1  arXiv:1909.12116v1 [cs.CV] 25 Sep 2019  1 2  Gyutaek Oh2  Sungjun Lim2  Jong Chul Ye1,2∗  Department of Mathematical Sciences, KAIST, Daejeon, Republic of Korea Department of Bio and Brain Engineering, KAIST, Daejeon, Republic of Korea  1  I NTRODUCTION  Inverse problems are ubiquitous in computer vision, biomedical imaging, scientific discovery, etc. In inverse problems, a noisy measurement y ∈ Y from an unobserved image x ∈ X is modeled by y  = Hx + w ,  (1)  where w is the measurement noise, and H : X 7→ Y is the measurement operator. In inverse problems originated from physics, the measurement operator is usually represented by an integral equation: Z Hx(r) := h(r, r 0 )x(r 0 )dr 0 , r ∈ D ⊂ Rd . (2) Rd 0  for d = 2, 3, where h(r, r ; x) is an integral kernel. Then, the inverse problem is formulated as an estimation problem of the unknown x from the measurement y. It is well-known that an inverse problem is ill-posed. A classical strategy to mitigate the ill-posedness is the penalized least squares (PLS) approach: x̂ = arg min c(x; y) := ky − Hxkq + R(x) x  ∗  Jong Chul Ye is the corresponding author of this paper. Email: jong.ye@kaist.ac.kr  1  (3)  for q ≥ 1, where R(x) is a regularization (or penalty) function such as l1 , total variation (TV), etc (Chaudhuri et al., 2014; Sarder & Nehorai, 2006; McNally et al., 1999). In some inverse problems, the measurement operator H is not well defined so that both the unknown operator H and the image x should be estimated. Recently, deep learning approaches with supervised training have become the main approaches to inverse problems because of their excellent and ultra-fast reconstruction performance. For example, in the low-dose x-ray computed tomography (CT) denoising problems, a convolutional neural network (CNN) is trained to learn the relationship between noisy image y and the matched noiseless (or high dose) label images x (Kang et al., 2017). In the context of (3), the supervised neural network can be understood as directly learning the operation x̂ = arg minx c(x; y). Unfortunately, in many applications, matched label data are not available for supervised training. Therefore, unsupervised training without matched reference data has become an important research topic. Recently, generative adversarial network (GAN) has attracted significant attention in machine learning community by providing a way to generate target data distribution from random distribution (Goodfellow et al., 2014). In particular, the authors in (Arjovsky et al., 2017) proposed so-called Wasserstein GAN (W-GAN), which is closely related to the mathematical theory of optimal transport (OT) (Villani, 2008; Peyré et al., 2019). In optimal transport, for given two probability measures supported on the X and Y spaces, one pays a cost for transporting one measure to another. Then, the minimization of the average transportation cost provides an unsupervised way of learning the transport map between the two measures. Unfortunately, these GAN approaches often generate artificial features due to the mode collapsing, so cycleGAN (Zhu et al., 2017) that imposes the one-to-one correspondency has been extensively investigated (Kang et al., 2019; Lu et al., 2017). Although classical PLS, optimal transport, and cycleGAN share the commonality of unsupervised learning which does not require matched training data, there is no mathematical theory to systematically link these seemingly different approaches. Therefore, one of the main contributions of this paper is to unveil the missing link between these methods. In particular, we reveal that cycleGAN architecture can be derived as a dual formulation of the optimal transport problem when the cost function of PLS with a deep learning penalty is used as a transport cost between the two measures. Thus, cycleGAN can be considered as stochastic generalization of PLS. Moreover, this framework is so general that various cycleGAN architecture can be easily obtained by changing the PLS cost. As a proof of concept, we provide novel cycleGAN architecture for unsupervised learning in two physical inverse problems: accelerated MRI, and deconvolution microscopy. In these applications, we show that only one single CNN generator is required, which significantly reduces the training complexity. Experimental results confirmed that the proposed unsupervised learning approaches can successfully provide accurate inversion results without any matched reference.  2 2.1  R ELATED W ORKS O PTIMAL T RANSPORT  Optimal transport compares two measures in a Lagrangian framework (Villani, 2008; Peyré et al., 2019). Formally, we say that T : X 7→ Y transports µ ∈ P (X ) to ν ∈ P (Y), if  ν(B) = µ T −1 (B) , for all ν-measureable set B, (4) which condition is often simply represented by ν = T# µ, where T# is often called the push-forward operator. Monge’s original optimal transport problem (Villani, 2008; Peyré et al., 2019) is then to find a transport map T that transport µ to ν at the minimum total transportation cost: Z min M(T ) := c(x, T (x))dµ(x) (5) T  X  subject to ν = T# µ However, this is usually computational expensive due to the nature of combinatorial assignment. Kantorovich relaxed the assumption to consider a probabilistic transport, which allows mass splitting from a source toward several targets (Villani, 2008; Peyré et al., 2019). Specifically, Kantorovich introduced a joint measure π ∈ P (X × Y) and the associated cost c(x, y), x ∈ X , y ∈ Y such that 2  dπ(x, y) becomes the amount of mass transferred from x to y and c(x, y)dπ(x, y) is the associated cost. Then, the Kantorovich’s relaxation is formulated as Z min K(π) := c(x, y)dπ(x, y) (6) π  X ×Y  subject to π(A × Y) = µ(A), π(X × B) = ν(B) for all measurable set A ∈ X and B ∈ Y. Here, the last two constraints come from the observation that the total amount of mass removed from any measurable set has to equal to the marginals (Villani, 2008; Peyré et al., 2019). Another important advantage of Kantorovich formulation is the dual formulation as stated in the following theorem: Theorem 1 (Kantorovich duality theorem). (Villani, 2008, Theome 5.10, p.57-p.59) Let (X , µ) and (Y, ν) be two Polish probability spaces (separable complete metric space) and let c : X × Y → R be a continuous cost function, such that |c(x, y)| ≤ cX (x) + cY (y) for some cX ∈ L1 (µ) and cY ∈ L1 (ν), where L1 (µ) denotes the set of 1-Lipschitz functions with the measure µ. Then, there is duality: Z Z nZ o c min c(x, y)dπ(x, y) = max ϕ(x)dµ(x) + ϕ (y)dν(y) 1 πΠ(µ,ν)  ϕ∈L (µ)  X ×Y  X  Y  and the above maximum is taken over so-called Kantorovich potential ϕ whose c-transform ϕc (y) = supx (c(x, y) − ϕ(x)) is properly defined. 2.2  P ENALIZED L EAST S QUARE WITH D EEP L EARNING P RIOR  Recently, model-based image reconstruction frameworks using a convolution neural network (CNN) based prior have been extensively studied (Zhang et al., 2017; Aggarwal et al., 2018) thanks to their similarities to the classical regularization theory. The main idea is to utilize a pre-trained neural network to stabilize the inverse solution. Specifically, the problem can be formulated as min c(x; y, Θ, H) = ky − Hxkq + kGΘ (y) − xkp x  (7)  for p ≥ 1, where GΘ (y) is a pre-trained CNN with the network parameter Θ and the input y. The minimization problem of (7) is to find a balance between data fidelity term and the CNN output.  3  M AIN C ONTRIBUTIONS  One of the basic assumptions in the classical PLS formulation is that the measurement y is fixed and one is interested in finding unknown x assuming some prior distribution. In this section, we relax the assumption and consider unsupervised learning situations where both of them are random samples from joint probability measure π(x, y). Interesting, this leads to cycleGAN architecture. Specifically, by considering x and y in (7) as random variables, we have the following reformulation of the cost: c(x, y; Θ, H) = ky − Hxkq + kGΘ (y) − xkp  (8)  where the first two terms before the semi-colon are the random vectors. Unlike (7), the network GΘ (y) in (8) is not pretrained and needs to be learned. More specifically, Θ, H are the network and measurement system parameters that should be estimated. These parameters can be found by minimizing the average transport cost for all combinations of x ∈ X and y ∈ Y with respect to the joint measure π(x, y): Z T(Θ, H) := min c(x, y; Θ, H)dπ(x, y) (9) π  X ×Y  where the minimum is taken over all joint distribution whose marginal distribution with respect to X and Y is µ and ν, respectively. Below we show that the average transportation cost in (9) has an interesting decomposition. 3  Lemma 1. If the mapping GΘ : Y 7→ X is single-valued, then the average transportation cost T(Θ, H) in (9) can be decomposed as T(Θ, H)  =  `cycle (Θ, H) + `OT 0 (Θ, H)  (10)  where Z `cycle (Θ, H) = min π  c(x, y; Θ, H)dπ(x, y)  `OT 0 (Θ, H) = min π  (11)  ZA∪B c(x, y; Θ, H)dπ(x, y)  (12)  X ×Y\A∪B  and A = {(x, y) ∈ X × Y | x = GΘ (y), y ∈ Y},  B = {(x, y) ∈ X × Y | y = Hx, x ∈ X }  Proof. From the definition of `cycle and `OT 0 , it is straightforward to show that T(Θ, H) ≥ `cycle (Θ, H) + `OT 0 (Θ, H). To show reversed inequality, we need the classical results on optimal transport for the restricted measure (Villani, 2008). Specifically, Theorem 2 in Appendix informs that for some optimal transportation plan π ∗ , the restrictions π ∗ |A∪B and π ∗ |X ×Y\(A∪B) acquire optimality. Therefore, Z T(Θ, H) = c(x, y; Θ, H)dπ ∗ (x, y) ZX ×Y Z = c(x, y; Θ, H)dπ ∗ (x, y) + c(x, y; Θ, H)dπ ∗ (x, y) X ×Y\(A∪B)  A∪B  = `cycle (Θ, H) + `OT 0 (Θ, H) This concludes the proof. The following proposition tells why we name the first term as `cycle . Indeed, the term `cycle (Θ, H) in (11) is the weighted version of cycle consistency term in cycleGAN (Zhu et al., 2017), which imposes the constraint x ' GΘ (Hx), ∀x ∈ X and y ' HGΘ (y), ∀y ∈ Y. However, our formulation is more general, since there exists weighting factors that can balance the importance of the cycle-consistency terms. Proposition 1. Z c(x, y; Θ, H)dπ(x, y) `cycle (Θ, H) := min π X ×Y\A∪B Z = c(x, y; Θ, H)π ∗ (dx, dy) ZA∪B Z = kx − GΘ (Hx)kp ρ(x)µ(dx) + ky − HGΘ (y)kq σ(y)ν(dy) (13) X  Y  where ρ and σ are measurable functions such that 0 ≤ ρ(x) ≤ 1 µ-a.e., and 0 ≤ σ(y) ≤ 1 ν-a.e. Proof. By virtue of the definitions of the cost function and the sets A and B, we have c(x, y; Θ, H) = ky − HGΘ (y)kq on A, c(x, y; Θ, H) = kx − GΘ (Hx)kp on B, c(x, y; Θ, H) = 0 on A ∩ B. Hence, Z  ∗  Z  c(x, y; Θ, H)dπ (x, y) = A∪B  p  ∗  Z  kx − GΘ (Hx)k dπ (x, y) + B  ky − HGΘ (y)kq dπ ∗ (x, y)  A  Furthermore, we use disintegration theorem (Simmons, 2012) to split the joint measure π ∗ as follows: dπ ∗ (x, y) = π ∗ (dy|x)µ(dx) = π ∗ (dx|y)ν(dy), 4  Figure 1: (a) Supervised learning with matched references, (b) optimal transport with absolute continuous measures, and (c) optimal transport having measures with singularities. CycleGAN corresponds to the scenario (c). where π ∗ (Y|x) = 1 µ-a.e. and π ∗ (X |y) = 1 ν-a.e. Then, we have Z Z kx − GΘ (Hx)kp dπ ∗ (x, y) = kx − GΘ (Hx)kp π ∗ (dy|x)µ(dx) B B Z Z kx − GΘ (Hx)kp π ∗ (dy|x)µ(dx) = X y=Hx Z Z p = kx − GΘ (Hx)k 1y=Hx π ∗ (dy|x)µ(dx) X Z = kx − GΘ (Hx)kp ρ(x)µ(dx) X  R where 1S denotes theRindicator function for the set S and ρ(x) = 1y=Hx π ∗ (dy|x). In a similar fashion, with σ(y) = 1x=GΘ (y) π ∗ (dx|y), we have Z Z ∗ c(x, y; Θ, H)dπ (x, y) = ky − HGΘ (y)kq σ(y)ν(dy) . Y  A  This concludes the proof. Here, care should be taken regarding the measurable functions ρ and σ. For the case of supervised learning, there exists perfectly matched reference as shown in Fig. 1(a) so that ρ(x) = 1; µ-a.e. and σ(y) = 1; ν-a.e. In this case, the `OT 0 = 0. On the other hand, if the joint distribution π(x, y) is absolutely continuous as shown in Fig. 1(b), then ρ(x) = 0; µ-a.e. and σ(y) = 0; ν-a.e. In this case, `cycle = 0. Most interesting case arises when the joint measure π(x, y) has singularities on the set of A ∪ B = {y = Hx or x = GΘ (y)} (see Fig. 1(c)). In fact, the unsupervised learning cares about such situation where once the mappings are found, significant portion of the data in X and Y can be paired with the mappings, even though there are still remaining unpaired data. This is when the cycle consistency terms plays the important role in unsupervised learning. The remaining terms in (10), which corresponds to the GAN term, can be obtained as follows: Proposition 2. For p = q = 1, the cost `OT 0 (Θ, H) in (12) can be equivalently represented as Z Z `OT 0 (Θ, H) = max ϕ(x)dµ(x) − ϕ(GΘ (y))dν(y) (14) ϕ ZX ZY + max ψ(y)dν(y) − ψ(Hx)dν(x) (15) ψ  Y  X  where ϕ and ψ are 1-Lipschitz Kantorovich potential functions. Proof. This is a just simple corollary of the original Kantorovich’s duality formulation proof and the classical results of optimal transport on the restricted measure. We start with the first term of lOT (Θ, H) in (12). Specifically, we have Z Z Z p ∗ 0 kGΘ (y) − xk π (dx, dy) = max ϕ(x)µ (dx) + ϕc (GΘ (y))ν 0 (dy) ϕ  X ×Y\(A∪B)  5  X  Y  where µ0 and ν 0 are marginals of the restriction of optimal transportation plan on the restricted set X × Y \ (A ∪ B). Now, using Proposition 3 in Appendix, we have Z Z Z kGΘ (y) − xkπ ∗ (dx, dy) = max ϕ(x)µ(dx) + ϕc (GΘ (y))ν(dy) ϕ X ×Y\(A∪B) X Y Z Z = max ϕ(x)µ(dx) − ϕ(GΘ (y))ν(dy) 1 ϕ∈L (µ)  X  Y  for the non-restricted marginals µ and ν, where for the last equality we use ϕc = −ϕ for p = 1 when ϕ is 1-Lipschitz (Villani, 2008). Using the same technique, the second term in (12) can be equivalently represented as Z Z Z ky − Hxkdπ(x, y) = max ψ(y)dν(y) − ψ(Hx)dν(x). 1 ψ∈L (ν)  X ×Y\(A∪B)  Y  X  By collecting terms, we conclude the proof. Now, the final step is implementing the Kantorovich potential using CNNs with parameters Φ and Ξ, i.e. ϕ := ϕΦ and ψ := ψΞ . By collecting all terms in the transportation cost T(Θ, H) together, the dual formulation results in a cycleGAN cost function: min T(Θ, H) = min max `(Θ, H; Φ, Ξ) Θ,H  (16)  Θ,h Φ,Ξ  where `(Θ, H; Φ, Ξ) = `cycle (Θ, H) + `GAN (Θ, H; Φ, Ξ)  (17)  where `cycle (Θ, H) denotes the cycle consistent loss in (13) and `GAN (Θ, h; Φ, Ξ) is the GAN loss given by: `GAN (Θ, H; Φ, Ξ) Z Z Z Z = ϕΦ (x)dµ(x) − ϕΦ (GΘ (y))dν(y) + ψΞ (y)dν(y) − ψΞ (Hx)dν(x) X  Y  Y  (18)  X  Here, the Kantorovich 1-Lipschitz potential ϕ := ϕΦ and ψ := ψΞ corresponds to the Wasserstein GAN discriminators. Specifically, ϕΦ tries to find the difference between the true image x and the generated image GΘ (y), whereas ψ := ψΞ attempts to find the fake measurement data that are generated by the synthetic measurement procedure Hx. Furthermore, if ρ and σ are constant almost everywhere, then they become simple scaling factors for the cycle-consistency terms: Z Z `cycle (Θ, H) = ρ kx − GΘ (Hx)kp dµ(x) + σ ky − HGΘ (y)kq dν(y) (19) X  Y  Note that our cost formulation using (17) with (18) and (19) is more general compared to the standard cycleGAN, since a general form of measurement data generator Hx can be used. In the following section, we will show how to choose H depending on specific applications.  4 4.1  A PPLICATIONS TO I NVERSE P ROBLEMS ACCELERATED MRI  In accelerated MRI, the goal is to recover high quality MR images from sparsely sampled k-space data to reduce the acquisition time. This problem has been extensively studied using compressed sensing (Lustig et al., 2007), but recently deep learning approaches have been the main research interest due to its excellent performance at significantly reduced run time complexity (Hammernik et al., 2018; Han & Ye, 2019). A standard method for neural network training for accelerated MRI is based on the supervised learning, where the MR images from fully sampled k-space data is used as reference and subsampled k-space data is used as the input for the neural network. Unfortunately, in accelerated magnetic resonance imaging (MRI), high-resolution fully sampled k-space data is very difficult to acquire due to the long scan time. Therefore, the need for unsupervised learning without matched reference data is increasing. 6  In accelerated MRI, the forward measurement model can be described as x̂ = PΩ Fx + w  (20)  where F is the 2-D Fourier transform and PΩ is the projection to Ω that denotes k-space sampling indices. To implement every step of the algorithm as the image domain processing, (20) can be converted to the image domain forward model by applying the inverse Fourier transform: y = F −1 PΩ Fx + F −1 w  (21)  This results in the following cost function for the PLS formulation: c(x, y; Θ) = ky − F −1 PΩ Fxk + kGΘ (y) − xk  (22)  Usually, the sampling mask Ω is known so that the forward mapping for the inverse problem is deterministic.  Figure 2: Proposed cycleGAN architecture for accelerated MRI with 1-D downsampling patterns. The schematic diagram of the associated cycleGAN architecture is illustrated in Fig. 2, whose generator and discriminator architecture are shown in Fig. 6 in Appendix. Note that we just need as single generator, as the mapping from the clean to aliased images is deterministic for a given sampling pattern. As for the loss function, we use (17) with the σ = ρ = 0.5 for the cycle consistency  Figure 3: Unsupervised learning results for accelerated MRI using proposed cycleGAN. The values in the corner are PSNR/SSIM values. 7  term in (19). For GAN implementation, we use the Wasserstein GAN in (18) with Lipschitz penalty loss (Gulrajani et al., 2017) to enforce that the Kantorovich potential become 1-Lipschitz. The detailed description of the training procedure is provided in Appendix. The reconstruction results in Fig. 3 clearly showed that the proposed unsupervised learning method using dedicated cycleGAN successfully recovered fine details without matched references. 4.2  D ECONVOLUTION M ICROSCOPY  Deconvolution microscopy is extensively used to improve the resolution of the widefield fluorescent microscopy. Recently, CNN approaches have been extensively studied as fast and high performance alternatives. Unfortunately, the CNN approaches usually require matched high resolution images for supervised training. Mathematically, a blurred measurement can be described as y  = h∗x+w,  (23)  where h is the point spread function (PSF). If the PSF h is not known, which is often referred to as the blind deconvolution problem, both the unknown PSF h and the image x should be estimated. This results in the following cost function for the PLS formulation: c(x, y; Θ, h) = ky − h ∗ xk + kGΘ (y) − xk  (24)  The corresponding cycleGAN architecture is illustrated in Fig. 4. In contrast to the conventional cycleGAN approaches that require two generators, the proposed cycleGAN approach needs only a single generator and the blur image generation can be done using a linear convolution layer corresponding to PSF, which significantly improves the robustness of network training. The detailed description of the network architecture and training are given in Appendix.  Figure 4: Proposed cycleGAN architecture with a blur kernel for deconvolution microscopy. Here, GΘ denotes the CNN-based low-resolution to high-resolution generator. The blur generator is composed of a linear blur kernel h. Fig. 5 show lateral views of deconvolution results of microtubule samples by various methods. Here, input images are degraded by blur and noises. The supervised learning and the standard cycleGAN with two generators showed better contrast and removed blur; however, the structural continuity was not preserved. On the other hand, in our cycleGAN approach, blurs and noise were successfully removed and preserved the continuity of the structure.  5  C ONCLUSIONS  In this paper, we presented a general design principle for cycleGAN architecture for various inverse problems. Specifically, the proposed architecture was derived as a dual formulation of an optimal transport problem that uses the penalized least squares cost as the transport cost between two measures supported on the measurement data and the images. As proofs of concept, we designed novel cycleGAN architecture for accelerated MRI and deconvolution microscopy examples, providing accurate reconstruction results without any matched reference data. Given the generality of our design 8  Figure 5: Comparison of reconstruction results by various methods: (a) Blurred image measurements, (b) supervised learning, (c) the conventional cycleGAN, and (d) the proposed cycleGAN. The ROIs (marked yellow) show the area for the enlarged parts. principle, we believe that our method can be an important platform for unsupervised learning for inverse problems.  R EFERENCES Hemant K Aggarwal, Merry P Mani, and Mathews Jacob. MoDL: Model-based deep learning architecture for inverse problems. IEEE transactions on medical imaging, 38(2):394–405, 2018. Martin Arjovsky, Soumith Chintala, and Léon Bottou. arXiv:1701.07875, 2017.  Wasserstein GAN.  arXiv preprint  Subhasis Chaudhuri, Rajbabu Velmurugan, and Renu Rameshan. Blind deconvolution methods: A review. In Blind Image Deconvolution, pp. 37–60. Springer, 2014. Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D UNet: learning dense volumetric segmentation from sparse annotation. In International conference on medical image computing and computer-assisted intervention, pp. 424–432. Springer, 2016. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672–2680, 2014. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of Wasserstein GANs. In Advances in neural information processing systems, pp. 5767–5777, 2017. Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P Recht, Daniel K Sodickson, Thomas Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI data. Magnetic resonance in medicine, 79(6):3055–3071, 2018. Yoseob Han and Jong Chul Ye. k-Space Deep Learning for Accelerated MRI. IEEE Transactions on Medical Imaging (in press), also available as arXiv preprint arXiv:1805.03779, 2019. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1125–1134, 2017. Eunhee Kang, Junhong Min, and Jong Chul Ye. A deep convolutional neural network using directional wavelets for low-dose X-ray CT reconstruction. Medical Physics, 44(10), 2017. Eunhee Kang, Hyun Jung Koo, Dong Hyun Yang, Joon Bum Seo, and Jong Chul Ye. Cycleconsistent adversarial denoising network for multiphase coronary CT angiography. Medical Physics, 46(2):550–562, 2019. Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 9  Yongyi Lu, Yu-Wing Tai, and Chi-Keung Tang. Conditional cyclegan for attribute guided face image generation. arXiv preprint arXiv:1705.09966, 2017. Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The application of compressed sensing for rapid MR imaging. Magn. Reson. Med., 58(6):1182–1195, 2007. James G McNally, Tatiana Karpova, John Cooper, and José Angel Conchello. Three-dimensional imaging by deconvolution microscopy. Methods, 19(3):373–385, 1999. Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends in Machine Learning, 11(5-6):355–607, 2019. Pinaki Sarder and Arye Nehorai. Deconvolution methods for 3-D fluorescence microscopy images. IEEE Signal Processing Magazine, 23(3):32–45, 2006. David Simmons. Conditional measures and conditional expectation; rohlin’s disintegration theorem. Discrete & Continuous Dynamical Systems-A, 32(7):2565–2582, 2012. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Cédric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008. Jure Zbontar, Florian Knoll, Anuroop Sriram, Matthew J Muckley, Mary Bruno, Aaron Defazio, Marc Parente, Krzysztof J Geras, Joe Katsnelson, Hersh Chandarana, et al. fastMRI: An open dataset and benchmarks for accelerated MRI. arXiv preprint arXiv:1811.08839, 2018. Jiawei Zhang, Jinshan Pan, Wei-Sheng Lai, Rynson WH Lau, and Ming-Hsuan Yang. Learning fully convolutional networks for iterative non-blind deconvolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3817–3825, 2017. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 2223–2232, 2017.  A A.1  A PPENDIX A DDITIONAL T HEOREMS AND L EMMAS  Theorem 2 (Optimality is inherited by restriction). (Villani, 2008, Theorem 4.6, p.46) Under the same assumption of Kantorovich duality theorem, let π ∗ be an optimal transport plan, and π̃ be a nonnegative measure on X × Y, such that π̃ ≤ π ∗ and π̃[X × Y] > 0. Let π 0 := π̃[Xπ̃×Y] , µ0 , ν 0 be the maginals of π 0 . Then optimality is inherited to its restriction, i.e., π 0 is an optimal transference plan between µ0 and ν 0 . Theorem 3 (Restriction for the Kantorovich duality). (Villani, 2008, Theorem 5.19, p.75-p.76) Under the same assumption of Kantorovich duality theorem, let π ∗ be an optimal transport plan and ϕ∗ be an optimal potential that attain the maximum, and π̃ be a measure on X × Y, such that 0 ≤ π̃ ≤ π ∗ and Z := π̃[X × Y] > 0. Let π 0 = π̃/Z, and µ0 , ν 0 be the marginals of π 0 . Then there exists ϕ0 such that ϕ0 = ϕ∗ on projX (supp(π ∗ )), and ϕ0 solves the dual Kantorovich problem between (X , µ0 ) and (Y, ν 0 ). We are now ready to provide our duality result for the restricted measure. Proposition 3. Let Z `OT 0  :=  min π  c(x, y)dπ(x, y) X ×Y\(A∪B)  Z `OT  :=  min π  c(x, y)dπ(x, y) X ×Y  10  Then, we can replace `OT 0 with `OT in the sense that both of them share the same optimal Kantorovich potential. More specifically, if ϕ∗ is the optimal Kantorovich potential, which satisfies Z Z `OT = ϕ∗ (x)dµ(x) + (ϕ∗ )c (y)dν(y), X  then  Z `OT 0 =  Y  ϕ∗ (x)dµ0 (x) +  X  Z  (ϕ∗ )c (y)dν 0 (y),  Y  where µ0 and ν 0 are marginals of the restriction of optimal transference plan. Proof. Let π̃ = π ∗ |X ×Y\(A∪B) . Then, π̃ is nonnegative measure on X × Y. When π̃(X × Y) = 0, `OT 0 = 0, and µ0 = 0, ν 0 = 0, so the statement trivially holds. Suppose π̃(X × Y) > 0. Then by lemma, there exists ϕ0 such that ϕ0 = ϕ∗ on projX (supp(π ∗ )), and ϕ0 solves the dual Kantorovich problem between (X , µ0 ) and (Y, ν 0 ). A.2 A.2.1  E XPERIMENTAL D ETAILS ACCELERATED MRI  We use dataset for fastMRI challenge (Zbontar et al., 2018) for our experiments. This dataset is composed of MR images of knees. We extracted 3500 MR images from fastMRI single coil validation set. Then, 3000 slices are used for training/evaultion, and 500 slices are used for test. These MR images are fully sampled images, so we make undersampled images by a randomly subsampling k-space lines. The acceleration factor is four, and autocalibration signal (ACS) region contains 4% of k-space lines. Each slice is normalized by standard deviation of the magnitude of each slice. To handle complex values of data, we concatenate real and imaginary values along the channel dimension. Each slice has different size, so we use only single batch. The images are center-cropped to the size of 320 × 320, and then the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values are calculated. We use U-Net generator to reconstruct fully sampled MR images from undersampled MR images as shown in Fig. 6(a). Our generator consists of 3 × 3 convolution, Instance normalization, and leaky ReLU operation. Also, there are skip-connection and pooling layers. At the last convolution layer, we do not use any operation. Our discriminator is same as the discriminator of original CycleGAN. We use PatchGAN (Isola et al., 2017), so the discriminator classifies inputs at patch scales. The discriminator also consists of convolution layer, instance normalization, and leaky ReLU operation as shown in Fig. 6(b). We use Adam optimizer to train our network, with momentum parameters β1 = 0.5, β2 = 0.9, and learning rate of 0.0001. The discriminator is updated 5 times for every generator updates. We use batch size of 1, and trained our network during 100 epochs. Our code was implemented by TensorFlow. A.2.2  D ECONVOLUTION MICROSCOPY  The network architecture of the high resolution image generator GΘ from the low-resolution image is a modified 3D-Unet (Çiçek et al., 2016) as shown in Fig 7. Our U-net structure consists of contracting and expanding paths. The contracting path consists of the repetition of the following blocks: 3D conv- Instance Normalization (Ulyanov et al., 2016)- ReLU. Here, the generator has symmetric configuration so that both encoder and decoder have the same number of layers, i.e. κ = 7. Throughout the network, the convolutional kernel dimension is 3 × 3 × 3. There exists a pooling layer and skipped connection for every other convolution operations. To enhance the image contrast, we add additional sigmoid layer at the end of U-Net. On the other hand, the low-resolution image generator from high resolution input is based on a single 3D convolution layer that models a 3D blurring kernel. The size of the 3D PSF modeling layer is chosen depending on the problem set by considering their approximate PSF sizes. In this paper, the size of the 3D PSF layer is set to 20 × 20 × 20. As for the discriminators, we follow the original CycleGAN that uses multi-PatchGANs (mPGANs) (Isola et al., 2017), where each discriminator has input patches with different sizes used. As shown in Fig 8, it consist of three independent discriminators. Each discriminator takes patches at different sizes: original, and half, and quarter size patches. 11  (a)  (b) Figure 6: Proposed network architecture for (a) generator and (b) discriminator.  Figure 7: A modified 3D U-net architecture for our high-resolution image generator.  Figure 8: Multi-PatchGANs discriminator architecture. A total 18 epi-fluorescent (EPF) microscopy images of tubulin with a size of 512 × 512 × 30 were used for training, and one for validation. As for unmatched sharp image volume, we used deblurred image generated by utilizing a commercial software AutoQuant X3 (Media Cybernetics, Rockville). The EPF volume depth was increased to 64 using the reflection boundary condition. Due to GPU memory limitations, the EPF volume was split into 64×64×64 size patches. For data augmentation, rotation, flip, translation, and scale were imposed on the input patches. We normalized the patches and set them to [0,1]. Adam optimizer (Kingma & Ba, 2014) was also used for training. The learning rate was initially set to 0.0001, which is decreased linearly after 40 epoch, and the total number of epoch was 200 epoch. For the optimizer, we used only a single batch.  12  "
"Generalized Planning: Non-Deterministic Abstractions and Trajectory Constraints  arXiv:1909.12135v1 [cs.AI] 26 Sep 2019  Blai Bonet Univ. Simón Bolı́var Caracas, Venezuela bonet@ldc.usb.ve  Giuseppe De Giacomo Sapienza Univ. Roma Rome, Italy degiacomo@dis.uniroma1.it  1  Introduction  Generalized planning, where a single plan works for multiple domains, has been drawing sustained attention in the AI community [Levesque, 2005; Srivastava et al., 2008; Bonet et al., 2009; Srivastava et al., 2011a; Hu and Levesque, 2010; Hu and De Giacomo, 2011; Bonet and Geffner, 2015; Belle and Levesque, 2016]. This form of planning has the aim of finding generalized solutions that solve in one shot an entire class of problems. For example, the policy “if block x is not clear, pick up the clear block above x and put it on the table” is general in the sense that it achieves the goal clear(x) for many problems, indeed any block world instance. In this work, we study the characterization and computation of such policies for families of problems that share a common structure. This structure has been characterized as made of two parts: a common pool of actions and observations [Hu and De Giacomo, 2011], and a common structural reduction into a single abstract problem P that is nondeterministic even if the concrete problems Pi are deterministic [Bonet and Geffner, 2015]. Policies µ that solve the ab-  Hector Geffner ICREA Univ. Pompeu Fabra Barcelona, Spain hector.geffner@upf.edu  Sasha Rubin Univ. Federico II Naples, Italy rubin@unina.it  stract problem P have been shown to solve any such problem Pi provided that they terminate in Pi . In this work, we shed light on why this termination condition is needed and how it can be removed. The key observation is that the abstract problem P captures the common structure among the concrete problems Pi that is local (Markovian) but misses common structure that is global. We show nonetheless that such global structure can be accounted for by extending the abstract problems with trajectory constraints; i.e., constraints on the interleaved sequences of actions and observations or states that are possible. For example, a trajectory constraint may state that a non-negative numerical variable X will eventually have value zero in trajectories where it is decreased infinitely often and increased finitely often. Similarly, a trajectory constraint can be used to express fairness assumptions; namely, that infinite occurrences of an action, must result in infinite occurrences of each one of its possible (non-deterministic) outcomes. The language of the partially observable non-deterministic problems extended with trajectory constraints provides us with a powerful framework for analyzing and even computing general policies that solve infinite classes of concrete problems. In the following, we first lay out the framework, discuss the limitations of earlier work, and introduce projections and trajectory constraints. We then show how to do generalized planning using LTL synthesis techniques, and for a specific class of problems, using efficient planners for fully observable non-deterministic problems.  2  Framework  The framework extends the one by Bonet and Geffner [2015].  2.1  Model  A partially observable non-deterministic problem (PONDP) is a tuple P = hS, I, Ω, Act, T, A, obs, F i where 1. 2. 3. 4. 5. 6. 7. 8.  S is a set of states s, I ⊆ S is the set of initial states, Ω is the finite set of observations, Act is the finite set of actions, T ⊆ S is the set of goal states, A : S → 2Act is the available-actions function, obs : S → Ω is the observation function, and F : Act × S → 2S \ {∅} is the partial successor function with domain {(a, s) : a ∈ A(s)}.  In this work we assume observable goals and observable action-preconditions; these assumptions are made uniformly when we talk about a class of PONDPs P: Definition 1. A class P of PONDPs consists of a set of PONDPs P with 1) the same set of actions Act, 2) the same set of observations Ω, 3) a common subset TΩ ⊆ Ω of goal observations such that for all P ∈ P, s ∈ T if and only if obs(s) ∈ TΩ , and 4) common subsets of actions Aω ⊆ Act, one for each observation ω ∈ Ω, such that for all P in P and s in P , A(s) = Aobs(s) . Example. Consider the class P of problems that involve a single non-negative integer variable X, initially positive, and an observation function obs that determines if X is 0 or not (written X = 0 and X 6= 0). The actions Inc and Dec increment and decrement the value of X by 1 respectively, except when X = 0 where Dec has no effect. The goal is X = 0. The problems in P differ on the initial positive value for X that may be known, e.g., X = 5, or partially known, e.g., X ∈ [5, 10]. PONDPs are similar to Goal POMDPs [Bonet and Geffner, 2009] except that uncertainty is represented by sets of states rather than by probabilities. Deterministic sensing is not restrictive [Chatterjee and Chmelı́k, 2015]. For fully observable non-deterministic problems (FONDP) the observation function obs(s) = s is usually omitted. The results below apply to fully observable problems provided that Ω is regarded as a set of state features, and obs(·) as the function that maps states into features.  2.2  Solutions  Let P = hS, I, Ω, Act, T, A, obs, F i be a PONDP. A stateaction sequence over P is a finite or infinite sequence of the form hs0 , a0 , s1 , a1 , · · ·i where each si ∈ S and ai ∈ Act. Such a sequence reaches a state s if s = sn for some n ≥ 0, and is goal reaching in P if it reaches a state in T . An observation-action sequence over P is a finite or infinite sequence of the form hω0 , a0 , ω1 , a1 , · · ·i where each ωi ∈ Ω and ai ∈ Act. Let us extend obs over state-action sequences: for τ = hs0 , a0 , s1 , a1 , . . .i, define obs(τ ) = hobs(s0 ), a0 , obs(s1 ), a1 , . . .i. A trajectory of P is a stateaction sequence over P such that s0 ∈ I, and for i ≥ 0, ai ∈ A(si ) and si+1 ∈ F (ai , si ). An observation trajectory of P is an observation-action sequence of the form obs(τ ) where τ is a trajectory of P . A policy is a partial function µ : Ω+ → Act where + Ω is the set of finite non-empty sequences over the set Ω. A policy µ is valid if it selects applicable actions, i.e., if hs0 , a0 , s1 , a1 , . . .i is a trajectory, then for the observation sequence xi = hobs(s0 ), . . . , obs(si )i, µ(xi ) is undefined, written µ(xi ) = ⊥, or µ(xi ) ∈ A(si ). A policy that depends on the last observation only is called memoryless. We write memoryless policies as partial functions µ : Ω → Act. A trajectory τ = hs0 , a0 , s1 , a1 , . . .i is generated by µ if µ(obs(s0 ) . . . obs(sn )) = an for all n ≥ 0 for which sn and an are in τ . A µ-trajectory of P is a maximal trajectory τ generated by µ, i.e., either a) τ is infinite, or b) τ is finite and cannot be extended with µ; namely, τ = hs0 , a0 , s1 , a1 , . . . , sn−1 , an−1 , sn i and  µ(obs(s0 ) . . . obs(sn )) = ⊥. If µ is a valid policy and every µ-trajectory is goal reaching then we say that µ solves (or is a solution of) P . A transition in P is a triplet (s, a, s0 ) ∈ S × Act × S such that s0 ∈ F (a, s). A transition (s, a, s0 ) occurs in a trajectory hs0 , a0 , s1 , a1 , . . .i if for some i ≥ 0, s = si , a = ai , and s0 = si+1 . A trajectory τ is fair if a) it is finite, or b) it is infinite and for all transitions (s, a, s0 ) and (s, a, s00 ) in P for which s0 6= s00 , if one transition occurs infinitely often in τ , the other occurs infinitely often in τ as well. If µ is a valid policy and every fair µ-trajectory is goal reaching then we say that µ is a fair solution to P [Cimatti et al., 2003].  2.3  The Observation Projection Abstraction  Bonet and Geffner [2015] introduce reductions as functions that can map a set of “concrete PONDPs problems” P into a single, often smaller, abstract PONDP P 0 that captures part of the common structure. A general way for reducing a class of problems into a smaller one is by projecting them onto their common observation space: Definition 2. Let P be a class of PONDPs (over actions Act, observations Ω, and the set of goal observations TΩ ). Define the FONDP P o = hS o , I o , Acto , T o, Ao , F o i, called the observation projection of P, where H1. H2. H3. H4. H5. H6.  S o = Ω, ω ∈ I o iff there is P ∈ P and s ∈ I s.t. obs(s) = ω, Acto = Act, T o = TΩ , Ao (ω) = Aω for every ω ∈ Ω, for a ∈ Ao (ω), define ω 0 ∈ F o (a, ω) iff there exists P ∈ P, (s, a, s0 ) in F s.t. obs(s) = ω and obs(s0 ) = ω 0 .  This definition generalises the one by Bonet and Geffner [2015] which is for a single PONDP P , not a class P. Example (continued). Recall the class P of problems with the non-negative integer variable X. Their observation projection P o is the FONDP with two states, that correspond to the two possible observations, and which we denote as X > 0 and X = 0. The transition function F o in P o for the action Inc maps the state X = 0 into X > 0, and the state X > 0 into itself; and for the action Dec it maps the state X = 0 into itself, and X > 0 into both X > 0 and X = 0. The initial state in P o is X > 0 and the goal state is X = 0. The following is a direct consequence of the definitions: Lemma 3. Let P be a class of PONDPs and let τ be a trajectory of some P ∈ P. Then (1) τ is goal reaching in P iff the observation-action sequence obs(τ ) is goal reaching in P o . Furthermore, if µ is a valid policy for P o , then (2) µ is a valid policy for P and (3) if τ is a µ-trajectory then obs(τ ) is a µ-trajectory of P o .  2.4  General Policies  A main result of Bonet and Geffner (2015, Theorem 5) is that: Theorem 4. Let µ be a fair solution for the projection P o of the single problem P . If all the µ-trajectories in P that are fair are also finite, then µ is a fair solution for P .  One of the main goals of this work is to remove the termination (finiteness) condition so that a policy µ that solves the projection P o of a class of problems P will generalize automatically to all problems in the class. For this, however, we will need to extend the representation of abstract problems. Example (continued). The policy µ that maps the observation X > 0 into the action Dec (namely, “decrease X if positive”) is a fair solution to P o , and to each problem P in the class P above. The results of Bonet and Geffner [2015], however, are not strong enough for establishing this generalization. What they show instead is that the generalization holds provided that µ terminates at each problem P ∈ P.  3  Trajectory Constraints  In the example, the finiteness condition is required by Theorem 4 because there are other problems, not in the class P, that also reduce to P o but which are not solved by µ. Indeed, a problem P 0 like any of the problems in P but where the Dec action increases X rather than decreasing it when X = 5 is one such problem: one that cannot be solved by µ and one on which µ does not terminate (when, initially, X ≥ 5). These other problems exist because the abstraction represented by the non-deterministic problem P o is too weak: it captures some properties of the problems in P but misses properties that should not be abstracted away. One such property is that if a positive integer variable keeps being decreased, while being increased a finite number of times only, then the value of the variable will infinitely often reach the zero value.1 This property is true for the class of problems P but is not true for the problem P 0 , and crucially, it is not true in the projection P o . Indeed, the projection P o captures the local (Markovian) properties of the problems in P but not the global properties. Global properties involve not just transitions but trajectories. We will thus enrich the language of the abstract problems by considering trajectory constraints C, i.e., restrictions on the set of possible observation or state trajectories. The constraint CX that a non-negative variable X decreased infinitely often and increased finitely often, will infitenly often reach the value X = 0 is an example of a trajectory constraint. Another example is the fairness constraint CF ; namely, that infinite occurrences of a non-deterministic action in a state imply infinite occurrences of each one of its possible outcomes following the action in the same state.  3.1  Strong Generalization over Projections  Formally, a trajectory constraint C over a PONDP P is a set of infinite state-action sequences over P or a set of infinite observation-action sequences over P . A trajectory τ satisfies C if either a) τ is finite, or b) if C is a set of state-action sequences then τ ∈ C, and if C is a set of observation-action sequences then obs(τ ) ∈ C. Thus trajectory constraints only constrain infinite sequences. A PONDP P extended with a set of trajectory constraints C, written P/C, is called a PONDP with constraints. Solutions of P/C are defined as follows: 1  The stronger consequent of eventually reaching the zero value and staying at it also holds for the problems in P. However, we prefer to use this weaker condition as it is enough for our needs.  Definition 5 (Solution of PONDP with constraints). A policy µ solves P/C iff µ is valid for P and every µ-trajectory of P that satisfies C is goal reaching. This notion of solution does not assume fairness as before (as in strong cyclic solutions); instead, it requires that the goal be reached along all the trajectories that satisfy the constraints. Still if C = CF is the fairness constraint above, a fair solution to P is nothing else than a policy µ that solves P extended with CF ; i.e. P/CF . When C contains all trajectories, the solutions to P and P/C coincide as expected. We remark that a trajectory constraint C over the observation projection P o of P is a trajectory constraint over every P in P. The next theorem says that a policy that solves P o /C also solves every P/C. Theorem 6 (Generalization with Constraints). If P o is the observation projection of a class of problems P and C is a trajectory constraint over P o , then a policy that solves P o /C also solves P/C for all P ∈ P. Proof. Suppose that µ solves P o /C. Let P be a problem in P. By Lemma 3 (2), µ is a valid policy for P ∈ P. To see that µ solves P/C, take a trajectory τ of P that satisfies C, i.e., obs(τ ) ∈ C. By Lemma 3 (3), obs(τ ) is a µ-trajectory of P o . By assumption obs(τ ) is goal reaching. By Lemma 3 (1), τ is goal reaching. Say that P satisfies the constraint C if all infinite trajectories in P satisfy C. Then, an easy corollary is that: Corollary 7. Let P o be the observation projection of a class P of PONDPs, let C be a trajectory constraint over P o , and let P be a problem in P. If P satisfies C, then a policy that solves P o /C also solves P . More generally, if C and C 0 are constraints over P , say that C implies C 0 if every infinite trajectory over P that satisfies C also satisfies C 0 . We then obtain that: Corollary 8. Let P o be the observation projection of a class P of PONDPs, and let C 0 be a trajectory constraint over P o . If P is a problem in P, C is a constraint over P , and C implies C 0 , then a policy that solves P o /C 0 also solves P/C. Trajectory constraints in P o are indeed powerful and can be used to account for all the solutions of a class P. Theorem 9 (Completeness). If P o is the observation projection of a class of problems P, and µ is a policy that solves all the problems in P, then there is a constraint C over P o such that µ solves P o /C. Sketch. Define C to be the set of observation sequences of the form obs(τ ) where τ varies over all the trajectories of all P ∈ P. Suppose µ solves every P in P. First, one can show that µ is valid in P o using the assumption that action preconditions are observable (i.e., Ao (ω) = Aω = A(s) for all s such that obs(s) = ω). Second, to see that µ solves P o /C take a µ-trajectory τ of P o that satisfies C. By definition of C, there is a trajectory τ 0 in some P ∈ P such that obs(τ 0 ) = τ . It is not hard to see that τ 0 is a µ-trajectory of P . Thus, since τ 0 is goal-reaching and goals are uniformly observable (Definition 1), then τ is goal reaching.  Example (continued). The problems P above with integer variable X and actions Inc and Dec and goal X = 0, are all solved by the policy µ “if X > 0, do Dec”. However, the observation projection P o is not solved by µ as there are trajectories where the outcome of the action Dec in P o , with non-deterministic effects X > 0 | X = 0, is always X > 0. Bonet and Geffner [2015] deal with this by taking µ as a fair solution to P and then proving termination of µ in P (a similar approach is used by Srivastava et al. [2011b]). The theorem and corollary above provide an alternative. The policy µ does not solve P o but solves P o with constraint CX , where CX is the trajectory constraint that states that if the action Dec is done infinitely often and the action Inc is done finitely often then infinitely often X = 0. Theorem 6 implies that µ solves P/CX for every P in the class. Yet, since P satisfies CX , Corollary 7 implies that µ must solve P too. The generalization also applies to problems P where increases and decreases in the variable X are non-deterministic as long as no decrease can make X negative. In such a case, if CF is the trajectory constraint over P that says that non-deterministic actions are fair, from the fact that CF implies CX in any such problem P , Corollary 8 implies that a policy µ that solves P o /CX must also solve P/CF ; i.e., the strong solution to the abstraction P o over the constraint CX , represents a fair solution to such non-deterministic problems P .  4  Generalized Planning as LTL Synthesis  Suppose that we are in the condition of Theorem 6. That is we have a class of problems P whose observation projection is P o , and a constraint C over P o . Let’s further assume that C is expressible in Linear-time Temporal Logic (LTL). Then we can provide an actual policy that solves P o /C, and hence solves P/C for all P ∈ P. We show how in this section. For convenience, we define the syntax and semantics of LTL over a set Σ of alphabet symbols (rather than a set of atomic propositions). The syntax of LTL is defined by the following grammar: Ψ ::= true | l | Ψ ∧ Ψ | ¬Ψ | ◦ Ψ | Ψ U Ψ, where l ∈ Σ. We denote infinite strings α ∈ Σω by α = α0 α1 · · · , and write α≥j = αj αj+1 . . . . The semantics of LTL, α |= Ψ, is defined inductively as follows: α |= true; α |= l iff α0 = l; α |= Ψ1 ∧ Ψ2 iff α |= Ψi , for i = 1, 2; α |= ¬Ψ iff α 6|= Ψ; α |= ◦ Ψ iff α≥1 |= Ψ; and α |= Ψ1 U Ψ2 iff there exists j such that α≥j |= Ψ2 and for all i < j we have that α≥i |= Ψ1 . We use the usual shorthands, e.g., ♦ Φ for true U Φ (read “eventually”) and  Φ for ¬ ♦ ¬Φ (read “always”). Define mod(Ψ) = {α ∈ Σω : α |= Ψ}. Assume that the trajectory constraint C is expressed as an LTL formula Ψ, i.e., Σ = Ω ∪ Act and mod(Ψ) = C. Let . Φ = Ψ ⊃ ♦ T o where ♦ T o is the reachability goal of P o expressed in LTL. To build a policy solving P o /C proceed as follows. The idea is to think of policies µ : Ω+ → Act as (Ωbranching Act-labeled) trees, and to build a tree-automaton accepting those policies such that every µ-trajectory satisfies the formula Φ. Here are the steps: 1. Build a nondeterministic Büchi automaton Ab for the formula Φ (exponential in Φ) [Vardi and Wolper, 1994]. 2. Determinize Ab to obtain a deterministic parity word automaton (DPW) Ad that accepts the models of Φ  X!=0 X!=0  1  Inc  3 Inc  Dec  Inc Dec Dec Inc  X!=0 X=0 Dec  1  X=0 X!=0  2  all  X=0  2 X=0  Figure 1: DPW (with priorities written in the states) for [♦  ¬Inc ∧  ♦ Dec ⊃  ♦(X = 0)] ⊃ ♦(X = 0).  (exponential in Ab , and produces linearly many priorities) [Piterman, 2007]. An infinite word α is accepted by a DPW A iff the largest priority of the states visited infinitely often by α is even. 3. Build a deterministic parity tree automaton At that accepts a policy µ iff every µ-trajectory satisfies Φ (polynomial in Ad and P o , and no change in the number of priorities). This can be done by simulating P o and Ad as follows: from state (s, q) (of the product of P and Ad ) and reading action a, launch for each s0 ∈ F (a, s) a copy of the automaton in state (s0 , q 0 ) in direction s0 where q 0 is the updated state of Ad . 4. Test At for non-emptiness (polynomial in At and exponential in the number of priorities of At ) [Zielonka, 1998]. This yields the following complexity (the lower-bound is inherited from Pnueli and Rosner [1989]): Theorem 10. Let P o /C be the observation projection with trajectory constraint C expressed as the LTL formula Ψ. Then solving P o /C (and hence all P/C with P ∈ P) is 2EXPTIME-complete. In particular, it is double-exponential in |Ψ| + |T o | and polynomial in |P o |. This is a worst-case complexity. In practice, the automaton . At may be small also in the formula Φ = Ψ ⊃ ♦ T o . Example (continued). We express CX in LTL. The constraint LTL formula ΨX (over alphabet {X = 0, X 6= 0} ∪ {Inc, Dec}) is ♦  ¬Inc ∧  ♦ Dec ⊃  ♦(X = 0) . . Then, mod(ΨX ) = CX . The resulting formula Φ = ΨX ⊃ ♦(X = 0) generates a relatively small automaton: the DPW Ad has 5 states and 3 priorities, see Figure 1. As a further example, consider the case of N ∈ N variables. Each variable Xi has a formula Ψi analogous to ΨX . Consider the constraint Ψ = ∧i Ψi . The above algorithm gives us, as worstO(N ) . case, DPW for Φ = Ψ ⊃ ♦(X = 0) of size 22 with O(N ) 2 many priorities. However, analogously to Figure 1, there is a DPW of size 2O(N ) with 3 priorities.  5  Qualitative Numerical Problems  To concretize, we consider a simple but broad class of problems where generalized planning can be reduced to FOND planning. These problems have a set of non-negative numeric variables, that do not have to be integer, and standard boolean propositions and actions that can increase or decrease  the value of the numeric variables non-deterministically. The general problem of stacking a block x on a block y in any blocks-world problem, with any number of blocks in any configuration, can be cast as a problem of this type. An abstraction of some of these problems appears in [Srivastava et al., 2011b; Srivastava et al., 2015]. A qualitative numerical problem or QNP is a tuple RV = hF, Init, Act, G, V, InitV , IN C, DECi where the first four components define a STRIPS planning problem extended with atoms X = 0 and X > 0 for numerical variables X in a set V , that may appear as action preconditions and goals, sets InitV (X) of possible initial values for each variable X ∈ V , and effect descriptors Dec(X) and Inc(X), with a semantics given by the functions IN C and DEC that take a variable X, an action a, and a state s, and yield a subset IN C(X, a, s) and DEC(X, a, s) of possible values x0 of X in the successor states. If these ranges contain a single number, the problem is deterministic, else it is nondeterministic. If x is the value of X in s, the only requirement is that for all x0 ∈ DEC(x, a, s), 0 ≤ x0 ≤ x, and that for all x0 ∈ IN C(X, a, s), x0 > x when x = 0 and x0 ≥ x when x > 0.2 All the propositions in F are assumed to be observable, while for numerical variables X, only the booleans X = 0 and X > 0 are observable. A QNP RV = hF, Init, Act, G, V, InitV , IN C, DECi represents a PONDP PV in syntactic form. If sV is a valuation over the variables in V , sV [X] is the value of X in sV , and b[sV ] its boolean projection; i.e. b[sV ][X] = 0 if sV [X] = 0 and b[sV ][X] = 1 otherwise. The PONDP PV corresponds to the tuple hS, I, Ω, Act, T, A, obs, F i where 1. S is the set of valuations hsF , sV i over the variables in F and V respectively, 2. I is the set of pairs hsF , sV i where sF is uniquely determined by Init and sV [X] ∈ InitV (X) for each X ∈ V , 3. Ω = {hsF , b[sV ]i : hsF , sV i ∈ S}, 4. Act is the finite set of actions given, 5. T ⊆ S is the set of states that satisfy the atoms in G, 6. A(s) is the set of actions whose precondition are true in s, 7. obs(s) = hsF , b[sV ]i for s = hsF , sV i, 8. F (a, s) for s = hsF , sV i is the set of states s0 = hs0F , s0V i where s0F is the STRIPS successor of a in s, and s0V [X] is in IN C(X, a, s), DEC(X, a, s), or {sV [X]} according to whether a features an Inc(X) effect, a Dec(X) effect, or none. An example is RV = hF, Init, Act, G, V, InitV , IN C, DECi where F is empty, V = {X, Y }, InitV (X) = {20} and InitV (Y ) = {30}, the goal is {X = 0, Y = 0}, and Act contains two actions: an action a with effects Dec(X) and Inc(Y ), and an action b with effect Dec(Y ). The functions IN C always increases the value by 1 and DEC decreases the value by 1 except when it is less than 1 when the value is decreased to 0. A policy µ that solves RV is “do a if X > 0 and Y = 0, and do b if Y > 0”. Simple variations of RV can be obtained by changing the initial situation, e.g., setting it to InitV (X) = [10, 20] and InitV (Y ) = [15, 30], or the IN C 2 Inc(X) effects always increase variable X when X = 0. Otherwise, trajectory constraints would be needed also for increments.  and DEC functions. For example, a variable X can be set to increase or decrease any number between 0 and 1 as long as values do not become negative. The policy µ solves all these variations except for those where infinite sequences of decrements fail to drive a variable to zero. We rule out such QNPs by means of the following trajectory constraint: Definition 11 (QNP Constraint). The trajectory constraint CX for a numerical variable X in a QNP excludes the trajectories that after a given time point contain a) an infinite number of actions with Dec(X) effects, b) a finite number of actions with Inc(X) effects, c) no state where X = 0. This is analogous to the LTL constraint in Section 4. The set of constraints CX for all variables X ∈ V is denoted as CV . We will be interested in solving QNPs RV given that the constraints CV hold. Moreover, as we have seen, there are policies that solve entire families of similar QNPs: Definition 12 (Similar QNPs). Two QNPs are similar if they only differ in the IN C, DEC or InitV functions, and for each variable X, X = 0 is initially possible in one if it is initially possible in the other, and the same for X > 0. We want to obtain policies that solve whole classes of similar QNPs by solving a single abstract problem. For a QNP RV , we define its syntactic projection as RVo :3 Definition 13 (Syntactic Projection of QNPs). If RV = hF, Init, Act, G, V, InitV , IN C, DECi is a QNP, its syntactic projection is the non-deterministic (boolean) problem RVo = hF 0 , Init0 , Act0 , Gi, where 1. F 0 is F with new atoms X = 0 and X > 0 added for each variable X; i.e. F 0 = F ∪ {X = 0, X > 0 : X ∈ V }, 2. Init0 is Init and X = 0 (resp. X > 0) true iff X > 0 (resp. X = 0) is not initially possible in InitV . 3. Act0 is Act but where in each action and for each variable X, the effect Inc(X) is replaced by the atom X > 0, and the effect Dec(X) is replaced by the conditional effect “if X > 0 then X > 0 | X = 0”. Recalling that X > 0 is an abbreviation for X 6= 0, the atoms X = 0 and X > 0 are mutually exclusive. We refer to the action with non-deterministic effects in RVo as the Dec(X) actions as such actions have Dec(X) effects in RV . This convention is assumed when applying CV constraints to RVo . The syntactic projection RVo denotes a FONDP that features multiple initial states when for some variable X, both X = 0 and X > 0 are possible in InitV : Theorem 14. The FONDP denoted by RVo is the observation projection of the class RV made of all the PONDPs RV0 that are similar to RV . The generalization captured by Theorem 6 implies that: Theorem 15 (QNP Generalization). Let µ a policy that solves RVo /CV ; i.e. the FONDP denoted by RVo given CV . Then, µ solves RV0 /CV for all QNPs RV0 similar to RV . In addition, the constraints CV are strong enough in QNPs for making the abstraction RVo complete for the class of problems RV similar to RV : 3 For simplicity we use the syntax of STRIPS with negation and conditional effects [Gazen and Knoblock, 1997].  Theorem 16 (QNP Completeness). Let µ be a policy that solves the class of problems RV made up of all the QNPs RV0 that are similar to RV given CV . Then µ must solve the projection RVo given CV . This is because the class RV contains a problem RV∗ where each variable X has two possible values X = 0 and X = 1 such that the semantics of the Dec(X) and Inc(X) effects make RV∗ equivalent to the projection RVo , with the valuations over the two-valued X variables in correspondence with the boolean values X = 0 and X > 0 in RVo .  5.1  QNP Solving as FOND Planning  The syntactic projection RVo of a QNP RV represents a FONDP with non-deterministic (boolean) effects X > 0 | X = 0 for the actions in RV with Dec(X) effects. It may appear from Theorem 15 that one could use off-theshelf FOND planners for solving RVo and hence for solving all QNPs similar to RV . There is, however, an obstacle: the effects X > 0 | X = 0 are not fair. Indeed, even executing forever only actions with Dec(X) effects does not guarantee that eventually X = 0 will be true. In order to use fair (strong cyclic) FOND planners off-the-shelf, we thus need to compile the FONDP RVo given the constraints CV into a fair FONDP with no constraints. For this, it is convenient to make two assumptions and to extend the problem RV with extra booleans and actions that do not affect the problem but provide us with handles in the projection. The assumptions are that actions with Dec(X) effects have the (observable) precondition X > 0, and more critically, that actions feature decrement effects for at most one variable. The new atoms are qX , one for each variable X ∈ V , initially all false. The new actions for each variable X in V are set(X) and unset(X), the first with no precondition and effect qX , the second with precondition X = 0 and effect ¬qX . Finally, preconditions qX are added to actions with effect Dec(X) and precondition ¬qX to all actions with effect Inc(X). Basically, qX is set in order to decrease the variable X to zero. When qX is set, X cannot be increased and qX can be unset only when X = 0. We say that RV is closed when RV is extended in this way and complies with the assumptions above (and likewise for its projection RVo ). Theorem 17 (Generalization with FOND Planner). µ is a fair solution to the FONDP RVo for a closed QNP RV iff µ solves all QNPs that are similar to RV given the constraints CV . Sketch: We need to show that µ is a fair solution to RVo iff µ solves RVo /CV . The rest follows from Theorems 15 and 16. (⇒). If µ does not solve the FONDP RVo given CV , there must a µ-trajectory τ that is not goal reaching but that satisfies CV and is not fair in RVo . Thus, there must be a subtrajectory hsi , ai , . . . , si+m i that forms a loop with si+m = si , where no sk is a goal state, and 1) some ak has a Dec(X) effect in RV , and 2) X > 0 is true in all sk , i ≤ k ≤ i + m. 1) must be true as τ is not fair in RVo and only actions with Dec(X) actions in RV are not deterministic in RVo , and 2) must be true as, from the assumptions in RV , X = 0 needs to be achieved by an action that decrements X, in contradiction with the assumption that τ is not fair in RVo . Finally, since τ satisfies CV , then it must contain infinite actions with Inc(X) effects,  but then the loop must feature unset(X) actions with precondition X = 0 in contradiction with 2. (⇐) If µ solves RVo /CV but µ is not a fair solution to RVo , then there must be an infinite µ-trajectory τ that is not goal reaching and does not satisfy CV , but which is fair in RVo . This means that there is a loop in τ with some Dec(X) action, no Inc(X) action, and where X = 0 is false. But τ can’t then be fair in RVo .  6  Discussion  We have studied ways in which a (possibly infinite) set of problems with partial observations (PONDPs) that satisfy a set of trajectory constraints can all be solved by solving a single fully observable problem (FONDP) given by the common observation projection, augmented with the trajectory constraints. The trajectory constraints play a crucial role in adding enough expressive power to the observation projection. The single abstract problem can be solved with automata theoretic techniques typical of LTL synthesis when the trajectory constraints can be expressed in LTL, and in some cases, by more efficient FOND planners. The class of qualitative numerical problems are related to those considered by Srivastava et al. [2011b; 2015] although the theoretical foundations are different. We obtain the FONDPs RVo from an explicit observation projection, and rather than using FOND planners to provide fair solutions to RV"
"arXiv:1909.12142v1 [cs.AI] 26 Sep 2019  Higher-Dimensional Potential Heuristics for Optimal Classical Planning Florian Pommerening and Malte Helmert  Blai Bonet  University of Basel, Switzerland {florian.pommerening, malte.helmert}@unibas.ch  Universidad Simón Bolívar, Venezuela bonet@ldc.usb.ve   Introduction Potential heuristics (Pommerening et al. 2015) are a family of declarative heuristic functions for state-space search. They fix the form of the heuristic to be a weighted sum over a set of features. Conditions on heuristic values such as admissibility and consistency can then be expressed as constraints on feature weights. Previous work on admissible potential heuristics is limited to atomic features, which consider the value of a single state variable in a factored state representation. Pommerening et al. (2015) show that admissible and consistent potential heuristics over atomic features can be characterized by a compact set of linear constraints. Seipp, Pommerening, and Helmert (2015) introduce several objective functions to select the best potential heuristic according to different criteria. They showed that a small collection of diverse potential heuristics closely approximates the state equation heuristic (SEQ) (van den Briel et al. 2007; Bonet 2013; Bonet and van den Briel 2014). Since the quality of the SEQ heuristic is an upper limit on the quality of any combination of potential heuristics over atomic features (Pommerening et al. 2015), more complex features are needed to significantly improve heuristic quality of potential heuristics. Additionally, theoretical analyses of common planning benchmarks (Chen and Giménez 2007; Chen and Giménez Copyright c 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  2009; Lipovetzky and Geffner 2012; Seipp et al. 2016) suggest that potential heuristics for binary features (that consider the joint assignment of two state variables rather than valuations over single state variables as in the atomic case) could already lead to a significant increase in accuracy in many planning domains. In this paper we generalize known results about potential heuristics with atomic features to those with larger features. After introducing some notation, we show that admissible and consistent potential heuristics for binary features are also characterized by a compact set of linear constraints. We then prove that such a compact characterization is not possible in the general case of features mentioning three or more variables. However, we show that compact representations are still possible for “sparse” features, as measured by the treewidth (Dechter 2003) of a new graphical structure we call the context-dependency graph. Finally, we generalize a known relation between atomic potential heuristics and optimal cost partitioning and show that potential heuristics correspond to optimal transition cost partitionings (Keller et al. 2016), i. e., cost partitionings that distribute the cost of each transition instead of each operator.  Background +  We consider SAS planning tasks (Bäckström and Nebel 1995) in transition normal form (TNF) (Pommerening and Helmert 2015). A planning task is a tuple Π = hV, O, sI , s? i with the following components. V is a finite set of variables where each V ∈ V has a finite domain dom(V ). A pair hV, vi of a variable V ∈ V and one of its values v ∈ dom(V ) is called a fact. Partial variable assignments p map a subset of variables vars(p) ⊆ V to values in their domain. Where convenient, we also treat them as sets of facts. A partial variable assignment s with vars(s) = V is called a state and S is the set of all states. The state sI is the initial state of Π and the state s? is the goal state. (Note that in TNF, there is a single goal state.) We call a partial variable assignment p consistent with a state s if s and p agree on all variables in vars(p). The set O is a finite set of operators o, each with a precondition pre(o), an effect eff (o), and a cost cost(o), where pre(o) and eff (o) are both partial variable assignments and cost(o) is a non-negative integer. The restriction to tasks in TNF means that we can assume that vars(pre(o)) = vars(eff (o)). We denote this set of  variables by vars(o). Considering only tasks in TNF does not limit generality, since there is an efficient transformation from general SAS+ tasks into equivalent tasks in TNF (Pommerening and Helmert 2015). An operator o is applicable in state s if s is consistent with pre(o). Applying o in s results in the state sJoK with sJoK[V ] = eff (o)[V ] for all V ∈ vars(o) and sJoK[V ] = s[V ] for all other variables. An operator sequence π = ho1 , . . . , on i is applicable in state s if there are states s = s0 , . . . , sn such that oi is applicable in si−1 and si−1 Joi K = si . We write sJπK for sn . If sJπK = s? , we call π an s-plan. If s = sI , we P call it a plan. The cost of n π under a cost function cost0 is i=1 cost0 (oi ). An s-plan 0 π with minimal cost (π) among all s-plans is called optimal and we write its cost as h∗ (s, cost0 ), or h∗ (s) if cost0 = cost. A heuristic function h maps states to values in R ∪ {−∞, ∞}. It is admissible if h(s) ≤ h∗ (s) for all s ∈ S, goal-aware if h(s? ) ≤ 0, and consistent if h(s) ≤ cost(o) + h(sJoK) for all s ∈ S and o ∈ O that are applicable in s. A task Π induces a weighted, labeled transition system TSΠ = hS, T , sI , {s? }i with the set of states S, the initial state sI , the single goal state s? and set of transitions T : for each s ∈ S and o ∈ O that is applicable in s, there is a trano sition s − → sJoK labeled with o and weighted with cost(o). Shortest paths in TSΠ correspond to optimal plans for Π. A conjunction of facts is called a feature and the number of conjuncts is called its size. Features of size 1 and 2 are called unary and binary features. We say a feature f is true in a state s (written as s  f ) if all its facts are in s. A weight function for features F is a function w : F → R. The potential of a state s under a weight function w is X ϕ(s) = w(f )[s  f ],  o  for all transitions s − → s0 ∈ T . We abbreviate the change of a feature’s truth value ([s  f ] − [sJoK  f ]) as ∆o (f, s). We partition the set of features into three subsets: irrelevant features F irr have no variables in common with vars(o), context-independent features F ind mention only variables in vars(o), and the remaining context-dependent features F ctx mention one variable from vars(o) Pand another variable not in vars(o). We write ∆irr o (s) for f ∈F irr w(f )∆o (f, s) and ctx analogously ∆ind (s) and ∆ (s). o o The truth value of an irrelevant feature never changes by applying o in some state. Thus, ∆irr o (s) = 0 for all states s. For a context-independent feature f , the effect of applying o in s is completely determined by o: f holds in s iff f is entailed by the precondition, and in sJoK iff it is entailed by the effect. Thus, ∆o (f, s) = [pre(o)  f ] − [eff (o)  f ] for every state s in which o is applicable. Clearly, ∆o (f, s) ind and ∆ind and o (s) do not depend on the state s for f ∈ F ind we write ∆o (f ) and ∆o . A feature f ∈ F ctx is a conjunction f = fo ∧ fō where fo is a fact over a variable in vars(o) and fō is a fact over a variable in Vō = V \ vars(o). If o is applied in a state s with s 2 fō , then s 2 f and sJoK 2 f , so ∆o (f, s) = 0. For the remaining features, we know that fō is present in both s and sJoK and the truth value of fo is solely determined by o. Thus ∆o (f, s) = ∆o (fo )[s  fō ]. If o is applicable in s, X X ∆ctx w(f )∆o (f, s) (4) o (s) = V ∈Vō  =  Two-Dimensional Potential Heuristics Two-dimensional potential heuristics only consider atomic and binary features. We use that consistent heuristics are admissible iff they are goal-aware (Russell and Norvig 1995). Let Π = hV, O, sI , s? , costi be a planning task in TNF and TSΠ = hS, T , sI , {s? }i its transition system. A potential heuristic ϕ over features F is goal-aware and consistent iff it satisfies the following constraints: ϕ(s? ) ≤ 0, (1) o  ϕ(s) − ϕ(s0 ) ≤ cost(o) for s − → s0 ∈ T . (2) This set of constraints has exponential size as there is one o constraint for each transition s − → s0 in T . Constraint (1) is a linear constraint P over the weights, which is easy to see since ϕ(s? ) = f ∈F w(f )[s?  f ]. Next, let o ∈ O be a fixed operator and consider constraint (2). Replacing ϕ(s) and ϕ(s0 ) by their definitions, we get the equivalent constraint X w(f )([s  f ] − [s0  f ]) ≤ cost(o) (3) f ∈F  X  w(f )∆o (fo )[fō = hV, s[V ]i] (5)  V ∈Vō f ∈F ctx f =fo ∧fō  f ∈F  where the bracket is an indicator function (Knuth 1992). We call ϕ the potential heuristic for features F and weights w. Its dimension is the size of a largest feature f ∈ F.  X  f ∈F ctx f =fo ∧fō vars(fō )={V }  ≤  X  X  w(f )∆o (fo )[fō = hV, vV∗ ]i]  (6)  V ∈Vō f ∈F f =fo ∧fō ctx  where X  vV∗ = arg max  v∈dom(V )  w(f )∆o (fo )[fō = hV, vi].  ctx  f ∈F f =fo ∧fō  If we denote the inner sum in (6) with boV , then X ϕ(s) − ϕ(s0 ) ≤ ∆ind boV o + o  (7)  V ∈Vō  for all transitions s − → s0 ∈ T . Therefore, if ∆ind o + P o b ≤ cost(o) for all operators o, then ϕ is consisV ∈Vō V tent. Conversely, if ϕ is consistent, then ϕ(s) − ϕ(sJoK) ≤ cost(o) for operator o and the states s in which o is applicable. In particular, for states s∗ such that s∗ [V ] = pre(o)[V ] for V ∈ vars(o), and s∗ [V ] = vV∗ otherwise. It is then not difficult to check that the inequality in (7) is P tight for such o states s∗ . Hence, ϕ is consistent iff ∆ind + o V ∈Vō bV ≤ cost(o) for all operators o. Putting everything together, we see that the constraint (3) for a fixed operator o is equivalent to the constraints X ∆ind zVo ≤ cost(o), (8) o + V ∈Vō  zVo ≥  X  w(f )∆o (fo ) for V ∈ Vō , v ∈ dom(V ) (9)  ctx  f ∈F f =fo ∧hV,vi  where zVo is a new variable that upper bounds boV . This set of constraints has O(|V|d) constraints where d bounds the size of the variable domains, while each constraint has size O(|F| + |V|). The set of constraints is over the variables {w(f ) : f ∈ F} ∪ {zVo : o ∈ O, V ∈ Vō }. Theorem 1. Let F be a set of features of size at most 2 for a planning task Π. The set of solutions to the constraints (1), and (8)–(9) for each operator, projected to w, corresponds to the set of weight functions of admissible and consistent potential heuristics for Π over F. The total number of constraints is O(|O||V|d), where d bounds the size of variable domains, while each constraint has size O(|F| + |V|).  High-Dimensional Potential Heuristics In this section we show that a general result like Theorem 1 is not possible, unless NP equals P, for sets of features of dimension 3 or more, but we identify classes of problems on which potential heuristics can be characterized compactly.  Intractability Theorem 1 allows one to answer many interesting questions in polynomial time about potential heuristics of dimension 2. In particular, by solving a single LP one can test whether a given potential heuristic is consistent and/or goal-aware. We use this idea to show that no general result like Theorem 1 is possible for potential heuristics of dimension 3, by making a reduction of non-3-colorability (a decision problem that is complete for coNP (Garey and Johnson 1979)) into the problem of testing whether a potential heuristic of dimension 3 is consistent. Let G = hV, Ei be an undirected graph. We first construct, in polynomial time, a planning task Π = hV, O, sI , s? i in TNF and a potential heuristic ϕ of dimension 3 such that G is not 3-colorable iff ϕ is consistent. The task Π has |V | + 1 variables: one variable Cv for the color of each vertex v ∈ V that can be either red, blue, or green, and one “master” binary variable denoted by M . For every vertex v ∈ V and pair of different colors c, c0 ∈ dom(Cv ), there is a unique operator ov,c,c0 of zero cost that changes Cv from c to c0 when M = 0. For the variable M , there is a unique operator oM , also of zero cost, that changes M from 0 to 1. These are all the operators in the task Π. Each state s ∈ S encodes a coloring of G, where the color of vertex v is the value s[Cv ] of the state variable Cv . The initial state sI is set to an arbitrary coloring but with the master variable set to 0; e. g., sI [M ] = 0 and sI [Cv ] = red for every vertex v ∈ V . The goal state s? is also set to an arbitrary coloring but with s? [M ] = 1; e. g., s? [M ] = 1 and s? [Cv ] = red for every vertex v ∈ V . The potential heuristic ϕ of dimension 3 is constructed as follows. For features f with vars(f ) = {M, Cu , Cv } such that {u, v} ∈ E is an edge in the graph, let its weight w(f ) = −1 when f [M ] = 1 and f [Cu ] 6= f [Cv ], and  w(f ) = 0 otherwise. For the feature fM = hM, 1i of dimension 1, let w(fM ) = |E| − 1. The weight w(f ) for all other features f is set to 0. Let us now reason about the states of the task Π and the values assigned to them by the heuristic. Let s be a state for Π. If s[M ] = 0, then ϕ(s) = 0. If s[M ] = 1, then no operator is applicable at s and ϕ(s) ≥ −1, with ϕ(s) = −1 iff s encodes a 3-coloring of G, as the feature fM contributes a value of |E| − 1 to ϕ(s), while the features corresponding to edges contribute a value of −|E| when s encodes a coloring. o Let us consider a transition s − → s0 ∈ T . Clearly, 0 s[M ] ≤ s [M ] as no operator decreases the value of M . If s[M ] = s0 [M ] = 0, then ϕ(s) = ϕ(s0 ) = 0. If s[M ] = 0 and s0 [M ] = 1, then ϕ(s) ≤ ϕ(s0 ) iff s0 does not encode a 3-coloring of G. The case s[M ] = s0 [M ] = 1 is not possible as no operator is applicable in states with s[M ] = 1. Therefore, since all operator costs are equal to zero, ϕ is o consistent iff there is no transition s − → s0 with s[M ] = 0, 0 0 s [M ] = 1 and s encoding a 3-coloring of G, and the latter iff the graph G is not 3-colorable. Finally, observe that testing whether a potential function ϕ is inconsistent can be done in non-deterministic polynomial time: guess a state s and an operator o, and check whether ϕ(s) > ϕ(sJoK) + cost(o). Theorem 2. Let F be a set of features for a planning task Π, and let ϕ be a potential heuristic over F. Testing whether ϕ is consistent is coNP-complete.  Parametrized Tractability We first give an algorithm for maximizing a sum of functions using linear programming, and then apply it to characterize high-dimensional potential heuristics. Maximizing a Sum of Functions. Let X be a set of finitedomain variables. We extend the notation dom(X ) to mean the set of variable assignments over X . For an assignment ν ∈ dom(X ) and a scope S ⊆ X , we use ν|S to describe the restriction of ν to S. Let Ψ be a set of scoped functions hS, ψi with S ⊆ X and ψ : S → V for a set of values V. For now, think of V as the real numbers R, but we will later generalize this. We only require that maximization and addition is defined on values in V, that both operations are commutative and associative, and that max {a + c, b + c} ≡ c + max {a, b} for all a, b, c ∈ P V. We are interested in the value Max(Ψ) = maxν∈dom(X ) hS,ψi∈Ψ ψ(ν|S ). Computing Max(Ψ) is the goal of constraint optimization for extensional constraints, an important problem in AI. It is challenging because the number of valuations in dom(X ) is exponential in the number |X | of variables. Bucket elimination (Dechter 2003) is a well-known algorithm to compute Max(Ψ). For reasons that will become clear later in this section, we describe the bucket elimination algorithm in a slightly unusual way: in our formulation, the algorithm generates a system of equations, and its output can be extracted from the (uniquely defined) solution to these equations. The system of equations makes use of auxiliary variables Aux1 , . . . , Auxm that take values from V. The generated equations have the form Auxi = maxj∈{1,...,ki } ei,j ,  where ei,j is a sum that contains only values from V or the variables Aux1 , . . . , Auxi−1 . Solutions to the system of equations guarantee that Auxm ≡ Max(Ψ). Bucket Elimination We now describe the general algorithm and state its correctness (without proof due to lack of space). Its execution depends on an order σ = hX1 , . . . , Xn i of the variables in X . The algorithm operates in stages which are enumerated in a decreasing manner, starting at stage n + 1 and ending at stage 0: • Stage n + 1 (Initialization). Start with a set {Bi }ni=0 of empty buckets. Place each hS, ψi ∈ Ψ into the bucket Bi if Xi is the largest variable in S, according to σ, or into the bucket B0 if S = ∅. • Stages i = n, . . . , 1 (Elimination). Let hSj , ψj i for j ∈ {1, . . . , ki } be the scoped functions S currently in bucket Bi . Construct the scope SXi = ( j∈{1,...,ki } Sj ) \ {Xi } and the function ψXi : SXi → V that represents the contribution of all functions that depend on Xi . The definition of ψXi is added to the generated system of equations by adding one auxiliary variable AuxXi ,ν for every ν ∈ dom(SXi ) which represents the value ψXi (ν): X AuxXi ,ν = max ψj (νxi |Sj ) xi ∈dom(Xi )  j∈{1,...,ki }  where νxi = ν ∪ {Xi 7→ xi } extends the variable assignment ν with Xi 7→ xi . If ψj is a function in Ψ, then ψj (νxi |Sj ) is an element of V. Otherwise ψj is a previously defined function ψXi0 for i0 > i and its value for ν 0 = νxi |Sj is represented by AuxXi0 ,ν 0 . The newly defined function ψXi no longer depends on Xi but depends on all variables in SXi , so hSXi , ψXi i is added to bucket Bj if Xj is the largest variable in SXi according to σ or to B0 if SXi = ∅. Observe that j < i because SXi only contains variables from scopes where Xi is the largest variable and SXi does not contain Xi . • Stage 0 (Termination). Let hSj , ψj i for j ∈ {1, . . . , k} be the scoped functions currently in bucket B0 . Add the P auxiliary variable AuxΨ and the equation AuxΨ = j∈{1,...,k} ψj analogously to the elimination step. (All Sj are empty and the maximum is over dom(∅) = {∅}.) Example. Consider Ψ = {h{X}, f i, h{X, Y }, gi} over the binary variables X = {X, Y }. Bucket elimination generates the following system of equations for the variable order σ = hX, Y i. Aux1 = AuxY,{X7→0} = max g(0, y) y∈{0,1}  = max {g(0, 0), g(0, 1)} Aux2 = AuxY,{X7→1} = max g(1, y) y∈{0,1}  = max {g(1, 0), g(1, 1)} Aux3 = AuxX,∅ = max (f (x) + AuxY,{X7→x} ) x∈{0,1}  = max {f (0) + Aux1 , f (1) + Aux2 } Aux4 = AuxΨ = AuxX,∅ = Aux3  Bucket Elimination for Linear Expressions As a generalization of the bucket elimination algorithm, consider V to be the following set E of mathematical expressions over a set of variable symbols Y. For every Y ∈ Y and r ∈ R, the expressions Y , r, and rY are in E. If a and b are elements of E, then the expressions (a + b) and max {a, b} are elements of E. There are no additional elements in E. An assignment f : Y → R that maps variables to values can be extended to E in the straight-forward way. Two expressions a, b ∈ E are equivalent if f (a) = f (b) for all assignments f . An expression is linear if it does not mention max. Clearly, maximization and addition are commutative, associative and satisfy max {a + c, b + c} ≡ c + max {a, b} for all expressions a, b, c ∈ E. Bucket elimination therefore generates a system of equations Auxi = maxj∈{1,...,ki } ei,j , where all ei,j are sums over expressions and variables Auxi0 with i0 < i. Since a variable is a mathematical expression, the whole result can be seen as a system of equations over the variables Y ∪ {Aux1 , . . . , Auxm }. If additionally all functions in Ψ only produce linear expressions over Y, then in the resulting system all ei,j are linear expressions over Y ∪ {Aux1 , . . . , Auxm }. Consider the example problem again. We define f and g so they map to linear expressions over Y = {a, b}: f (x)  x=0 3a − 2b  x=1 4a + 2b  g(x, y) y=0 y=1  x=0 8a 7b  x=1 −3b 0  In the resulting system of equations all elements in the maxima are linear expressions over the variables {a, b, Aux1 , Aux2 , Aux3 , Aux4 }: Aux1 Aux2 Aux3 Aux4  = max {8a, 7b} = max {−3b, 0} = max {3a − 2b + Aux1 , 4a + 2b + Aux2 } = Aux3  Bucket elimination guarantees that Aux4 ≡ Max(Ψ) for any value of a and b in E. We argue that this system of equations can be solved by an LP solver. Theorem 3. Let Y and {Aux1 , . . . Auxm } be disjoint sets of variables. Let Pmax be the system of equations Auxi = maxj∈{1,...,ki } ei,j for i ∈ {1, . . . , m}, where ei,j is a linear expression over variables Y ∪ {Aux1 , . . . , Auxi−1 }. Let PLP be the set of linear constraints Auxi ≥ ei,j for i ∈ {1, . . . , m} and j ∈ {1, . . . , ki }. Every solution of Pmax is a solution of PLP and for every solution f of PLP there is a solution f 0 of Pmax with f 0 (Y ) = f (Y ) for all Y ∈ Y and f 0 (Aux) ≤ f (Aux) for all Aux ∈ / Y. As a corollary, we can use this result to represent a “symbolic” version of the bucket elimination algorithm with unknowns Y as an LP. (Note that the constraints generated by the bucket elimination algorithm have exactly the form required by the theorem if functions in Ψ produce linear expressions.) This LP has the property that for every assignment to the unknowns Y there exists a feasible solution, and the values of Auxm in these feasible solutions are exactly the set of numbers greater or equal to Max(Ψ) for the given assignment to Y. (For simplicity, it would be preferable if only  Max(Ψ) itself resulted in a feasible assignment to Auxm , but we will see that the weaker property where Auxm may overestimate Max(Ψ) is sufficient for our purposes.) We denote the set of constraints for this LP by PLP(Ψ,σ) . This LP can be solved in time that is polynomial in the size of PLP(Ψ,σ) , so to bound the complexity, we have to consider the number and size of the constraints in PLP(Ψ,σ) . Dechter (2003) defines the dependency graph of a problem Ψ over variables X as the undirected graph G(Ψ) = hX , Ei with set of vertices given by the variables X and an edge hX, X 0 i ∈ E iff X 6= X 0 and there is a scoped function hS, ψi in Ψ with {X, X 0 } ⊆ S. Given an undirected graph G and an order of its nodes σ, a parent of a node n is a neighbor of n that precedes n in σ. Dechter defines the induced graph of G along σ as the result of processing each node of G in descending order of σ and for each node connecting each pair of its parents if they are not already connected. The induced width of G along σ then is the maximal number of parents of a node in the induced graph of G along σ. If there are n variables in X and each of their domains is bounded by d, then eliminating variable Xi adds one equation AuxXi ,ν = maxj∈dom(Xi ) ei,j for each valuation ν of the scope SXi (variables relevant for Xi ). The size of this scope is limited by the induced width w(σ), so the number of valuations is limited by dw(σ) . As there are n buckets to eliminate, the number of auxiliary variables in the LP can thus be bounded by O(ndw(σ) ). Each such variable occurs in |dom(Xi )| ≤ d constraints of the form AuxXi ,ν ≥ ei,j in PLP(Ψ,σ) , so there are O(ndw(σ)+1 ) constraints. Theorem 4. Let Ψ be a set of scoped functions over the variables in X that map to linear expressions over Y. Let σ be an ordering for X . Then PLP(Ψ,σ) has O(|Y| + |X |dw(σ) ) variables and O(|X |dw(σ)+1 ) constraints, where d = maxX∈X |dom(X)| and w(σ) is the induced width of G(Ψ). The smallest possible induced width of G(Ψ) along any order σ is called the induced width of G(Ψ) and equals the treewidth of G(Ψ) (Dechter 2003). Unfortunately, finding the induced width or a minimizing order is NP-hard. However, it is fixed-parameter tractable (Downey and Fellows 1999) with the treewidth as the parameter (Bodlaender 1996). High-Dimensional Potential Functions. Of the two conditions that characterize goal-aware and consistent potential heuristics, constraint (2) is the most challenging to test as it really denotes an exponential number of constraints, one per state. The constraint is equivalent to cost(o) ≥ max (ϕ(s) − ϕ(sJoK)) spre(o) X = max w(f )∆o (f, s) spre(o)  =  max  f ∈F  ν∈dom(Vō )  X  w(f )∆o (f, so,ν )  f ∈F  for all operators o ∈ O where the partial state so,ν is so,ν = pre(o) ∪ ν.  As done before, for a fixed operator o ∈ O, we partition F as F = F irr ∪ F ind ∪ F ctx and consider the functions ∆irr o (s), ctx ∆ind (s) and ∆ (s) for states s on which the operator o is o o ind ind applicable. We have ∆irr o (s) = 0 and ∆o (s) = ∆o as the value of the latter is independent of the state s. Therefore, the constraint (2) is equivalent to cost(o) ≥ ∆ind o +  max  ν∈dom(Vō )  ∆ctx o (so,ν )  (10)  P where ∆ctx o (so,ν ) = f ∈F ctx w(f )∆o (f, so,ν ). Observe that for f ∈ F ctx , the value of ∆o (f, so,ν ) is equal to [pre(o) ∪ so,ν |Vf \Vo  f ] − [eff (o) ∪ so,ν JoK|Vf \Vo  f ].  Let us define the functions ψof that maps partial assignments ν ∈ dom(Vf \ Vo ) to expressions in {0, w(f ), −w(f )} as ψof (ν) = w(f )([pre(o) ∪ ν  f ] − [eff (o) ∪ ν  f ]). For operator o ∈ O, we define Ψo = {ψof : f ∈ F ctx }. Then X ψof (so,ν |Vf \Vo ) max ∆ctx max o (so,ν ) = ν∈dom(Vō )  ν∈dom(Vō )  f ∈F ctx  = Max(Ψo ). Constraint (10) is then equivalent to  cost(o) ≥ ∆ind o + Max(Ψo ). Applying Theorem 4 to Ψo along an ordering σo for the variables V in Π, we obtain a set of constraints PLP(Ψo ,σo ) that characterize Max(Ψo ). We can replace the above constraint by cost(o) ≥ ∆ind o + AuxΨo and the constraints in PLP(Ψo ,σo ) . The constraints PLP(Ψo ,σo ) allow setting AuxΨo higher than necessary, but this is never beneficial as long as AuxΨo does not occur in the objective. Theorem 5. Let F be a set of features for a planning task Π, and let {σo }o∈O be an indexed collection of orderings of the variables in Π (one ordering σo for each operator o ∈ O). Then, the set of solutions to ϕ(s? ) ≤ 0, ∆ind o  (11)  + AuxΨo ≤ cost(o), for each operator o ∈ O, (12) PLP(Ψo ,σo ) for each operator o ∈ O (13)  (projected on the feature weights) corresponds to the set of weight functions of admissible and consistent potential heuristics for Π over F. We finish the section by bounding the number and size of the constraints in PLP(Ψo ,σo ) . We define the contextdependency graph G(Π, F, o) for a planning task Π, features F and an operator o as follows: the vertices are the variables of Π and there is an edge between V and V 0 with V 6= V 0 iff there is a feature f ∈ F with vars(f )∩vars(o) 6= ∅ and {V, V 0 } ⊆ vars(f ) \ vars(o). Theorem 6. Let F be a set of features for a planning task Π, let o be an operator of Π, and let σo an ordering on the variables in Π. The number of constraints in PLP(Ψo ,σo ) is O(ndw(σo )+1 ) where n is the number of variables, d bounds the size of the variable domains, and w(σo ) is the induced width of G(Π, F, o) along the ordering σo . The number of variables in PLP(Ψo ,σo ) is O(|F| + ndw(σo ) ).  By combining the constraints from the different operators according to Theorem 5, we thus obtain the following fixedparameter tractability result. Corollary 1. Let F be a set of features for a planning task Π. Define the parameter w∗ as the maximum treewidth of all context-dependency graphs for Π and F. Computing a set of linear constraints that characterize the admissible and consistent potential heuristics with features F for Π is fixed-parameter tractable with parameter w∗ . We remark that the general result (again) implies a polynomial characterization of potential heuristics where all features have dimension at most 2. In this case, no feature can simultaneously include a variable mentioned in a given operator o and two further variables not mentioned in o, and hence all context-dependency graphs are devoid of edges. In edge-free graphs, all orderings have width 0, and hence for each operator o, PLP(Ψo ,σo ) has O(nd) constraints and O(|F| + n) variables. The parameter w∗ is 0 in this case.  Relation to Cost Partitioning Operator cost partitioning (Katz and Domshlak 2007; Yang et al. 2008; Katz and Domshlak 2010) is a technique to make the sum of several heuristics admissible by distributing the cost of each operator between them. Katz and Domshlak (2010) show how the optimal operator cost partitioning (OCP) can be computed in polynomial time for a large family of abstraction heuristics. Pommerening et al. (2015) show that an admissible and consistent potential heuristic over all atomic features that achieves the maximal initial heuristic value corresponds to an OCP over atomic projection heuristics. Here, we extend this result to all potential heuristics that use the abstract states of a set of abstractions as features. To that end, we first discuss a generalization of OCP, which we call optimal transition cost partitioning (TCP) (Keller et al. 2016)1 . Definition 1. Let Π be a planning task with cost function cost and transitions T . A set of transition cost functions costi : T → R, 1 ≤ i ≤ n, is a transition cost partitioning if n X  o  costi (s − → s0 ) ≤ cost(o)  o  for all s − → s0 ∈ T .  i=1  To use cost partitionings, the notion of operator cost functions must be extended to cost functions with possibly negative values. For example, the cost of an s-plan π = ho1 , . . . , on i under transition cost function cost0 is Pn oi 0 → si ), where s = s0 , . . . , sn = sJπK i=1 cost (si−1 − are the states visited by the plan π. The cheapest plan cost under cost0 can now be negative or even −∞ (in the case of negative cost cycles). Proposition 1 (Keller et al. (2016)). Let Π be a planning task, P = hcost1 , . . . , costn i be a transition cost partitioning for Π, and h1 , . . . ,P hn be admissible heuristics for Π. n The heuristic hP (s) = i=1 hi (s, costi ) is admissible. 1 Keller et al. use state-dependent cost partitioning, which may be confused with partitionings that are re-optimized for each state.  Definition 2. Let Π be a planning task, h1 , . . . , hn be admissible heuristics for Π, and P be the set of all transition cost partitionings. The optimal transition cost partitioning heuristic hTCP is hTCP (s) = maxP ∈P hP (s). Operator cost partitionings and the OCP heuristic are special cases where all cost functions satisfy cost(t1 ) = cost(t2 ) for all pairs of transitions t1 , t2 labeled with the same operator. The TCP heuristic thus dominates the OCP heuristic. The paper by Keller et al. contains examples where this dominance is strict. The linear program defined by Katz and Domshlak (2010) to compute an optimal OCP for a set of abstraction heuristics can be extended to transition cost partitionings. Formally, an abstraction heuristic is based on a homomorphic mapping α : S → S α that maps states of a planning task Π to abstract states of an abstract transition system TSα = hS α , T α , α(sI ), {α(s? )}i, such that for every transio o tion s − → s0 of Π, T α contains the transition α(s) − → α(s0 ) with the same weight. For a collection A of abstractions, the optimal TCP can be encoded using two kinds of variables. The variable h(s) represents the goal distance for each abstract state s ∈ S α of each α ∈ A (we assume the states are uniquely named). The variable cα (t) represents the cost of a transition t ∈ T that is attributed to abstraction α. Using linear constraints over these variables, we can express that the variables h(s) do not exceed the true goal distance under the cost function encoded in cα and that the cost functions respect the cost partitioning property: h(α(s? )) = 0  for α ∈ A (14) (15) h(α(s)) − h(α(s0 )) ≤ cα (s − → s0 ) for α ∈o A 0 and s − →s ∈T X o o cα (s − → s0 ) ≤ cost(o) for s − → s0 ∈ T (16) o  α∈A  S For a αset of abstractions A, we define the set FA = α∈A S of features for A with the interpretation that a state s has the feature s0 ∈ S α iff α(s) = s0 . In the special case where α is a projection to k variables, the features S α correspond to conjunctions of k facts. We want to show that TCP heuristics over A and admissible and consistent potential heuristics over features FA have the same maximal heuristic value. Proposition 2. Let s be a state of a planning task Π and A be a set of abstractions of Π. The Pset of solutions for constraints (14)–(16) that maximize α∈A h(α(s)) (projected to h) is equal to the set of solutions for constraints (1)–(2) for FA that maximize ϕ(s). Proof: The important P observation for this proof is that we can write ϕ(s) = f ∈FA w(f )[s  f ] as ϕ(s) = P w(α(s)). α∈A Assume we have an optimal solution to constraints (14)– (16) and set w = h. Obviously, constraint (14) implies constraint (1): if all features that are present in the goal state have a weight of 0, then ϕ(s? ) = 0. Summing constraint (15) over all abstractions for a given transition, reP P o sults in α∈A h(α(s)) − h(α(s0 )) ≤ α∈A cα (s − → s0 ). Together with constraint (16)"
"Superintelligence Safety: A Requirements Engineering Perspective Hermann Kaindl and Jonas Ferdigg  arXiv:1909.12152v1 [cs.AI] 26 Sep 2019  Institute of Computer Technology TU Wien Vienna, Austria {hermann.kaindl, e1226597}@tuwien.ac.at  I. I NTRODUCTION The idea of technology becoming sentient has been a common theme in the literature and movies for decades. One might think of the famous movie “2001: A Space Odyssey” by Stanley Kubrick, the Matrix, or the Terminator movies. These stories seem to be mostly consistent in the impression that a suddenly arising uncontrolled Artificial Intelligence (AI) will not mean us well, but is more likely to stand in direct opposition to humanity’s goals. Furthermore, the AIs described in books and movies are never dumb machines, but rather potent entities with cognitive superpowers far beyond the capacities of human general intelligence — they are superintelligent. In his book “Superintelligence” [1], Oxford philosophy professor Nick Bostrom provides good reasons to believe that we can create such an entity. In the AI research priorities document published in [2], apart from the desirability of safety, e.g., of self-driving cars, “AI safety” is addressed with regard to “superhuman AI”. This is partly based on forecasting work on intelligence explosion and “superintelligence” [1], where the importance of given goals to be exactly aligned with those of mankind is emphasized.  Even that may be insufficient, however, the system must also somehow be deliberately constructed to pursue them [1]. In terms of requests like “Make paperclips”, such a system is envisaged to take everything it can (with high “intelligence”) and to make as many paperclips as it can make, whatever it may cost. Much like the example of king Midas or the core theme of the movie with the title “Bedazzled”, we view this as a requirement that is not specified precisely enough. While other references regarding “AI safety” can be found at https://vkrakovna.wordpress.com/ai-safety-resources/, it is mentioned nowhere that it is actually a problem with requirements. Since mankind may eventually create a “superintelligence”, it should rather sooner than later specify the requirements on such a system, which are traditionally the required functions, qualities and constraints. Goal-oriented RE (GORE) focusses on goals, but these are usually goals of the stakeholders rather than those of an AI system (for GORE see, e.g., [3], [4] and successor work, and the very recent mapping study [5]). Hence, we propose to extend GORE theoretically in this direction for “AI safety”, where the goals of the “superintelligence” based on AI technology are supposedly of critical importance. Note, that such goals will most likely be conditions of something to achieve, in contrast to certain problem-solving algorithms where goals are defined states, see, e.g., [6], [7]. For unidirectional search, conditions as goals can be used, in contrast to bidirectional search. Reasoning of a superintelligence on its own goals may involve a kind of self-awareness [8], or at least selfrepresentation [9]. It is not clear, however, what the implications are on the problem at hand. Having a glass-box view on a system implementing a superintelligence may be helpful for keeping control of it, such as following an approach to (object-oriented) software engineering for AI systems [10], [11]. Also making architectural decisions upfront, e.g., for a generic architecture [12], may be useful in this regard. Another approach is to model safety frameworks, see, e.g., [13]. However, implementing a superintelligence may well involve different approaches such as deep neural networks, which are considered opaque, or approaches yet to be developed. Hence, we take a black-box view here and propose to focus on requirements. Strictly speaking, the notion “AI safety” is misleading in  PT-AI AGI EETN TOP100 Combined  10% 2023 2022 2020 2024 2022  50% 2048 2040 2050 2050 2040  90% 2080 2065 2093 2070 2075  TABLE I W HEN WILL HUMAN - LEVEL MACHINE INTELLIGENCE BE ATTAINED ? S OURCE : [1, P. 19]  our opinion, since it may also include the safety of certain AI-based systems, e.g., self-driving cars. Hence, we prefer the notion of “superintelligence safety”. Safety of software-intensive systems is certainly a related area [14], [15], but it is primarily concerned with hazards in the environment of the system and related safety risks. Functional safety as dealt with, e.g., in the current automotive standard ISO 26262 focuses on failures of functions and how to manage them. Hence, we do not see how previous safety research could help much with regard to superintelligence safety. The remainder of this paper is organized in the following manner. First, we provide some background material on superintelligence safety, in order to make this paper self-contained. Then we view superintelligence safety from the perspective of RE, and envisage formulating requirements accordingly. In order to address the essence of the given issue, we argue for doing RE on the superintelligence in the first place. Finally, we motivate how the theory of the requirements problem should be extended to cover goals of a superintelligence. II. BACKGROUND  ON  S UPERINTELLIGENCE S AFETY  Surveys revealed that AI experts attribute a moderate likelihood of superintelligence emerging within a 100-year horizon, as can be seen in Tables I and II.1 Several AI experts where asked to estimate a year by which the likelihood of having created Human-Level Machine Intelligence (HLMI) and superintelligence, respectively, reaches a certain percentage. Assuming the creation of a superintelligence is possible, how can we control it and avoid the doomsday scenarios so eloquently depicted by authors and film directors? While an emerging superintelligence might not be inherently malevolent, it still poses a great risk. Without any information about the internal workings of such an entity, we have to assume that its way of ‘thinking’ might be very different from the way a human thinks. A superintelligence most likely will not share human morals or have a concept of value at all, if the developers did not intentionally design it to have one. While the most terrifying scenario is a malevolent superintelligence with the intent to eradicate humanity, another potential scenario is a superintelligence with no inherent motivations 1 PT-AI: Participants of the conference Philosophy and Theory of AI, 2011. AGI: Participants of the conference Artificial General Intelligence and Impacts and Risks of Artificial General Intelligence, 2012. EETN: Sampled members of the Greek Association for Artificial Intelligence, 2013. TOP100: The 100 top authors in artificial intelligence, measured by citation index, May 2013  TOP100 Combined  2 years after HLMI 5% 10%  30 years after HLMI 50% 75%  TABLE II H OW LONG FROM HUMAN LEVEL TO SUPERINTELLIGENCE ? S OURCE : [1, P. 20]  at all, simply following the instructions of it’s creator to the best of its abilities. Bostrom describes some of the abilities a superintelligence could have under the term “Cognitive Superpowers” [1, p. 91]. A superintelligence could excel at strategic planning, forecasting, analysis for optimizing chances of achieving distant goals, social and psychological modeling and manipulation, rhetoric persuasion, hacking into computer systems, design and modeling of advanced technologies, and the list goes on [1, p. 94]. One can see that even equipped with only a subset of these skills, a superintelligence pursuing a goal to the best of its abilities and without being constrained by concepts like morality or value, can cause substantial damage to its environment. Bostrom depicts this with his famous thought experiment where a superintelligence is given the simple task to make as many paper clips as possible. While the superintelligence might start off by acquiring monetary resources by predicting the stock market and building paper clip factories, it may not just stop there. Since its goal is the unconstrained maximization of the production of paperclips, it will soon discover that humans pose a hindrance to its endeavor, as we might try to stop the superintelligence from producing more paperclips or even try to shut it off. Since the superintelligence is multiple magnitudes smarter than humanity combined, humanity fails to stop the superintelligence and ultimately the whole world (the whole observable universe, in fact) will be made into paperclips. Even if this was just a contrived example, one can see the risks of giving a superintelligence an unconstrained optimization problem. This argument is also extended in [1] to constrained optimization problems, but the details are not necessary for our paper. III. S PECIFYING  AND C OMMUNICATING R EQUIREMENTS TO A S UPERINTELLIGENCE  Under the (usually unrealistic) assumption that we knew the requirements already, ‘just‘ telling the superintelligence about them properly is the problem here. It can be decomposed into specifying and communicating. According to the Standard ISO/IEC 10746-2:2009 Information technology – Open Distributed Processing – Reference Model: Foundations, 7.4, a specification is a “concrete representation of a model in some notation”. In order to better understand this sub-problem of specifying requirements, let us follow the observation in [16] that requirements representations are often confused with requirements per se. This confusion is also widespread in practice, as exemplified in the very recent Standard ISO SE Vocabulary 24765:2017. In fact,  it defines a requirement both as a “statement that translates or expresses a need and its associated constraints and conditions” and “a condition or capability that must be present in a product . . . ” (dots inserted). As it is well-known in RE, specifying requirements can be done informally, say, using some natural language, or formally, e.g., using some formal logic as the notation. Possibilities for semi-formal representations within this spectrum are manyfold. In order to avoid ambiguity in the course of specifying requirements for a superintelligence, formal representation of the requirements should be the choice, of course, possibly in some formal logic. Grounding the logic used in the domain will, however, still leave loopholes for the superintelligence to ‘misunderstand’ the given specification of our requirements. For example, for some predicate on paperclips, a grounding of what paperclips are in the real world will be necessary. In addition, incompleteness of the specification remains an open issue. Unfortunately, no general solution appears to be feasible at the current state of the art in RE. Communicating a given requirements specification to a superintelligence may be investigated according to [17], based on speech-act theory [18], under the premise that stakeholders communicate information in the course of requirements engineering. This view was not taken for the sake of really communicating requirements in [17], but for using the semantics of various speech acts to theoretically (re-)define the requirements problem originally defined by Zave and Jackson [19] (see also below). Everything is assumed to be communicated by speech acts here. However, we can only communicate representations of requirements, and not requirements per se. It is clear that the text given in the examples in [17] represents something by describing it, the very confusion addressed in [16], where specific examples of this confusion are given. Anyway, for communicating specifications of requirements to a superintelligence, speech-act theory could be helpful for annotating the specific kind of speech act, e.g., Question or Request. For example, the text “Can you produce paperclips?” may be interpreted either way, unless specified more precisely. Still, we have to face that ‘perfectly’ specifying and communicating requirements to a superintelligence may not be possible. Even if we could, this would not help in case of malevolent superintelligence that would not necessarily satisfy requirements as specified and communicated. Hence, let us consider building a superintelligence in such a way that these problems can be mitigated. IV. RE  ON A  S UPERINTELLIGENCE  In our view, building a superintelligence should certainly include RE (much as building any non-trivial system). For specifically addressing superintelligence safety, we may ignore defining functional requirements on its superintelligent abilities. Still, some functionality may have to be defined for operationalizing certain “non-functional” safety requirements, which will in the first place be cast as constraints on the superintelligence.  Capability Control  Boxing methods  Incentive methods  Stunting  Tripwires  Fig. 1. Overview of capability control techniques  Working on these requirements could be done as usual according to best practice of RE. This will involve major stakeholders (in particular, AI researchers), requirements elicitation may be done including questionnaires in the Web, etc. While the core questions will be about requirements on superintelligence safety, of course, a bigger question should be asked in our opinion, what the goals and requirements are that mankind has regarding AI in the first place. We expect that RE for superintelligence safety will pose even greater challenges than the usual practice of RE. After all, it is not just about conceiving such a superintelligence but about making sure that its creation will not raise uncontrollable safety risks. Let our RE endeavor be also informed by some preliminary thought by the author who raised the issue of superintelligence safety. In [1, Chapter 9], controlling a superintelligence is discussed, in order to deal with this specific safety problem. Two broad classes of potential methods are distinguished — capability control and motivation selection. Within each of them, several specific techniques are examined. We review the key ideas here, starting with capability control, where Figure 1 shows an overview of the related techniques summarized as follows [1, p. 143]: • Boxing methods “The system is confined in such a way that it can affect the external world only through some restricted preapproved channel. Encompasses physical and informational containment methods.” • Incentive methods “The system is placed within an environment that provides appropriate incentives. This could involve social integration into a world of similarly powerful entities. Another variation is the use of (cryptographic) reward tokens. . . . ” (dots inserted) • Stunting “Constraints are imposed on the cognitive capabilities of the system or its ability to affect key internal processes.” • Tripwires “Diagnostic tests are performed on the system (possibly without its knowledge) and a mechanism shuts down the system if dangerous activity is detected.” Viewed from an RE perspective, it seems as though most of what is discussed here could be elaborated as constraint requirements. Both current theory and practice of RE will most likely be sufficient for dealing with capability control as laid out here. With regard to the ideas on motivation selection, Figure 2 shows an overview of the related techniques summarized as  Motivation Selection  Direct specification  Domesticity  Indirect normativity  Augmentation  Fig. 2. Overview of motivation selection techniques  follows [1, p. 143]: • Direct specification “The system is endowed with some directly specified motivation system, which might be consequentialist or involve following a set of rules.” • Domesticity “A motivation system is designed to severely limit the scope of the agent’s ambitions and activities.” • Indirect normativity “Indirect normativity could involve rule-based or consequentialist principles, but is distinguished by its reliance on an indirect approach to specifying the rules that are to be followed or the values that are to be pursued.” • Augmentation “One starts with a system that already has substantially human or benevolent motivations, and enhances its cognitive capacities to make it superintelligent.” In our opinion, these ideas will be much harder to elaborate according to the current state of the art in RE than the ideas on capability control above. When interpreting some of them in such a way as involving dynamic goals of the superintelligence, GORE comes to mind. However, can the current GORE approaches and especially the current theoretical formulation of the RE problem really cover that? V. T OWARDS A N EW T HEORY  OF THE  RE P ROBLEM  In order to address this question, we raise further questions: • What exactly is the requirements problem in this regard? • Can we cast it in terms of the theoretical requirements problem? In this regard, we investigate the state of the art on formulations of the (theoretical) requirements problem. Based on the seminal work of Zave and Jackson [19], Jureta et al. [17] extended the formulation of the requirements problem. The essence of this new formulation is as follows (where the notation is simplified for the purposes of this paper): Given: K domain assumptions G goals Q quality constraints and softgoals A attitudes (preferences) To be found: P plans K, P ∼ G, Q, A An important change in this formulation is the defeasible consequence relation for non-monotonic satisfaction instead  of (monotonic) entailment, based on the insight that the latter would be unrealistic with regard to practice. We consider this a useful variation of the previous formulation in [19], in particular with respect to what is given in terms of goals and softgoals. Hence, this formulation elaborates the RE problem towards GORE. Goals in GORE research are typically wishes (desires) of some agent / stakeholder. In addition, Jureta et al. [17] introduced attitudes for evaluation in terms of degree of favor or disfavor of other elements in the problem formulation. While the formulation in [19] assumed that all given “requirements” are compulsory, the inclusion of attitudes leads to a more realistic formulation where some are not compulsory but optional. It also covers preferences for comparison between different components of the same type, which establishes orders between components of the same type. (In particular, for each preference order in A over softgoals, there is a preference order that maintains that same ordering over quality constraints that stand in the justified approximation relationship to the given softgoals defined in [17].) This makes it possible to define an optimum in terms of stakeholder attitudes. From a practical point of view, we do not think that both quality constraints and softgoals (Q) will normally be given a priori (as assumed in [17]). RE should include the derivation of quality constraints from softgoals. Such derivations are actually the key task of the softgoal approach to RE. In contrast to the original approach in [19], this theoretical formulation involves goals. However, these are only goals of stakeholders in RE, while goals of the system-to-be-built itself are not included. Later, Jureta et al. [20] formally showed the fundamental differences between standard RE (as sketched above) and RE for adaptive systems. A system is adaptive if it can detect differences between its requirements and runtime performance and can adjust its behavior to cope with such deviations. It spans design-time and run-time; design-time, because design decisions influence the range of monitored inputs for the system, and the feedback mechanisms it will have; and runtime, because these mechanisms enable the system to react to at least some changes rather than ignore them. The solution concept, called configurable specification, amounts to a set of requirements configurations and evolution requirements for switching between configurations. Each configuration is shown to satisfy all properties required by [17], [19]. During adaptation, the system switches from one configuration to another and does so because awareness requirements became violated during the last period of stability. The latter RE problem defined for adaptive systems fits  an AI-based software-intensive system most likely better than the previous RE theory. However, it still takes only goals of stakeholders into account like [17] but not goals of the systemto-be-built itself. For more conventional software and systems, this was not necessary, of course. Rational agents like those whose decision-making has been formally verified by Dennis et al. [21] have goals assigned, but statically and at design-time. Such agents can be designed, e.g., using the knowledge level software engineering methodology for agent-oriented programming Tropos [22]. Tropos takes goals of agents into account for building agentoriented software systems. In the course of GORE, the goals of stakeholders are broken down into goals assigned to agents. As long as this is done correctly, the goals of the stakeholders are aligned with the goals of the agents. However, an AI-based system may also create new goals itself at run-time [23]. We assume that a superintelligence will (have to) be able to formulate goals on its own. This is not covered theoretically yet in terms of theoretical formulations of requirements problems. Hence, for treating superintelligence safety as a requirements problem, we propose to develop a new theoretical formulation applicable to it, since it involves goals of both stakeholders and of the AI-based system-to-be-built. In particular, it involves that these goals will be aligned, and that the system-to-be-built can change them itself at run-time. It will be challenging to capture all this in new theoretical formulations of extended requirements problems covering superintelligence safety. VI. C ONCLUSION In this paper, we address the potentially very important issue of “AI safety”, in the sense of superintelligence safety [1], from an RE perspective. To our best knowledge, this is the first RE approach to this issue, although it may seem obvious that RE is the very discipline of choice here. We actually distinguish two different approaches: • Specifying and communicating requirements for specific problems to a concrete superintelligence, and • Doing RE in the course of building a superintelligence in the sense of an AI-based software-intensive system. Based on common wisdom of RE at the current state of the art, we tentatively conclude that ‘perfectly’ specifying and communicating requirements to a superintelligence may not be possible. And even if this became possible in the future, it would not help in case of malevolent superintelligence that would not necessarily satisfy requirements as specified and communicated. Hence, we envision doing RE on a superintelligence to be built. Such an RE endeavor could be informed by some preliminary thought on controlling a superintelligence in [1]. The first approach through capability control may be dealt with properly through constraint requirements. The second approach for controlling through motivation selection, however, appears to go beyond the current theory of RE. In particular,  we raise the challenge of extending GORE with dynamic goals of a superintelligence. After having revisited the existing formulations of the (theoretical) requirements problem, we tentatively conclude that new and extended formulations will be needed for theoretically founded RE on superintelligence. Defining a new requirements problem for a dynamic goal model of the system-to-be-built is a challenge, but the impact on an improved understanding of problems like “AI safety” (in the sense of superintelligence safety) would be great. R EFERENCES [1] Nick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, Oxford, 2014. [2] Stuart Russell, Daniel Dewey, and Max Tegmark. Research priorities for robust and beneficial artificial intelligence. AI Magazine, 36(4), 2015. [3] A. Dardenne, A. van Lamsweerde, and S. Fickas. Goal-directed requirements acquisition. Science of Computer Programming, 20:3–50, 1993. [4] John Mylopoulos, Lawrence Chung, and Eric Yu. From object-oriented to goal-oriented requirements analysis. Commun. ACM, 42(1):31–37, January 1999. [5] Jennifer Horkoff, Fatma Başak Aydemir, Evellin Cardoso, Tong Li, Alejandro Maté, Elda Paja, Mattia Salnitri, Luca Piras, John Mylopoulos, and Paolo Giorgini. Goal-oriented requirements engineering: an extended systematic mapping study. Requirements Engineering, 24(2):133–160, June 2019. [6] H. Kaindl, G. Kainz, A. Leeb, and H. Smetana. How to use limited memory in heuristic search. In Proc. Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), pages 236–242. San Francisco, CA: Morgan Kaufmann Publishers, 1995. [7] H. Kaindl and G. Kainz. Bidirectional heuristic search reconsidered. Journal of Artificial Intelligence Research (JAIR), 7:283–317, 1997. [8] Axel Jantsch and Kalle Tammemäe. A framework of awareness for artificial subjects. In Proceedings of the 2014 International Conference on Hardware/Software Codesign and System Synthesis, CODES ’14, pages 20:1–20:3, New York, NY, USA, 2014. ACM. [9] H. Kaindl, M. Vallee, and E. Arnautovic. Self-representation for self-configuration and monitoring in agent-based flexible automation systems. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 43(1):164–175, Jan 2013. [10] H. Kaindl. Object-oriented approaches in software engineering and artificial intelligence. Journal of Object-Oriented Programming, 6(8):38–45, January 1994. [11] Hermann Kaindl. Is object-oriented requirements engineering of interest? Requirements Engineering, 10(1):81–84, Jan 2005. [12] Mike Mannion, Oliver Lewis, Hermann Kaindl, Gianluca Montroni, and Joe Wheadon. Representing requirements on generic software in an application family model. In William B. Frakes, editor, Software Reuse: Advances in Software Reusability, pages 153–169, Berlin, Heidelberg, 2000. Springer Berlin Heidelberg. [13] Tom Everitt, Ramana Kumar, Victoria Krakovna, and Shane Legg. Modeling AGI safety frameworks with causal influence diagrams. In Proceedings of the Workshop on Artificial Intelligence Safety 2019, volume Vol-2419. CEUR Workshop Proceedings, 2019. [14] David J. Smith and Kenneth G. L. Simpson. Safety Critical Systems Handbook: : A Straightfoward Guide to Functional Safety, IEC 61508 (2010 Edition) and Related Standards, Including Process IEC 61511 and Machinery IEC 62061 AND ISO 13849. Elsevier, 2011. [15] B. Hulin, H. Kaindl, T. Rathfux, R. Popp, E. Arnautovic, and R. Beckert. Towards a common safety ontology for automobiles and railway vehicles. In 2016 12th European Dependable Computing Conference (EDCC), pages 189–192, Sep. 2016. [16] Hermann Kaindl and Davor Svetinovic. On confusion between requirements and their representations. Requirements Engineering, 15(3):307– 311, Sep 2010. [17] I. Jureta, J. Mylopoulos, and S. Faulkner. Revisiting the core ontology and problem in requirements engineering. In 2008 16th IEEE International Requirements Engineering Conference, pages 71–80, Sep. 2008.  [18] John R. Searle. Speech Acts: An Essay in the Philosophy of Language. Cambridge University Press, Cambridge, England, 1969. [19] Pamela Zave and Michael Jackson. Four dark corners of requirements engineering. ACM Trans. Softw. Eng. Methodol., 6(1):1–30, January 1997. [20] Ivan J. Jureta, Alexander Borgida, Neil A. Ernst, and John Mylopoulos. The requirements problem for adaptive systems. ACM Trans. Manage. Inf. Syst., 5(3):17:1–17:33, September 2014. [21] Louise A. Dennis, Michael Fisher, Nicholas K. Lincoln, Alexei Lisitsa, and Sandor M. Veres. Practical verification of decision-making in agent-based autonomous systems. Automated Software Engineering, 23(3):305–359, Sep 2016. [22] Paolo Bresciani, Anna Perini, Paolo Giorgini, Fausto Giunchiglia, and John Mylopoulos. A knowledge level software engineering methodology for agent oriented programming. In Proceedings of the Fifth International Conference on Autonomous Agents, AGENTS ’01, pages 648–655, New York, NY, USA, 2001. ACM. [23] Michael T. Cox. Perpetual self-aware cognitive agents. AI Magazine, 28(1), 2007.  "
"Controlling an Autonomous Vehicle with Deep Reinforcement Learning*  arXiv:1909.12153v1 [cs.RO] 24 Sep 2019  Andreas Folkers1  Matthias Rick1  I. INTRODUCTION Self driving cars have the potential to sustainably change modern societies which are heavily based on mobility. The benefits of such a technology range from self-providing car sharing to platooning approaches, which ultimately yield a much more effective usage of vehicles and roads [1]. In recent years, great progress has been made in the development of these systems, with a major factor being the results achieved through deep learning methods. One example is the neural network P ILOT N ET, which was trained to steer a vehicle solely based on camera images [2], [3]. More classic methods divide the processing of sensor data and the calculation of vehicle controls into separate tasks. The latter can be achieved by various model-based control approaches, one method being the linear quadratic controller [4], also known as Riccati controller. It minimizes a quadratic objective function in the state deviation and control energy while taking into account a linear model of the underlying system. There are various examples for the application of this technique to autonomous driving [5], [6], [7]. While a Riccati-controller is comparably fast, however, it does not directly allow the consideration of constraints such as obstacles or more advanced objective functions within the optimization. Such requirements are met by a general nonlinear model predictive control (MPC) approach based on solving an optimal control problem in every time step. Although the calculations required are considerably more complex, such methods were successfully implemented for autonomous vehicles, e. g., [8] or most recently [9] utilizing efficient solvers such as T RANS WORHP [10] based on WORHP [11]. *The authors would like to thank Deutsches Zentrum für Luft- und Raumfahrt (DLR) Raumfahrtmanagement, Navigation in Bonn-Oberkassel for providing the research vehicle [grant number 50NA1615]. 1 WG Optimization and Optimal Control, Center for Industrial Mathematics, University of Bremen, Germany afolkers@uni-bremen.de  Christof Büskens1  A combination of the advantages of both, the speed of the Riccati controller and the generality of MPC, can be achieved by finding a function that maps state values to control variables, e. g., by training a deep neural network. Such a model could, for example, be learned supervised, as done for P ILOT N ET, or by reinforcement learning. The latter in particular led to excellent results in the training of such agents for controlling real-world systems such as robots [12] or helicopters [13]. Recent work also shows promising applications of reinforcement learning for autonomous driving by making strategic decisions [14], [15] or by the computation of control commands [16], [17], [18]. Although these results show the success of this approach in simulated environments, there are very few examples of evaluations on real vehicles. One of them was presented by the WAYVE research team where a policy for lane following is learned based on corresponding camera images. Training is done onboard, with the only feedback for improvement coming from the intervention of a safety driver. While this method works when training is carried out without the proximity of real obstacles and at low speeds, it may be difficult to implement this approach for more general situations. In this work, we show how to realize the autonomous exploration of a parking lot based on deep reinforcement learning. In particular, this setting is much more challenging than simple lane following due to sharp turning maneuvers and road constrictions caused by obstacles. To this end, we describe how a policy is trained to compute sophisticated control commands which depend on an estimate of the current vehicle state. This is done by designing an appropriate Markov decision process and a corresponding proximal policy optimization [20] learning algorithm. For that purpose a simulated environment is used for data generation. Performance of the resulting deep controller is evaluated in both, simulation and real-world experiments. To the best of our knowledge, this work extends the state of the art results for successfully driving an autonomous vehicle by a deep reinforcement learning policy. II. CONTINUOUS DEEP REINFORCEMENT LEARNING In recent years, various methods from classical reinforcement learning [21] have been combined with neural networks and their optimization through backpropagation [22], leading to algorithms such as Deep Q-Learning [23], [24], [25], and several actor-critic-methods [18], [26], [27]. In particular, the latter class of algorithms allows agents to be trained with continuous action spaces, which is crucial for their  application as controller on a real-world system like an autonomous vehicle. Our training procedure is based on the proximal policy optimization (PPO) algorithm [20]. For this, an infinite Markov decision process (MDP) is considered, s0 defined by the six-tuple (S, A, Psa , P0 , r, γ), with S and A being the continuous and bounded spaces of States and s0 Actions, respectively. The probability density function Psa characterizes the transition from state s ∈ S to s0 ∈ S given action a ∈ A while P0 incorporates a separate distribution of possible start states. The reward function is denoted by r : S × S × A → R and γ is the discount factor. With these definitions the goal of a reinforcement learning algorithm is to find an optimal policy, π : S × A → [0, 1] representing the probability density of the agent’s action, given a certain state. Optimality is specified in relation to the expected discounted return ∞  P t η(π) := Es0 ,a0 ,... γ rt+1 , (1) t=0 s , s0 ∼ P0 (s0 ), at ∼ π(at |st ), st+1 ∼ Pstt+1 at with rt+1 := r(st+1 , st , at ). While in the actor-critic setting the policy π is identified with the actor, the state value function ∞  P k V π (st ) := Eat ,st+1 γ rt+k+1 k=0  will behave as the critic which is responsible to evaluate the policy’s actions. Correspondingly, the state action value function and the advantage are given by ∞  P k π Q (st , at ) := Est+1 ,at+1 γ rt+k+1 and k=0  the tth data point is approximated by Ât which can further be used to define a cost function ζV for V θ as the quadratic error (3) ζV (θ) := −(Ât )2 . Finally, a robust approximation Ât can be computed using the generalized advantage estimation [28] given as Âγ,λ := t  ∞ P k=0  θ (γλ)k δt+k ,  which allows a sophisticated trade-off between bias and variance through the parameter λ > 0. Since the rollout set M is given by trajectories, this estimate can be computed for each data point by summing up to the end of the corresponding episode. Our implementation of the PPO method is summarized in Algorithm 1. Algorithm 1: Proximal policy optimization Initialize πθ and V θ ; while not converged do Set πθ0 ← πθ ; Generate a rollout set M following πθ0 and for each data point; compute Âγ,λ t for K steps do Draw a random batch of M data points from M; Update πθ and V θ using a stochastic gradient descent algorithm with backpropagation and the cost functions ζπ (2) and ζV (3); end end  Aπ (st , at ) := Qπ (st , at ) − V π (st ). To find a (nearly) optimal policy in continuous state and action spaces, it proved helpful to use neural networks as function approximators, which leads to parameterized πθ and V θ . Altogether, a deep actor-critic training algorithm has to find parameters θ to both, maximize the true target (1) and approximate the corresponding state value function V θ . Regarding the former, proximal policy optimization is based on a first order approximation of η around a reference policy πθ0 for the local optimization of the parameters of πθ . The distance between both is approximately measured by ξtθ0 (θ) :=  πθ (at |st ) πθ0 (at |st ) .  The samples (st , at ) to be evaluated are generated within the computation of a rollout set M. For this purpose a total of N data points is computed by following the reference policy πθ0 . If a terminating state is reached in episodic tasks, a new start state is sampled with respect to P0 . Altogether, the PPO objective for training the policy πθ is given by   ζπ (θ) := −E(st ,at )∈M min ξtθ0 (θ)Ât , (2)  clip(ξtθ0 (θ), 1 − ε, 1 + ε)Ât which introduces a pessimistic balancing of two terms controlled by the clip parameter ε > 0 [20]. The advantage of  III. DEEP CONTROLLER FOR AUTONOMOUS DRIVING This work is part of the research project AO-C AR whose objective is the development of algorithms for navigation and optimal control of autonomous vehicles in an urban environment. Here, information about the vehicle’s surrounding are measured by, e. g., laser scanners and are further extended by a rough knowledge about the geometry of the drivable area (see Fig. 1). Based on this, a desired target state z t , including a speed value, is defined as illustrated in Fig. 1b for the exemplary situation of the autonomous parking lot exploration. The measurements and targets are updated at high frequency, ultimately resulting in a control loop. The task of the corresponding controller is to provide steering and acceleration values at every iteration, so that a safe trajectory to the target is obtained. Within this setting, all other vehicles are assumed to be static [9]. Training a deep reinforcement learning agent to implement a controller for solving this task involves the definitions of a corresponding MDP and the topologies of the neural networks πθ and V θ in accordance to it. Finally, an appropriate environment has to be designed in which the network parameters are learned through Algorithm 1. Here, in contrast  v β y % (a) Sensors.  t  (b) Target state z .  L  (c) Perception Ø.  Fig. 1: During the exploration of a real parking lot, obstacles are perceived by sensors (a). Together with its knowledge about the drivable area, a current target state is defined (b). The surrounding from the perspective of the vehicle can be described by a coarse perception map where the target is represented by a red dot (c).  to [19], training is preformed within a simulation in order to quickly obtain a model as general as possible from various and diverse situations. Within the resulting episodic MDP, the policy has to make the vehicle reach the desired target state including a specified speed value. In particular, this should lead to a controlled stop, if the latter is zero. After this training step, the resulting agent can be deployed to control a real-world vehicle. Details of the simulated MDP are presented in the following. A. State and Action Spaces The research vehicle can be controlled algorithmically by specifying the steering wheel angle ν as well as the longitudinal acceleration a. The former can be mapped bijectively to the mean angle of the front wheels, defined as β. To prevent arbitrary fast changes, the angular velocity ω = β̇ is defined as part of the action space instead of β or ν, leading to A := {(a, ω)> ∈ R2 | both bounded}. These control variables have direct influence on the set of vehicle coordinates Z := {(x, y, v, %, β)> ∈ R5 | v, %, β bounded} as shown in Fig. 2. The tuple (x, y) describes the position of the center of the vehicle’s rear axle with respect to an inertial system. The speed in the longitudinal direction is called v, where v̇ = a, and the orientation of the vehicle with respect to the inertial system is referred to as %. Both, position and orientation of the current target z t ∈ Z, can be expressed by the bounded relative position (xr , y r ) with respect to the vehicle coordinates z ∈ Z and the complex number representation (%r< , %r= ) of the corresponding relative orientation. The form of the latter has the advantage of avoiding discontinuities. Moreover the controller needs to know the desired target speed as well as the current speed of the vehicle in order to predict its next states and to allow safe driving maneuvers. The target steering angle β t is not of interest, which ultimately leads to the first part of the state space S̃ = {(xr , y r , v, v t , β, %r< , %r= )> ∈ R7 | all bounded}.  x  (x, y)  Fig. 2: Left: Coordinates of the vehicle state vector. Right: Single track model simplification.  The second component is given by the vehicle’s relative perception of its surrounding, combined with the a priori knowledge about the drivable area and the position of the target. This can be defined by a coarse grid Ø of size n × m with entries in {−1, 0, 1}, as shown in Fig. 1c. Finally the MDP state space is specified by S := {(s̃, Ø) | s̃ ∈ S̃, Ø ∈ {−1, 0, 1}n×m }. B. State Transitions The action-dependent transition between states of a real vehicle can be incorporated within the simulation using a system of differential equations to describe its behavior. This leads to an update of the relative vehicle coordinates as well as a new measurement of the obstacle perception. For low speeds, kinematic considerations such as simple single-trackmodels as in [29] or [30] are sufficient. Here, the vehicle is assumed to have only one wheel at the front and back respectively, each centered between the real ones as shown in Fig. 2. This leads to the system of equations     ẋ v cos(%)  ẏ   v sin(%)      ,  v̇  =  a      %̇  v/L tan(β) ω β̇ where L is the vehicle-specific wheelbase. Further physical constraints, such as a limited steering angle, can be directly considered within the simulation. C. Reward Function The reward function should encode the goal of the agent to reach the given target position with an appropriate orientation and speed. However, since the MDP at hand is continuous, it is almost impossible to ever fulfill this task starting with a random policy and to learn from such a success. This is in particular true in the difficult case of a target speed of zero, which requires an exact stopping maneuver at the end of the corresponding episode. As a result, training is performed in two phases and two separate policies, DRIVER and STOPPER, are learned depending on the task. While the former is rewarded for quickly reaching the desired speed, the STOPPER should approach the target slowly and stop in the end. For both models, the first learning phase rewards proximity at every time step of the episode. Reaching the final position or rather  performing a stop is especially highly rewarded. For the exemplary case of the DRIVER this results in r1D := cp ∆p + [r0 ]p + [1/2r0 ]p,% ,  cp , r0 ∈ R+ ,  where ∆p measures the squared proximity to the target and [·]p (or [·]p,% ) is only granted if the agent reaches the desired position (or position and orientation respectively). As soon as a policy learned to fulfill the goals of the first phase, additional behaviors such as an appropriate speed and small steering angles are taken into account, which is given by r2D := r1D + cv ∆v + cβ ∆β,  cv , cβ ∈ R+  in case of the DRIVER. Here, ∆v and ∆β measure the proximity of speed and current steering angle. For learning the STOPPER model, similar rewards apply and are complemented with, e. g., an additional weight on the desired speed in the first phase. D. Policy and Value Function as Neural Networks As suggested in Sect. II, two neural networks are trained to be identified as policy πθ and value function V θ . Here, both share the same topology but not the same parameters. The state component s̃ ∈ S̃ is processed by two dense layers of each 200 ReLU activations, as shown in Fig. 3. On the other hand, the evaluation of the perception map Ø is based on two convolutions, which allow to learn from the structure of the input. The spatial dimension is halved by a max pooling operation both times. While the first convolution consists of C = 30 feature maps {Σc }c=1,...,C , the latter is reduced to only one, which is then flattened and also processed by a dense layer of 200 ReLU activations. The result of both inputs is then concatenated and passed through a last 200 ReLU layer. Finally, the output of the value function model V θ is computed by a subsequent dense layer of one linear activation. Furthermore, the return of the policy πθ is a pair (µ, σ) for each possible action, which is identified with the mean and standard deviation of a gaussian probability distribution. While the former is computed based on a tanh activation, the latter is defined to be independent from the input, which yields a general measure of how certain the model is about its actions. In particular, the noise introduced by σ controls the policy’s level of exploration when defining the rollout set M. s̃ ∈ S̃ µ  Furthermore, it results in robustness against disturbances in the expected state transition with respect to the most preferred action µ. Constraints on the control commands can be incorporated by properly scaling the tanh activation and an additional clip operation in the case of action selection based on the normal distribution. IV. TRAINING AND RESULTS We evaluate our training procedure and present the performance of the resulting policy in simulation and on a real vehicle. Learning is realized on a GTX 1080 GPU while the simulator is conducted by an Intel Xeon E5 CPU kernel. Within the latter, random control tasks are generated as shown in Fig. 4. The reward is shaped to make the policy drive the vehicle to the target state, which would define the end of an episode. Alternative termination criteria are the collision with an obstacle or the boundary polygon, exceeding a speed value of 3.3 m/s (12 km/h) as well as reaching the maximum time step T = 250. The simulated time between two such steps is defined to be 100 ms. The control values a and ω are bounded by ±1.2 m/s2 and ±1.2 rad/s respectively. Proximal policy optimization is carried out by collecting rollout sets consisting of 16,384 time steps, which are then used within K = 16 optimization steps with batches of size M = 1,024. One such epoch takes on average 150 s, while a maximum of 350 epochs is required for convergence. This results in a training time of less than 15 h. However, policies of similar quality can be obtained even without incorporating other vehicles as obstacles during the learning process, as outlined in the next section. The resulting simulation is much faster without detailed computation of the agent’s perception, which leads to optimization cycles of 90 s. This reduces the maximum training time to 9 h. The typical development of the average reward during training is shown in Fig. 5. As one would suggest, convergence of a STOPPER policy is much slower in the beginning, due to the fact that it is harder for the agent to learn about  Input s̃ ∈ S̃ ReLU  Ø σ  tanh constant linear  V  3 × 3 ReLU convolution 2×2 max pooling  {Σc }c=1,...,C  Fig. 3: Neural network topology of policy πθ (top) and value function V θ (bottom).  Fig. 4: Randomly selected scenarios computed by the simulator. The initial state is represented by the dark blue vehicle at the center of the coordinate system and includes random values for speed v and steering angle β. The light blue car defines the target. Red dots indicate sensor measurements of obstacle vehicles and the drivable area is defined by a closed polygon.  1  1  1  0.5 0.5  0.5 DRIVER  STOPPER  0 100 200 300 400 Epochs  0  its final goal. D RIVER models usually converge in less than 200 epochs in the simplified setting, which results in a computation time of 5 h. Furthermore, the overall rewards are considerably lower in the case of obstacle vehicles included during learning. Since that task is more difficult, this result is also to be expected. A. Results within the Simulation We evaluate DRIVER and STOPPER models resulting from learning with obstacle vehicles (type A) and without them (type B), while all other parameters remain identical. In particular, we demonstrate their performance within the simulated environment which is summarized in table I. The evaluation is done based on 10,000 randomly generated control tasks including other vehicles. Results show high success rates with other terminations mainly due to obstacles. Further investigations show that this outcome is very often caused by an infeasible combination of initial orientation, speed and steering angle with respect to the placement of obstacles which would trigger an emergency brake in reality. Even though type B models were only trained based on the polygon representing the drivable area, they still achieve results of similar quality than the type A agent. In case of the DRIVER model they are even better. Most importantly this indicates that type B models are capable of handling unknown obstacles, even if those lead to constrictions on the lane. This result is further supported by Fig. 6 and Fig. 7 which present an exemplary task solved by a type B STOPPER policy. The former displays the path as well as the corresponding TABLE I: Comparison of termination criteria and average reward of models trained with (type A) and without obstacles (type B). Values are generated in 10,000 simulation runs including other vehicles. The average cumulated reward per episode is normalized to their respective maximum.  D RIVER S TOPPER  Obstacle Training X (A) (B) X (A) (B)  ∅ Rew. 1.0 0.984 0.967 1.0  Succ. 90.7 87.5 80.8 82.9  0  100 200 300 400 Epochs  Fig. 5: Normalized average reward during learning of the DRIVER and STOPPER policy. The dotted lines represent training with obstacle vehicles (type A), while the solid lines indicates that they were excluded (type B). Progress is displayed until the respective maximum value is reached.  Policy  v  −1  0 0  0 −0.5  Termination [%] Coll. Time Speed 6.1 3.2 0.0 10.5 2.0 0.0 11.5 7.8 0.0 14.5 2.6 0.0  β  vt , β t  100 200 Time steps  Fig. 6: Exemplary solution of a STOPPER agent which was trained without obstacle vehicles (type B). The values in the right plot are normalized to their respective maximum.  Target Agent  Fig. 7: Perception input Ø (left) and its attention map A (right) of the initial situation in Fig. 6. While dark blue pixels indicate lowest attention, it is at maximum for red ones.  normalized speed and steering values when applying this agent. In particular, the agent is not pulling out before turning to take the obstacle vehicle on the lane into account. Fig. 7 shows the perception map Ø of the initial situation in Fig. 6 relative to the agent. To evaluate what parts of it are most important to the neural network, an attention map A can be computed as suggested in [31]. For that the feature maps {Σc }c=1,...,C of the first convolutional filter (see Fig. 3) PC 2 are squared and summed up, yielding A := c=1 Σc . In the situation at hand, high attention is paid to the agent’s immediate surrounding as well as to important points in the long term such as the corner, where it has to turn around, or the surrounding of the target. In addition, comparably high attention is paid to the obstacle standing on the lane in front of it. Altogether, these results show that a high quality agent can be obtained even with a simple simulation in short training time. B. Results on a Real Vehicle After the training in simulation has been completed, a STOPPER and DRIVER model are combined into one deep controller. This is applied within the system for autonomous driving, developed as part of the research project AO-C AR [9], as the main controller during the autonomous exploration of a parking lot (as described in Sect. III). Experiments are performed on a standard Volkswagen Passat GTE Plug-in-Hybrid with additional laser scanners at its front and rear2 . During exploration, an estimate of the vehicle’s state is computed every 20 ms using an Extended Kalman Filter as 2 For further details visit www.math.uni-bremen.de/zetem/ aocar  relative y position  relative x position Fig. 8: Exemplary path (blue line) taken by the deep controller when five times circling a real parking area counterclockwise. The positions are relative to the prior knowledge about the parking lot (black lines). The start is marked by a black dot. The underlying satellite image ( c 2009 G EO BASIS -DE/BKG) is inserted as a reference only (so it does not show the actual occupancy of parking spaces).  described in [32]. From this, a current target state is deduced and corresponding control commands are computed by the controller. Due to the target being updated at high frequency, the vehicle continues driving until a position where to stop is provided. Since the outputs of the deep controller are the acceleration and the steering angle velocity, the latter is numerically integrated for another 20 ms to define the desired steering angle and thus ultimately the vehicle control. The computation of one control command by the neural network representing the policy takes approximately 1.2 ms on an Intel Core i7-4790 CPU. General characteristics of the deep controller can be evaluated on the basis of Fig. 8. The driven path is in the center of the lane at all times, leading to a high safety of the corresponding trajectory. This applies in particular to the turning maneuvers, after which the vehicle is immediately aligned with the lane again. One can further notice that the paths taken in every turn are very similar to each other. The only deviations occur at the start or in the case of a third party vehicle influencing the control commands (top left). Fig. 9 shows three specific scenarios which can successfully and safely be handled by the proposed controller. In particular, we overlaid a series of state estimates and the corresponding trajectories that the agent would execute based on the vehicle’s current perception. One can see that the controller is able to perform highly challenging maneuvers such as sharp turnings with unknown obstacles. The system is further able to make decisions based on new objects which makes the controller execute an evasive maneuver or a soft stop. In all cases, following the deep controller leads to very smooth driving behavior of the research vehicle. Altogether, the presented method is able to provide sophisticated control commands while still being able to fulfill strict requirements on the computation time.  Fig. 9: Application of the deep controller on a real vehicle in three exemplary scenarios. Red dots indicate laser measurements and the drivable area is marked by black lines. The currently planned path of the controller is given by trajectories with a color gradient from light yellow to dark red. Top: Turning. Middle: Driving around an obstacle and stopping afterwards. Bottom: Detection of a blockade on the street and stopping in front of it.  V. CONCLUSION We presented a general design approach for a nonlinear controller that is able to provide highly advanced control commands in extremely short computation time. This was done by approximating the control problem within a simulation as a Markov decision process which was then solved by an agent in the setting of a reinforcement learning problem. Training was performed by a proximal policy optimization method with the policy being defined as a neural network. We evaluated our approach in the context of autonomous driving and showed that a high quality controller could be obtained within a few hours of training. Furthermore, we demonstrated its performance on a full-size research vehicle during the autonomous exploration of a parking lot. For example, the controller was able to handle sharp turnings as well as unknown obstacles by performing an evasive maneuver or a stop. In particular, this work is one of the first successful and, to the best of our knowledge, the currently most general application of a deep reinforcement learning agent to a real autonomous vehicle. Future work will include a more detailed analysis of the neural network structure and the state representation as well as applications to further scenarios. The training process as well as the evaluation on the research vehicle are available as a video at https://youtu.be/1HwHdL7bY3A.  APPENDIX TABLE II: Overview of hyperparameters T RAINING Descript. Var. Steps / episode T Step length Rollout size N K Optim. / rollout Minibatch size M Discount γ Gener. advantage λ clip parameter ε Scaling of ζV α Adam optimizer (cf. [33]) β1 β2   Value 250 0.1 [s] 16,384 16 1,024 0.99 0.95 0.1 0.1 5e−5 0.9 0.999 1e−5  Descript. D RIVER  R EWARD Var. Value cp 0.1 cv 0.5 cβ 0.5 r0 50  DYNAMICS Descript. Bounds Acc. |a| ≤ 1.2 [m/s2 ] Ang. vel. |w| ≤ 1.2 [rad/s] Steering |β| ≤ 0.55 [rad] Speed v ∈ [0, 3.3] [m/s]  R EFERENCES [1] M. Maurer, J. C. Gerdes, B. Lenz, and H. Winner, Eds., Autonomous Driving: Technical, Legal and Social Aspects. Heidelberg: Springer, 2016. [2] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba, “End to end learning for self-driving cars,” 2016. [3] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel, and U. Muller, “Explaining how a deep neural network trained with end-to-end learning steers a car,” 2017. [4] K. Ogata, Modern Control Engineering, 5th ed. Pearson, 2010. [5] D. Kim, J. Kang, and K. Yi, “Control strategy for high-speed autonomous driving in structured road,” International IEEE Conference on Intelligent Transportation Systems, pp. 186–191, 2011. [6] F. Lin, Z. Lin, and X. Qiu, “LQR controller for car-like robot,” in 35th Chinese Control Conference (CCC), 2016, pp. 2515–2518. [7] N. Tavan, M. Tavan, and R. Hosseini, “An optimal integrated longitudinal and lateral dynamic controller development for vehicle path tracking,” Latin American Journal of Solids and Structures, vol. 12, pp. 1006–1023, 2015. [8] P. Falcone, F. Borrelli, J. Asgari, and D. Hrovat, “Low complexity MPC schemes for integrated vehicle dynamics control problems,” International Symposium on Advanced Vehicle Control, 2008. [9] L. Sommer, M. Rick, A. Folkers, and C. Büskens, “AO-Car: transfer of space technology to autonomous driving with the use of WORHP,” in Proceedings of the 7th International Conference on Astrodynamics Tools and Techniques, 2018. [10] M. Knauer and C. Büskens, “From WORHP to TransWORHP,” in Proceedings of the 5th International Conference on Astrodynamics Tools and Techniques, 2012. [11] C. Büskens and D. Wassel, “The ESA NLP solver WORHP,” Modeling and Optimization in Space Engineering, vol. 73, pp. 85–110, 2013. [12] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates,” 2016. [13] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng, “An application of reinforcement learning to aerobatic helicopter flight,” in Advances in Neural Information Processing Systems 19, B. Schölkopf, J. C. Platt, and T. Hoffman, Eds. MIT Press, 2007, pp. 1–8. [14] D. Isele, R. Rahimi, A. Cosgun, K. Subramanian, and K. Fujimura, “Navigating occluded intersections with autonomous vehicles using deep reinforcement learning,” 2017. [15] B. Mirchevska, M. Blum, L. Louis, J. Boedecker, and M. Werling, “Reinforcement learning for autonomous maneuvering in highway scenarios,” in Workshop Fahrerassistenzsysteme und automatisiertes Fahren, 2017. [16] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement learning framework for autonomous driving,” Electronic Imaging, Autonomous Vehicles and Machines, pp. 70–76, 2017.  [17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,” 2015. [18] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in Proceedings of The 33rd International Conference on Machine Learning, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48, 2016, pp. 1928–1937. [19] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam, A. Bewley, and A. Shah, “Learning to drive in a day,” 2018. [20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” 2017. [21] R. S. Sutton and A. G. Barto, Reinforcement learning - an introduction, ser. Adaptive computation and machine learning. MIT Press, 2010. [22] D. E. Rumelhart, G. Hinton, and R. J. Williams, “Learning representations by back-propagating errors,” Nature, vol. 323, pp. 533–536, 1986. [23] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassab"
"A Framework for Data-Driven Robotics  arXiv:1909.12200v1 [cs.RO] 26 Sep 2019  Serkan Cabi, Sergio Gómez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong, Konrad Żołna, Yusuf Aytar, David Budden, Mel Vecerik, Oleg Sushkov, David Barker, Jonathan Scholz, Misha Denil, Nando de Freitas, Ziyu Wang  I. I NTRODUCTION Deep Learning (DL) has successfully advanced many fields of Artificial Intelligence (AI), including computer vision [1], [2], speech recognition [3]–[5], natural language processing [6], and Reinforcement Learning (RL) [7], [8]. The success of DL in each of these fields was made possible by the availability of huge amounts of labeled training data. Researchers in vision and language can easily train and evaluate deep neural networks on standard datasets with crowdsourced annotations such as ImageNet [9], COCO [10] and CLEVR [11]. In simulated environments like video games, where experience and rewards are easy to obtain, Deep RL is tremendously successful in outperforming top skilled humans by ingesting huge amounts of data [8], [12], [13]. The OpenAI Five DOTA bot [13] processes 180 years of simulated experience every day to play at a professional level. Even playing simple Atari games typically requires 40 days of game play [7]. This stands in contrast to robotics, where we lack abundant data since designing a policy typically requires execution on a real robot, which cannot be accelerated beyond real time. This limits the effectiveness of Deep RL in robotics. This paper presents a framework to bring the power of Deep RL to robotics through an approach that makes the generation and handling of a large dataset tractable. While previous attempts required several robots operating in parallel [14], [15], our framework requires only a single robot. To generate enough data to feed the data hungry machinery of Deep RL with a single robot we record experience continuously and persistently, regardless of the purpose or quality of the behavior. This means that for a new task our data is necessarily off policy, and is also typically off task as This work is done at Deepmind. Videos are available on this website: https://sites.google.com/view/data-driven-robotics/  Demonstrations Reward Sketching  Evaluation Storage  Batch RL  Learning Reward  Fig. 1: Left: illustration of the proposed framework (see Fig. 2 for more details). Top right: reward annotation. Bottom right: stacking objects and lifting cloth manipulation tasks. well. Nevertheless, we show that the accumulated experience can be reused to learn new tasks. Reuse is made possible by automatically annotating historical data using task-specific learned reward model. Our framework introduces three novel components: 1) The NeverEnding Storage (NES) system, which captures and stores all experience generated by the robot, regardless of purpose. 2) The reward sketching procedure for elicitating reward functions from humans, so that past experience can be automatically annotated for new tasks. 3) The use of off-policy batch RL allows us to train as many policies as possible given the computation budget, using data that is generated using different behavior policies without further execution on the robot. The NES system captures all the camera and sensor data generated by the robot. This includes demonstrations (by human teleoperators) of various tasks, behaviors generated by trained policies for these tasks (successful or not), as well as experience generated by scripted or random policies. If the robot is moving, then the experience is being captured by NES. Our approach to task specification relies on human judgments about progress toward the goal to define task-specific reward functions. Annotations are elicited from humans in the form of per-timestep reward annotations using a process we call reward sketching (Fig. 1, top right). The sketching procedure is intuitive for humans, and allows them to label many timesteps rapidly and accurately. We use the human annotations to train a reward model, which is then used to annotate the remainder of the database automatically. The use of a learned reward function in this way allows us to repurpose an arbitrary amount of past experience using a fixed amount of annotation effort per task.  State Estimation Environment Reward System Identification World Model Domain Gap  Classic Yes No Yes Yes No  Pure RL Yes Yes No No No  Sim2real No Yes∗ Yes Yes Yes  Ours No No No No No  TABLE I: Comparison between several frameworks in robotics, showing different challenges that arise in each of them. We address these limitations with our newly proposed framework. (∗ Sim2real does not require reward from the real world, but does require reward in the simulated environment.) Table I compares our framework to other common approaches in robotics. Through learning of both reward functions and controllers we avoid the need for explicit state estimation, which is an essential component of classic robotics. RL systems also typically rely on state estimation to deliver rewards, even when they learn policies from pixels. System identification and environment modelling arise in any situation where explicit models (either of the robot or of the task) are required and significant effort is needed to build these models in both classical robotics and sim2real. Similarly, unlike the sim2real setting we have no perceptual domain gap to address, since we always train on experience collected directly from the robot. II. R ELATED W ORK RL: RL has a long history in robotics [14]–[20] but applying RL to robots inherits all the general difficulties of applying RL to the real world [21]. Most published works either rely on state estimation for a specific task, or work in a very data limited regime to learn from raw observations. These methods typically entail highly engineered reward functions. Our work handles these limitations. Besides, we go beyond the usual scale of RL methods in robotics. Among the RL for robotics literature, QT-Opt [15] is the closest approach to ours. The authors collected a dataset of over 580,000 grasps over the course of several weeks with 7 robots. The resulting distributed Q-learning agent shows remarkable generalization to different objects. Yet, the whole system focuses on a single task: grasping. Grasping is particularly well-suited for hand-engineering a reward function and scripting policies for collecting data. However, neither hand-engineering reward functions nor scripting policies are easy to design for many tasks. So, relying on these techniques limits the applicability of the method. In contrast, in our work we learn the reward functions. Besides, experiences for multiple tasks contribute to learning a target task. Demonstrations: Demonstrations in RL have gained popularity in recent years [22], [23] as they help to address the exploration problem. As in prior works [24]–[26], we use demonstrations as part of the agent experience and train with temporal difference learning in a model-free setting. Demonstrations can also be incorporated through Behavioral Cloning (BC) [27], [28], but this requires high-quality consistent demonstrations of the target task. As such, it cannot benefit from heterogeneous data. Moreover, BC policies cannot outperform the human demonstrator.  Sim2real: The sim2real framework aims to seamlessly transfer policies trained in simulated environments to the real world [29]–[33]. These methods have achieved remarkable success in dexterous in-hand manipulation [32] and deformable object manipulation [34]. The challenges of sim2real lie in the need for extensive system identification to construct simulated environments that closely mirror the target task. Even with careful simulation design there remains a domain gap between simulation and reality that needs to be dealt with [30], [34]. In our framework there is no need to simulate the robot or the environment, and, thus, no domain gap. Reward learning: Learning reward functions using inverse RL dates back to Ng and Russell [35] and has achieved tremendous success recently [36]–[42]. This class of methods works best when applied to states or well-engineered features. Making it work for high-dimensional input spaces, particularly raw pixels, remains a challenge. Learning from preferences [43]–[48] also has a long history. For example, interactive learning and optimization with human preferences has been applied to animation [49]–[51]. Preference learning is also used in RL for reward learning [52], [53]. Preferences can be extracted with whole episode comparisons [54]–[57] or shorter clip comparisons [58], [59]. Binary success labels of frames are also used to learn reward functions [26], [60]. In our framework we propose a new form of annotation, reward sketching (Section IV-C), that allows perceptual reward learning [61] from any trajectory, not only successful demonstrations. Batch RL: Batch RL describes the scenario where the learning experience is fixed a priori [62]. We rely on Batch RL for training our agents. This is an active area of research, with a number of recent works aimed at improving its stability [63]–[66]. These additional advancements can further improve our agents. III. F RAMEWORK OVERVIEW AND MOTIVATION The general workflow is illustrated in Fig. 2. The distinct properties of our pipeline come from the interaction of the two key features: NES and learning the reward model from sketches. NES allows us to accumulate a large dataset of task-agnostic experience. A task-specific reward model allows us to retrospectively annotate data in NES with reward signals for a new task. With rewards, we can then train batch RL agents with all the data in NES. The procedure for training an agent to complete a new task using our framework has the following steps which are described in turn in the remainder of the section, with more details given in Section IV: 1) A human teleoperates the robot to provide first-person demonstrations of the target task (IV-A). 2) All robot experience, including these demonstrations, is accumulated into NES (IV-B). 3) A subset of data from NES (including the task-specific demos) are annotated by humans with reward sketches for the target task (IV-C).  Evaluate Execute  Learn Q/pi  Learn R  7  Task Specific Task Agnostic  6  Teleoperate  5  NeverEnding Storage  4  Labeled Experience  No Human Operator  2  Sketch  1  Robot Cloud  3  Human Operator  Fig. 2: Structure of the data driven workflow. See main text for a description of each step. 4) A reward model for the target task is trained using the labelled experience (IV-D). 5) An agent for the target task is trained using all experience in NES, with rewards provided by the learned reward model (IV-E). 6) The resulting policy is deployed on a real robot for execution, which at the same time records more data into NES (IV-F). 7) Occasionally we select an agent for careful evaluation, to track overall progress on the task. Teleoperation: To specify a new target task, a human operator first remotely controls the robot to provide several successful (and optionally unsuccessful) examples of completing the task. By employing the demonstration trajectories, we circumvent the problem of exploration in RL: Instead of requiring that the agent explores the state space autonomously, we use expert knowledge about the intended outcome of the task to guide the agent. NeverEnding Storage: NES captures all of the robot experience generated across all tasks in a central repository. This allows us to make use of historical data when learning a new target task, instead of generating a new dataset from scratch each time. NES includes teleoperated trajectories for various tasks, human play data, and accumulated experience from the execution of numerous learned policies. The experience data is stored together with various metadata for organization and retrieval. Reward Sketching: The second step in task specification is reward sketching. We ask human experts to provide pertimestep annotations of reward using a custom user interface. As illustrated in Fig. 3, user draws a curve indicating the progress towards accomplishing the target task as a function of time, while the interface shows the frame corresponding to the current cursor position. This intuitive interface allows a single annotator to produce hundreds of frames of reward annotations per minute. The reward sketches allow comparison of perceived value of any two frames. In addition, the green region in Fig. 3 is reserved for timesteps where the goal is achieved. For each task the episodes to be annotated are drawn from NES. They include both the demonstrations provided for the target task, as well as experience generated for prior tasks. Annotating  Fig. 3: Sketch of a reward function. Task: stack green on red. A video sequence (top) with a reward sketch (bottom), shown in blue. Reward is the perceived progress towards achieving the target task. The annotators are instructed to indicate successful timesteps with reward high enough to reach green area. data from prior tasks ensures better coverage of the state space. Reward learning: The reward annotations produced by sketching are used as supervision to train a reward model. This model is then used to predict reward values for all experience in NES. As a result, we can leverage all historical data in training a policy for a new task, without requiring manual human annotation of the entire repository. Off-policy batch RL: We train an agent using off-policy pure batch RL. It allows us to learn as many policies as possible given the computation budget, using data that is generated using different behavior policies without further execution on the robot. Besides, the agent is trained by only using RL objective and it does not use feature pretraining, BC initialization, any special batch correction terms, or auxiliary losses. We do, however, find it important to use the historical data from other tasks. Execution: Once an agent is trained, we can run it on the real robot. By running the agent on the robot, we collect more experience data which can be used for reward sketching or RL in future iterations. Running the agent also allows us to observe its performance and make judgments about the steps needed to improve it. At this point we can iterate the workflow to improve the agent. Typically this involves sketching more data to improve the reward function. We can choose to sketch the new data generated by executing the policy on the robot, or more of the historical data stored in NES. If particular failure cases are observed during execution, we can teleoperate the robot to collect examples near these failure modes. Then, we sketch them to direct an agent in areas of the state space where it does yet not perform well. IV. M ETHODS A. Teleoperation Our approach starts by human operators providing firstperson demonstrations of the target tasks on the robot. The robot is controlled with a 6-DoF mouse or hand-held virtual reality controllers. A demonstrated sequence contains pairs of observations and corresponding actions for each time step t: ((x0 , a0 ), . . . , (xt , at ), . . . , (xT , aT )). Observations xt  contain all available sensor data including raw pixels as well as proprioceptive inputs. In addition to full episodes of demonstrations, interactive interventions can be also performed: A human operator can take over from, or return control to, an agent at any time. This data is useful for fixing particular corner cases that the agents might encounter. All demonstrations are stored in NES with corresponding metadata. Note that in later workflow iterations, all demonstrations are used for RL and possibly reward sketching. B. NeverEnding Storage NeverEnding Storage (NES) is a database of episode data stored as files on disk and metadata associated with it. Episode data includes video recordings from several cameras in the cage and at the robot wrist (Fig. 4). Metadata includes unique episode ID, operator name, date and time of operation, task ID, episode type, a paths to separately stored experience data, and others. Each episode can be marked with an arbitrary set of tags to offer a flexible way to identify sets of relevant episodes. NES also allows to associate an arbitrary number of reward sketches with each episode that could be task-specific. Metadata and reward sketches can be jointly queried with SQL queries to select subsets of data. C. Reward Sketching In order to convey the desired behaviour to the agent, a human operator sketches the reward values for several examples of successful and unsuccessful trajectories for each task. For annotation, we select demonstration episodes as they are likely to exhibit positive examples. At the same time, we select episodes from other tasks as they exhibit negative examples. This combination gives good coverage of the state space. Sketching works on an episodic basis. An example of a sketch is shown in Fig. 3. To sketch an episode, a user interactively selects a frame xt and provides an associated reward value s(xt ) ∈ [0, 1]. The sketching interface allows the annotator to draw reward curves while “scrubbing” through a video episode, rather than annotating frame by frame. This efficient procedure provides a rich source of information about the reward across the entire episode. The sketches for an episode {s(xt )}|Tt=1 are stored in NES as described in Section IV-B. D. Reward Learning Episodes annotated with reward sketches are used to train a reward function in the form of neural network with parameters ψ in a supervised manner. We find that although there is high agreement between annotators on the relative quality of timesteps within an episode, annotators are often not consistent in the overall scale of the sketched rewards. We therefore adopt an intra-episode ranking approach to learn reward functions, rather than trying to regress the sketched values directly. Specifically, given two frames xt and xq in the same episode, we train the reward model to satisfy two conditions.  First, if frame xt is (un)successful according to the sketch s(xt ), it should be (un)successful according the estimated reward function rψ (xt ). Second, if s(xt ) is higher than s(xq ) by a threshold µs , then rψ (xt ) should be higher than rψ (xq ) by another threshold µr . These conditions are captured by the following two hinge losses: Lrank (ψ) = max {0, rψ (xt ) − rψ (xq ) + µr } 1s(xq )−s(xt )>µs Lsuccess (ψ) = max {0, τr1 − rψ (x)} 1s(x)>τs + max {0, rψ (x) − τr2 } 1s(x)<τs  (1)  The total loss is obtained by adding these terms: Lrank + λLsuccess . In our experiments, we set µs = 0.2, µr = 0.1, τs = 0.85, τr1 = 0.9, τr2 = 0.7, and λ = 10. E. Off-policy batch RL We use an algorithm similar to of D4PG [67] as our main training algorithm, but with a recurrent state. It maintains an online value network Q(xt , hQ t , a | θ) and an online policy network π(x, hπt | φ). Given the effectiveness of recurrent values functions [68], both Q and π are recurrent with hQ t and hπt representing the corresponding recurrent hidden states. The target networks are of the same structures as the value and policy network, but are parameterized by different parameters θ0 and φ0 which are periodically updated to the current parameters of the online networks. Given the Q function, we update the policy using DPG [69]: π ∇φ Q(xt , hQ t , π(xt , ht | φ) | θ)  (2)  As in D4PG, instead of using a scalar Q function, we adopt a distributional value function [70]. We refer the readers to the original paper for details on the loss for learning distributional value functions which we use to compute the gradient for learning the critic:   Q 0 ∇θ Ldist Q xt+1 , h̄Q t+1 , ât+1 | θ ) , rψ (xt ), γ, Q xt , ht , at | θ) with ât+1 = π(xt+1 , h̄πt+1 | φ0 )  (3)  During learning, we sample a batch of sequences of observations and actions {xit , ait , · · · , xit+n }i and use a zero start state to initialize all recurrent states at the beginning of sampled sequences. We then update φ and θ following gradients defined in (2) and (3) respectively using BPTT [71]. Since NES contains data from many different tasks, a randomly sampled batch from NES may contain data mostly irrelevant to the task at hand. To increase the representation of data from the current task, we construct fixed ratio batches, with 75% of the batch drawn from the entirety of NES and 25% from the data specific to the target task. This is similar to the solution proposed in previous work [25], where fixed ratio batches are formed with agent and demonstration data.  Type  Wrist depth Basket back  Wrist wide angle 1  No. Episodes  Teleoperation lift green stack green on red random watcher Total  Type  Training object set  Basket front  Fig. 4: The robot setup. We record using 3 cage cameras and 3 wrist cameras.  F. Execution After policies are trained, we choose some of them for execution on the robot. No matter how successful the episodes are, the data is accumulated in NES to train next models. In early workflow iterations, before the reward functions are trained with sufficient coverage of state space, the policies often exploit “delusions” where high rewards are assigned to undesired behaviors. To fix a reward delusion, a human annotator sketches some of the episodes where the delusion is observed. New annotations are used to improve the reward model, which is used in training a new policy. For each target task, this cycle is typically repeated 2–3 times until the predictions of a reward function are satisfactory. V. E XPERIMENTS A. Experimental Setup Robotic setup: Our experimental setup consists of a Sawyer robot with a Robotiq 2F-85 gripper and a wrist forcetorque sensor facing a 35 × 35cm basket. The action space has seven degrees of freedom, corresponding to Cartesian translational and rotational velocity targets of the gripper pinch point and gripper fingers. The agent control loop is executed at 10Hz. For safety, the pinch point movement is restricted to be in a 35 × 35 × 15cm workspace with maximum rotations of 30◦ , 90◦ , and 180◦ around each axis. Observations are provided by three cameras around the cage, as well as two wide angle cameras and one depth camera mounted at the wrist, and proprioceptive sensors in the arm (Fig. 4). NES captures all of the observations, and we indicate what subset is used for each learned component. Tasks and datasets: We focus on two subsets of NES, containing data recorded during manipulations of: 1) three colored objects, one of each color: red, green and blue (rgb dataset, Fig. 3); 2) three deformable objects: a soft ball, a rope and a piece of cloth (deformable dataset, Fig. 1, bottom right). The rgb dataset is used to learn policies for two tasks: lift green and stack green on red, the deformable dataset is used for the pull cloth up task. Final statistics for both datasets are presented in Table II. Both datasets were  K K K K K  No. steps 1.2 1.4 2.4 2.7 7.5  Hours  M M M M M  33 39 67 75 208  No. steps  Hours  592 K 2.8 M 1.2 M 7.8 M  16 78 32 215  (a) RGB dataset.  Wrist wide angle 2  Basket front  6.5 7.9 13.8 13.7 41.0  No. Episodes  Teleoperation pull cloth up random watcher Total  3 15.5 6 41.2  K K K K  (b) Deformable dataset.  TABLE II: Statistics of the two datasets. Note that Total includes off-task data not counted in individual rows, and some of the episodes are counted more than once, e.g. a lift green teleoperation episode is counted as both Teleoperation and lift green.  grown progressively by iterating the process in Fig. 2. Each episode lasts for 200 steps (20 seconds) unless an episode is terminated earlier for safety reasons. To generate initial datasets for training we use a scripted policy called the random watcher. This policy moves the end effector to randomly chosen locations and opens and closes the gripper at random moments in time. When following this policy, the robot occasionally picks up or pushes the objects, but is typically just moving in free space. This data not only serves to seed the initial iteration of learning, but removing it from the final datasets degrades performance of the final agents. The two datasets contain a significant number of teleoperated episodes, although the majority are recorded via interactive teleoperation (section IV-A), and thus required limited human intervention. Only about 600 full teleoperated episodes correspond to the lift green or stack green on red tasks. There are 894, 1201, and 585 sketched episodes for the lift green, stack green on red and pull cloth up tasks, respectively. Approximately 90% of the episodes are used for training and 10% for validation. Notice that not all sketches were obtained at once, but they were accumulated over several iterations of the process in Fig. 2. Reward network architecture: The reward network is a non-recurrent residual network with a spatial softmax layer (Fig. 5). The spatial softmax layer [20] produces a list of 64 (x, y) coordinates. Proprioceptive features are embedded using linear layers, layer-normalized [72], and concatenated with the camera encoding vectors. As the sketched values are in the range of [0, 1], the reward network ends with a sigmoid non-linearity. Agent architecture: The final agent for lift green and stack green on red tasks observes two cameras, a basket front left camera (80 × 128) and one of wrist-  16 filters  USB wrist camera  32 filters  32 filters  64 filters  48  ReLU, 3x3 conv, ReLU, 3x3 conv 3x3 conv Max pool  24  ReLU, 3x3 conv, ReLU, 3x3 conv  64  96  12  ReLU, 3x3 conv, ReLU, 3x3 conv  32  16 filters  16  32 filters 3x3 conv Max pool  48  Spatial softmax  32 filters 3x3 conv Max pool  24  32 filters ReLU 1x1 conv  12  2D expectation coordinates list  Actions + Proprio 256  x1, y1 x2, y2 . . . x64, y64  64  32  12  16  256  16  Value and Policy heads  LSTMs  128 Additional cameras  256  256  Proprio  Fig. 5: Agent network architecture. Note that the wrist camera encoder is shown, but in practice we encode each camera independently and concatenate the results.  Agent Our approach No random watcher data No other task data Non-distributional  Normal  Hard  Unseen  80% 80% 0% 30%  80% 70% 0% 20%  50% 20% 0% 10%  TABLE III: Performance on the lift green task. Agent Our approach No random watcher data No other task data Non-distributional  Normal  Hard  Unseen  60% 50% 0% 20%  40% 30% 10% 0%  40% 30% 0% 0%  TABLE IV: Performance on the stack green on red task.  mounted wide angle cameras (96 × 128) (Fig. 4). The agent for pull cloth up also uses additional basket back left camera (80 × 128). The agent network is illustrated in Fig. 5. Each camera is encoded using a residual network followed by a spatial softmax keypoint encoder with 64 channels. We use one such column for each camera and concatenate the results. Before applying the spatial softmax, noise from the distribution U[−0.1, 0.1] was added to the logits. Proprioceptive features are concatenated, embedded using a linear layer, layer-normalized [72] and passed through a tanh activation and appended to the camera encodings to form joint the input features. The actor network consumes these joint input features directly. The critic network additionally passes them through a linear layer, concatenates the the result with action features (obtained by passing the action through a linear layer), and passes the result through an additional linear layer with ReLU activation. Everything is then fed through a two layer layernormed LSTMs with 256 hidden units each, followed by actor or critic heads. Action outputs are processed through a tanh layer putting them in the range [−1, 1], and then rescaled to their native ranges before being sent to the robot. Training: We train multiple RL agents in parallel and briefly evaluate promising ones on the robot. Each agent is trained for 400k update steps. To further improve performance, we save all episodes from RL agents, and sketch more reward curves if necessary, and use them when training the next generation of agents. We iterated this procedure 2–3 times to get the final agents that we report here.  Evaluation: Finally, we run controlled evaluations on the physical robot with fixed initial conditions across different policies. For the lift green and stack green on red datasets we devise three different evaluation conditions with varying levels of difficilty: • normal – basic rectangular green blocks well represented in training data, large red object close to center in starting position; • hard – more diverse objects that are less well represented in training data, smaller red objects with diverse starting locations; • unseen – green objects unseen during training with a large red object. Each condition specifies 10 different initial positions of the objects (set by human operator) as well as the initial pose of the robot (set automatically). The hard and unseen conditions are especially challenging, since they require the agent to cope with novel objects and novel object configurations. Figure 6 shows examples of the initial conditions. We use the same 3 evaluation sets for both the lift green and stack green on red tasks. To evaluate the pull cloth up task, we randomize initial conditions at every trial. As a quality metric, we measure the rate of successfully completed episodes, where success is judged by the human operator. B. Results Results on the rgb dataset are summarized in Table III and IV. Our agent achieves a success rate of 80% for lifting and 60% for stacking. Even with rarely seen objects positioned in adversarial ways, the agent is quite robust with success rates being 80% and 40%, respectively. Remarkably, when dealing with objects that were never seen before, it can lift or stack them in 50% and 40% of cases. The success rate of our agent for pull cloth up task in 50 episodes with randomized initial conditions is 74%. Our results compares favorably with those of Zhu et al. [41], where block lifting and stacking success rates are 64% and 35%.1 Wulfmeier et al. [73] also attempted the block stacking task. But instead of learning directly from pixels, they rely on QR code based state estimation of fixed set of cubes, whereas our policies can handle a variety of objects. To understand the importance of accumulating robot experience in NES, we run the following ablations. First, 1 The  results are not directly comparable due to different physical setups.  (A): normal  (B): hard  (C): unseen  Fig. 6: Examples of the evaluation initial conditions. we train an agent using only task-specific data that resembles the conventional RL approach. This strategy is interesting because although the amount of training data is reduced, the similarity between training data and target behavior is increased (i.e. the training data is more on-policy). Second, we train an agent while excluding only the random watcher data. As this data is unlikely to contain episodes relevant to the task, we are interested in knowing how much it contributes to the final performance. Tables III and IV show the results of these ablations. Remarkably, using only a task-specific dataset dramatically degrades the performance of the policy. Random watcher data proves to be valuable as it contributes up to an additional 30% improvement, showing the biggest advantage in the hardest case with unseen objects. We also evaluate the effect of distributional value functions. Confirming previous findings [65], [67], we found distributional value functions to be very helpful. For qualitative results we refer the reader to the accompanying video that demonstrates the robustness of our agents. They successfully deal with adversarial perturbations by a human operator, stacking several unseen and non-standard objects and lifting toys, such as a robot and a pony. Last but not least, our agents move faster and are more efficient compared to a human operator. VI. C ONCLUSIONS We proposed a framework for data-driven robotics that makes use of a large dataset of diverse robot experience and reward functions learned from a novel form of human feedback. We presented a successful instantiation of this framework to train policies using pure batch RL. Our experimental results allow us to draw the following conclusions: 1) Reward sketching is an effective way to elicit reward functions, since humans are good at judging progress toward a task goal. Importantly, this approach can be directly applied to many other tasks. 2) Stored robot experience over a long period of time and across different tasks can be efficiently harnessed to learn policies in a completely offline manner. 3) Diversity of training "
"arXiv:1909.12220v1 [cs.CV] 26 Sep 2019  Implicit Semantic Data Augmentation for Deep Networks Yulin Wang1∗ Xuran Pan1∗ Shiji Song1 Hong Zhang2 Cheng Wu1 Gao Huang1† 1 Department of Automation, Tsinghua University, Beijing, China Beijing National Research Center for Information Science and Technology (BNRist), 2 Baidu Inc., China {yulin.bh, fykalviny}@gmail.com, pxr18@mails.tsinghua.edu.cn, {shijis, wuc, gaohuang}@tsinghua.edu.cn   Introduction  Data augmentation is an effective technique to alleviate the overfitting problem in training deep networks [1, 2, 3, 4, 5]. In the context of image recognition, this usually corresponds to applying content preserving transformations, e.g., cropping, horizontal mirroring, rotation and color jittering, on the input samples. Although being effective, these augmentation techniques are not capable of performing semantic transformations, such as changing the background of an object or the texture of a foreground object. Recent work has shown that data augmentation can be more powerful if (class identity preserving) semantic transformations are allowed [6, 7, 8]. For example, by training a generative adversarial network (GAN) for each class in the training set, one could then sample infinite number of samples from the generator. Unfortunately, this procedure is computationally intensive because training generative models and inferring them to obtain augmented samples are ∗ †  Equal contribution. Corresponding author.  33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.  Deep Features  … Augmented Images (Not Shown Explicitly)  Augmented Samples  Deep Networks ISDA Loss  Training Data Augment Semantically  Corresponding to  Figure 1: An overview of ISDA. Inspired by the observation that certain directions in the feature space correspond to meaningful semantic transformations, we augment the training data semantically by translating their features along these semantic directions, without involving auxiliary deep networks. The directions are obtained by sampling random vectors from a zero-mean normal distribution with dynamically estimated class-conditional covariance matrices. In addition, instead of performing augmentation explicitly, ISDA boils down to minimizing a closed-form upper-bound of the expected cross-entropy loss on the augmented training set, which makes our method highly efficient. both nontrivial tasks. Moreover, due to the extra augmented data, the training procedure is also likely to be prolonged. In this paper, we propose an implicit semantic data augmentation (ISDA) algorithm for training deep image recognition networks. The ISDA is highly efficient as it does not require training/inferring auxiliary networks or explicitly generating extra training samples. Our approach is motivated by the intriguing observation made by recent work showing that the features deep in a network are usually linearized [9, 10]. Specifically, there exist many semantic directions in the deep feature space, such that translating a data sample in the feature space along one of these directions results in a feature representation corresponding to another sample with the same class identity but different semantics. For example, a certain direction corresponds to the semantic translation of ""make-bespectacled"". When the feature of a person, who does not wear glasses, is translated along this direction, the new feature may correspond to the same person but with glasses (The new image can be explicitly reconstructed using proper algorithms as shown in [9]). Therefore, by searching for many such semantic directions, we can effectively augment the training set in a way complementary to traditional data augmenting techniques. However, explicitly finding semantic directions is not a trivial task, which usually requires extensive human annotations [9]. In contrast, sampling directions randomly is efficient but may result in meaningless transformations. For example, it makes no sense to apply the ""make-bespectacled"" transformation to the “car” class. In this paper, we adopt a simple method that achieves a good balance between effectiveness and efficiency. In specific, we perform an online estimate of the covariance matrix of the features for each class, which captures the intra-class variations. Then we sample directions from a zero-mean multi-variate normal distribution with the estimated covariance, and apply them to the features of training samples in that class to augment the dataset. In this way, the chance of generating meaningless semantic transformations can be significantly reduced. To further improve the efficiency, we derive a closed-form upper bound of the expected cross-entropy (CE) loss with the proposed data augmentation scheme. Therefore, instead of performing the augmentation procedure explicitly, we can directly minimize the upper bound, which is in fact a novel robust loss function. As there is no need to generate explicit data samples, we call our algorithm implicit semantic data augmentation (ISDA). Compared to existing semantic data augmentation algorithms, the proposed ISDA can be conveniently implemented on top of most deep models without introducing auxiliary models or noticeable extra computational cost. Although being simple, the proposed ISDA algorithm is surprisingly effective, and complements existing non-semantic data augmentation techniques quite well. Extensive empirical evaluations on several competitive image classification benchmarks show that ISDA consistently improves the generalization performance of popular deep networks, especially with little training data and powerful traditional augmentation techniques.  2  Related Work  In this section, we briefly review existing research on related topics. Data augmentation is a widely used technique to alleviate overfitting in training deep networks. For example, in image recognition tasks, data augmentation techniques like random flipping, mirroring 2  and rotation are applied to enforce certain invariance in convolutional networks [4, 5, 3, 11]. Recently, automatic data augmentation techniques, e.g., AutoAugment [12], are proposed to search for a better augmentation strategy among a large pool of candidates. Similar to our method, learning with marginalized corrupted features [13] can be viewed as an implicit data augmentation technique, but it is limited to simple linear models. Complementarily, recent research shows that semantic data augmentation techniques which apply class identity preserving transformations (e.g. changing backgrounds of objects or varying visual angles) to the training data is effective as well [14, 15, 6, 8]. This is usually achieved by generating extra semantically transformed training samples with specialized deep structures such as DAGAN [8], domain adaptation networks [15] or other GANbased generators [14, 6]. Although being effective, these approaches are nontrivial to implement and computationally expensive, due to the need to train generative models beforehand and infer them during training. Robust loss function. As shown in the paper, ISDA amounts to minimizing a novel robust loss function. Therefore, we give a brief review of related work on this topic. Recently, several robust loss functions are proposed for deep learning. For example, the Lq loss [16] is a balanced noise-robust form for the cross entropy (CE) loss and mean absolute error (MAE) loss, derived from the negative Box-Cox transformation. Focal loss [17] attaches high weights to a sparse set of hard examples to prevent the vast number of easy samples from dominating the training of the network. The idea of introducing large margin for CE loss has been proposed in [18, 19, 20]. In [21], the CE loss and the contrastive loss are combined to learn more discriminative features. From a similar perspective, center loss [22] simultaneously learns a center for deep features of each class and penalizes the distances between the samples and their corresponding class centers in the feature space, enhancing the intra-class compactness and inter-class separability. Semantic transformations in deep feature space. Our work is motivated by the fact that highlevel representations learned by deep convolutional networks can potentially capture abstractions with semantics [23, 10]. In fact, translating deep features along certain directions is shown to be corresponding to performing meaningful semantic transformations on the input images. For example, deep feature interpolation [9] leverages simple interpolations of deep features from pre-trained neural networks to achieve semantic image transformations. Variational Autoencoder(VAE) and Generative Adversarial Network(GAN) based methods [24, 25, 26] establish a latent representation corresponding to the abstractions of images, which can be manipulated to edit the semantics of images. Generally, these methods reveal that certain directions in the deep feature space correspond to meaningful semantic transformations, and can be leveraged to perform semantic data augmentation.  3  Method  Deep networks are known to excel at forming high-level representations in the deep feature space [4, 5, 9, 27], where the semantic relations between samples can be captured by the relative positions of their features [10]. Previous work has demonstrated that translating features towards specific directions corresponds to meaningful semantic transformations when the features are mapped to the input space [9, 28, 10]. Based on this observation, we propose to directly augment the training data in the feature space, and integrate this procedure into the training of deep models. The proposed implicit semantic data augmentation (ISDA) has two important components, i.e., online estimation of class-conditional covariance matrices and optimization with a robust loss function. The first component aims to find a distribution from which we can sample meaningful semantic transformation directions for data augmentation, while the second saves us from explicitly generating large amount of extra training data, leading to remarkable efficiency compared to existing data augmentation techniques. 3.1 Sematic Transformations in Deep Feature Space As aforementioned, certain directions in the deep feature space correspond to meaningful semantic transformations like “make-bespectacled” or ‘change-view-angle’. This motivates us to augment the training set by applying such semantic transformations on deep features. However, manually searching for semantic directions is infeasible for large scale problems. To address this problem, we propose to approximate the procedure by sampling random vectors from a normal distribution with zero mean and a covariance that is proportional to the intra-class covariance matrix, which captures the variance of samples in that class and is thus likely to contain rich semantic information. Intuitively, features for the person class may vary along the “wear-glasses” direction, while have nearly zero variance along the “has-propeller” direction which only occurs for other classes like the 3  plane class. We hope that directions corresponding to meaningful transformations for each class are well represented by the principle components of the covariance matrix of that class. Consider training a deep network G with weights Θ on a training set D = {(xi , yi )}N i=1 , where yi ∈ {1, . . . , C} is the label of the i-th sample xi over C classes. Let the A-dimensional vector ai = [ai1 , . . . , aiA ]T = G(xi , Θ) denote the deep features of xi learned by G, and aij indicate the jth element of ai . To obtain semantic directions to augment ai , we randomly sample vectors from a zero-mean multivariate normal distribution N (0, Σyi ), where Σyi is the class-conditional covariance matrix estimated from the features of all the samples in class yi . In implementation, the covariance matrix is computed in an online fashion by aggregating statistics from all mini-batches. The online estimation algorithm is given in Section A in the supplementary. During training, C covariance matrices are computed, one for each class. The augmented feature ãi is obtained by translating ai along a random direction sampled from N (0, λΣyi ). Equivalently, we have ãi ∼ N (ai , λΣyi ), (1) where λ is a positive coefficient to control the strength of semantic data augmentation. As the covariances are computed dynamically during training, the estimation in the first few epochs are not quite informative when the network is not well trained. To address this issue, we let λ = (t/T )×λ0 be a function of the current iteration t, thus to reduce the impact of the estimated covariances on our algorithm early in the training stage. 3.2 Implicit Semantic Data Augmentation (ISDA) A naive method to implement ISDA is to explicitly augment each ai for M times, forming an N k augmented feature set {(a1i , yi ), . . . , (aM i , yi )}i=1 of size M N , where ai is k-th copy of augmented features for sample xi . Then the networks are trained by minimizing the cross-entropy (CE) loss: LM (W , b, Θ) =  T k N M ewyi ai +byi 1 X 1 X −log( PC wT ak +b ), N i=1 M e j i j  k=1  (2)  j=1  where W = [w1 , . . . , wC ]T ∈ RC×A and b = [b1 , . . . , bC ]T ∈ RC are the weight matrix and biases corresponding to the final fully connected layer, respectively. Obviously, the naive implementation is computationally inefficient when M is large, as the feature set is enlarged by M times. In the following, we consider the case that M grows to infinity, and find that an easy-to-compute upper bound can be derived for the loss function, leading to a highly efficient implementation. Upper bound of the loss function. In the case M → ∞, we are in fact considering the expectation of the CE loss under all possible augmented features. Specifically, L∞ is given by: T N 1 X ewyi ãi +byi L∞ (W , b, Θ|Σ) = Eã [−log( PC wT ã +b )]. N i=1 i e j i j  (3)  j=1  If L∞ can be computed efficiently, then we can directly minimize it without explicitly sampling augmented features. However, Eq. (3) is difficult to compute in its exact form. Alternatively, we find that it is possible to derive an easy-to-compute upper bound for L∞ , as given by the following proposition. Proposition 1. Suppose that ãi ∼ N (ai , λΣyi ), then we have an upper bound of L∞ , given by N 1 X L∞ (W , b, Θ|Σ) ≤ −log( PC N i=1  j=1  T  ewyi ai +byi T  λ  T  T  ewj ai +bj + 2 (wj −wyi )Σyi (wj −wyi )  ) , L∞ .  (4)  Proof. According to the definition of L∞ in (3), we have: L∞ (W , b, Θ|Σ) =  C N X T T 1 X Eãi [log( e(wj −wyi )ãi +(bj −byi ) )] N i=1 j=1  4  (5)  ≤  N C X T T 1 X log( Eãi [e(wj −wyi )ãi +(bj −byi ) ]) N i=1 j=1  (6)  =  N C X T T T T λ 1 X log( e(wj −wyi )ai +(bj −byi )+ 2 (wj −wyi )Σyi (wj −wyi ) ) N i=1 j=1  (7)  (8) = L∞ . In the above, the Inequality (6) follows from the Jensen’s inequality E[logX] ≤ logE[X], as the logarithmic function log(·) is concave. The Eq. (7) is obtained by leveraging the moment-generating function: 1 2 2 E[etX ] = etµ+ 2 σ t , X ∼ N (µ, σ 2 ), due to the fact that (wjT −wyTi )ãi +(bj −byi ) is a Gaussian random variable, i.e.,  (wjT −wyTi )ãi +(bj −byi ) ∼ N (wjT −wyTi )ai +(bj −byi ), λ(wjT −wyTi )Σyi (wj −wyi ) . Essentially, Proposition 1 provides a surrogate loss for our implicit data augmentation algo- Algorithm 1 The ISDA Algorithm. rithm. Instead of minimizing the exact loss func- 1: Input: D, λ0 2: Randomly initialize W , b and Θ tion L∞ , we can optimize its upper bound L∞ 3: for t = 0 to T do in a much more efficient way. Therefore, the 4: Sample a mini-batch {xi , yi }B proposed ISDA boils down to a novel robust i=1 from D 5: Compute ai = G(xi , Θ) loss function, which can be easily adopted by most deep models. In addition, we can observe 6: Estimate the covariance matrices Σ1 , Σ2 , ..., ΣC that when λ → 0, which means no features are augmented, L∞ reduces to the standard CE loss. 7: Compute L∞ according to Eq. (4) 8: Update W , b, Θ with SGD In summary, the proposed ISDA can be simply 9: end for plugged into deep networks as a robust loss func- 10: Output: W , b and Θ tion, and efficiently optimized with the stochastic gradient descent (SGD) algorithm. We present the pseudo code of ISDA in Algorithm 1. Details of estimating covariance matrices and computing gradients are presented in Appendix A.  4  Experiments  In this section, we empirically validate the proposed algorithm on several widely used image classification benchmarks, i.e., CIFAR-10, CIFAR-100 [1] and ImageNet[29]. We first evaluate the effectiveness of ISDA with different deep network architectures on these datasets. Second, we apply several recent proposed non-semantic image augmentation methods in addition to the standard baseline augmentation, and investigate the performance of ISDA. Third, we present comparisons with state-of-the-art robust lost functions and generator-based semantic data augmentation algorithms. Finally, ablation study is conducted to examine the effectiveness of each component. We also visualize the augmented samples in the original input space with the aid of a generative network. 4.1 Datasets and Baselines Datasets. We use three image recognition benchmarks in the experiments. (1) The two CIFAR datasets consist of 32x32 colored natural images in 10 classes for CIFAR-10 and 100 classes for CIFAR-100, with 50,000 images for training and 10,000 images for testing, respectively. In our experiments, we hold out 5000 images from the training set as the validation set to search for the hyper-parameter λ0 . These samples are also used for training after an optimal λ0 is selected, and the results on the test set are reported. Images are normalized with channel means and standard deviations for pre-procession. For the non-semantic data augmentation of the training set, we follow the standard operation in [30]: 4 pixels are padded at each side of the image, followed by a random 32x32 cropping combined with random horizontal flipping. (2) ImageNet is a 1,000-class dataset from ILSVRC2012[29], providing 1.2 million images for training and 50,000 images for validation. We adopt the same augmentation configurations in [2, 4, 5]. Non-semantic augmentation techniques. To study the complementary effects of ISDA to traditional data augmentation methods, two state-of-the-art non-semantic augmentation techniques are applied, with and without ISDA. (1) Cutout [31] randomly masks out square regions of input during training to regularize the model. (2) AutoAugment [32] automatically searches for the best augmentation policies to yield the highest validation accuracy on a target dataset. All hyper-parameters are the same as reported in the papers introducing them. 5  Table 1: Evaluation of ISDA on CIFAR with different models. The average test error over the last 10 epochs is calculated in each experiment, and we report mean values and standard deviations in three independent experiments. The best results are bold-faced. Method ResNet-32 [4] ResNet-32 + ISDA ResNet-110 [4] ResNet-110 + ISDA SE-ResNet-110 [33] SE-ResNet-110 + ISDA Wide-ResNet-16-8 [34] Wide-ResNet-16-8 + ISDA Wide-ResNet-28-10 [34] Wide-ResNet-28-10 + ISDA ResNeXt-29, 8x24d [35] ResNeXt-29, 8x24d + ISDA DenseNet-BC-100-12 [5] DenseNet-BC-100-12 + ISDA DenseNet-BC-190-40 [5] DenseNet-BC-190-40 + ISDA  Params 0.5M 0.5M 1.7M 1.7M 1.7M 1.7M 11.0M 11.0M 36.5M 36.5M 34.4M 34.4M 0.8M 0.8M 15.2M 15.2M  CIFAR-10 7.39 ± 0.10% 7.09 ± 0.12% 6.76 ± 0.34% 6.33 ± 0.19% 6.14 ± 0.17% 5.96 ± 0.21% 4.25 ± 0.18% 4.04 ± 0.29% 3.82 ± 0.15% 3.58 ± 0.15% 3.86 ± 0.14% 3.67 ± 0.12% 4.90 ± 0.08% 4.54 ± 0.07% 3.52% 3.24%  CIFAR-100 31.20 ± 0.41% 30.27 ± 0.34% 28.67 ± 0.44% 27.57 ± 0.46% 27.30 ± 0.03% 26.63 ± 0.21% 20.24 ± 0.27% 19.91 ± 0.21% 18.53 ± 0.07% 17.98 ± 0.15% 18.16 ± 0.13% 17.43 ± 0.25% 22.61 ± 0.10% 22.10 ± 0.34% 17.74% 17.42%  Table 2: Evaluation of ISDA with state-of-the-art non-semantic augmentation techniques. ‘AA’ refers to AutoAugment [32]. We report mean values and standard deviations in three independent experiments. The best results are bold-faced. Dataset  Networks Wide-ResNet-28-10 [34] CIFAR-10 Shake-Shake (26, 2x32d) [36] Shake-Shake (26, 2x112d) [36] Wide-ResNet-28-10 [34] CIFAR-100 Shake-Shake (26, 2x32d) [36] Shake-Shake (26, 2x112d) [36]  Cutout [31] 2.99 ± 0.06% 3.16 ± 0.09% 2.36% 18.05 ± 0.25% 18.92 ± 0.21% 17.34 ± 0.28%  Cutout + ISDA 2.83 ± 0.04% 2.93 ± 0.03% 2.25% 17.02 ± 0.11% 18.17 ± 0.08 % 16.24 ± 0.20 %  AA [32] 2.65 ± 0.07% 2.89 ± 0.09% 2.01% 16.60 ± 0.40% 17.50 ± 0.19% 15.21 ± 0.20%  AA + ISDA 2.56 ± 0.01% 2.68 ± 0.12% 1.82% 15.62 ± 0.32% 17.21 ± 0.33% 13.87 ± 0.26%  Baselines. Our method is compared to several baselines including state-of-the-art robust loss functions and generator-based semantic data augmentation methods. (1) Dropout [37] is a widely used regularization approach which randomly mutes some neurons during training. (2) Large-margin softmax loss [18] introduces large decision margin, measured by a cosine distance, to the standard CE loss. (3) Disturb label [38] is a regularization mechanism that randomly replaces a fraction of labels with incorrect ones in each iteration. (4) Focal loss [17] focuses on a sparse set of hard examples to prevent easy samples from dominating the training procedure. (5) Center loss [22] simultaneously learns a center of features for each class and minimizes the distances between the deep features and their corresponding class centers. (6) Lq loss [16] is a noise-robust loss function, using the negative Box-Cox transformation. (7) For generator-based semantic augmentation methods, we train several state-of-the-art GANs [39, 40, 41, 42], which are then used to generate extra training samples for data augmentation. For fair comparison, all methods are implemented with the same training configurations when it is possible. Details for hyper-parameter settings are presented in Appendix B. Training details. For deep networks, we implement the ResNet, SE-ResNet, Wide-ResNet, ResNeXt, DenseNet and PyramidNet on the two CIFAR datasets, and ResNet on ImageNet. Detailed configurations for these models are given in Appendix B. The hyper-parameter λ0 for ISDA is selected from the set {0.1, 0.25, 0.5, 0.75, 1} according to the performance on the validation set. On ImageNet, due to GPU memory limitation, we approximate the covariance matrices by their diagonals, i.e., the variance of each dimension of the features. The best hyper-parameter λ0 is selected from {1, 2.5, 5, 7.5, 10}. 4.2 Main Results Table 1 presents the performance of several state-of-the-art deep networks with and without ISDA. It can be observed that ISDA consistently improves the generalization performance of these models, especially with fewer training samples per class. On CIFAR-100, for relatively small models like ResNet-32 and ResNet-110, ISDA reduces test errors by about 1%, while for larger models like 6  28  24  WRN-28-10 WRN-28-10 + ISDA WRN-28-10 + AA WRN-28-10 + AA +ISDA  24  23  Error Rate (%)  Test Error (%)  26  22 20 18  22 21 20 19 18  16 40  60  80  100  120  140  160  180  200  220  17 60  240  Epoch  ResNet-152 ResNet-152 ResNet-152 ResNet-152 70  (Test) + ISDA (Test) (Train) + ISDA (Train) 80  90  Epoch  100  110  120  Figure 2: Curves of test errors on CIFAR-100 Figure 3: Training and test errors on ImageNet. with Wide-ResNet (WRN). Table 3: Comparisons with the state-of-the-art methods. We report mean values and standard deviations of the test error in three independent experiments. Best results are bold-faced. Method Large Margin [18] Disturb Label [38] Focal Loss [17] Center Loss [22] Lq Loss [16] WGAN [39] CGAN [40] ACGAN [41] infoGAN [42] Basic Basic + Dropout ISDA ISDA + Dropout  ResNet-110 CIFAR-10 CIFAR-100 6.46±0.20% 28.00±0.09% 6.61±0.04% 28.46±0.32% 6.68±0.22% 28.28±0.32% 6.38±0.20% 27.85±0.10% 6.69±0.07% 28.78±0.35% 6.63±0.23% 6.56±0.14% 28.25±0.36% 6.32±0.12% 28.48±0.44% 6.59±0.12% 27.64±0.14% 6.76±0.34% 28.67±0.44% 6.23±0.11% 27.11±0.06% 6.33±0.19% 27.57±0.46% 5.98±0.20% 26.35±0.30%  Wide-ResNet-28-10 CIFAR-10 CIFAR-100 3.69±0.10% 18.48±0.05% 3.91±0.10% 18.56±0.22% 3.62±0.07% 18.22±0.08% 3.76±0.05% 18.50±0.25% 3.78±0.08% 18.43±0.37% 3.81±0.08% 3.84±0.07% 18.79±0.08% 3.81±0.11% 18.54±0.05% 3.81±0.05% 18.44±0.10% 3.82±0.15% 18.53±0.07% 3.58±0.15% 17.98±0.15%  Wide-ResNet-28-10 and ResNeXt-29, 8x24d, our method outperforms the competitive baselines by nearly 0.7%. Compared to ResNets, DenseNets generally suffer less from overfitting due to their architecture design, thus appear to benefit less from our algorithm. Table 2 shows experimental results with recent proposed powerful traditional image augmentation methods (i.e. Cutout [31] and AutoAugment [32]). Interestingly, ISDA seems to be even more effective when these techniques exist. For example, when applying AutoAugment, ISDA achieves performance gains of 1.34% and 0.98% on CIFAR-100 with the Shake-Shake (26, 2x112d) and the Wide-ResNet-28-10, respectively. Notice that these improvements are more significant than the standard situations. A plausible explanation for this phenomenon is that non-semantic augmentation methods help to learn a better feature representation, which makes semantic transformations in the deep feature space more reliable. The curves of test errors during training on CIFAR-100 with WideResNet-28-10 are presented in Figure 2. It is clear that ISDA achieves a significant improvement after the third learning rate drop, and shows even better performance after the forth drop. Table 4: Evaluation of ISDA on ImageNet.  Table 4 presents the performance of ISDA on the large scale ImageNet dataset. It can be observed that ISDA reduces Top-1 error rate by 0.45% for the ResNet-152 model. The training and test error curves are shown in Figure 3. Notably, ISDA achieves a slightly higher training error but a lower test error, indicating that ISDA performs effective regularization on deep networks.  Method ResNet-50 [4] ResNet-50 + ISDA ResNet-152 [4] ResNet-152 + ISDA  Top-1 23.58% 23.30% 21.65% 21.20%  Top-5 6.92% 6.82% 6.01% 5.67%  4.3 Comparison with Other Approaches We compare ISDA with a number of competitive baselines described in Section 4.1, ranging from robust loss functions to semantic data augmentation algorithms based on generative models. The results are summarized in Table 3, and the training curves are presented in Appendix D. One can 7  Initial Restored  Initial Restored  Augmented  Augmented  Figure 4: Visualization results of semantically augmented images. observe that ISDA compares favorably with all the competitive baseline algorithms. With ResNet-110, the test errors of other robust loss functions are 6.38% and 27.85% on CIFAR-10 and CIFAR-100, respectively, while ISDA achieves 6.23% and 27.11%, respectively. Among all GAN-based sematic augmentation methods, ACGAN gives the best performance, especially on CIFAR-10. However, these models generally suffer a performance reduction on CIFAR-100, which do not contain enough samples to learn a valid generator for each class. In contrast, ISDA shows consistent improvements on all the datasets. In addition, GAN-based methods require additional computation to train the generators, and introduce significant overhead to the training process. In comparison, ISDA not only leads to lower generalization error, but is simpler and more efficient. 4.4 Visualization Results To demonstrate that our method is able to generate meaningful semantically augmented samples, we introduce an approach to map the augmented features back to the pixel space to explicitly show semantic changes of the images. Due to space limit, we defer the detailed introduction of the mapping algorithm and present it in Appendix C. Figure 4 shows the visualization results. The first and second column represent the original images and reconstructed images without any augmentation. The rest columns present the augmented images by the proposed ISDA. It can be observed that ISDA is able to alter the semantics of images, e.g., backgrounds, visual angles, colors and type of cars, color of skins, which is not possible for traditional data augmentation techniques. 4.5 Ablation Study Table 5: The ablation study for ISDA. To get a better understanding of the effectiveness of different components in Setting CIFAR-10 CIFAR-100 ISDA, we conduct a series of ablation Basic 3.82±0.15% 18.58±0.10% study. In specific, several variants are Identity matrix 3.63±0.12% 18.53±0.02% considered: (1) Identity matrix means 3.70±0.15% 18.23±0.02% Diagonal matrix replacing the covariance matrix Σc by Single covariance matrix 3.67±0.07% 18.29±0.13% the identity matrix. (2) Diagonal matrix Constant λ0 3.69±0.08% 18.33±0.16% means using only the diagonal elements ISDA 3.58±0.15% 17.98±0.15% of the covariance matrix Σc . (3) Single covariance matrix means using a global covariance matrix computed from the features of all classes. (4) Constant λ0 means using a constant λ0 without setting it as a function of the training iterations. Table 5 presents the ablation results. Adopting identity matrix increases the test error by 0.05% on CIFAR-10 and nearly 0.56% on CIFAR-100. Using a single covariance matrix greatly degrades the generalization performance as well. The reason is likely to be that both of them fail to find proper directions in the deep feature space to perform meaningful semantic transformations. Adopting a diagonal matrix also hurts the performance as it does not consider correlations of features.  5  Conclusion  In this paper, we proposed an efficient implicit semantic data augmentation algorithm (ISDA) to complement existing data augmentation techniques. Different from existing approaches leveraging generative models to augment the training set with semantically transformed samples, our approach is considerably more efficient and easier to implement. In fact, we showed that ISDA can be formulated as a novel robust loss function, which is compatible with any deep network with the cross-entropy loss. Extensive results on several competitive image classification datasets demonstrate the effectiveness and efficiency of the proposed algorithm. 8  Acknowledgments Gao Huang is supported in part by Beijing Academy of Artificial Intelligence (BAAI) under grant BAAI2019QN0106 and Tencent AI Lab Rhino-Bird Focused Research Program under grant JR201914.  References [1] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from tiny images,” Citeseer, Tech. Rep., 2009. [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in NeurIPS, 2012, pp. 1097–1105. [3] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in ICLR, 2015. [4] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, 2016, pp. 770–778. [5] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely Connected Convolutional Networks,” in CVPR, 2017, pp. 2261–2269. [6] A. J. Ratner, H. Ehrenberg, Z. Hussain, J. Dunnmon, and C. Ré, “Learning to compose domainspecific transformations for data augmentation,” in NeurIPS, 2017, pp. 3236–3246. [7] C. Bowles, L. J. Chen, R. Guerrero, P. Bentley, R. N. Gunn, A. Hammers, D. A. Dickie, M. del C. Valdés Hernández, J. M. Wardlaw, and D. Rueckert, “Gan augmentation: Augmenting training data using generative adversarial networks,” CoRR, vol. abs/1810.10863, 2018. [8] A. Antoniou, A. J. Storkey, and H. A. Edwards, “Data augmentation generative adversarial networks,” CoRR, vol. abs/1711.04340, 2018. [9] P. Upchurch, J. R. Gardner, G. Pleiss, R. Pless, N. Snavely, K. Bala, and K. Q. Weinberger, “Deep feature interpolation for image content changes,” in CVPR, 2017, pp. 6090–6099. [10] Y. Bengio, G. Mesnil, Y. Dauphin, and S. Rifai, “Better mixing via deep representations,” in ICML, 2013, pp. 552–560. [11] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Training very deep networks,” in NeurIPS, 2015, pp. 2377–2385. [12] E. D. Cubuk, B. Zoph, D. Mané, V. Vasudevan, and Q. V. Le, “Autoaugment: Learning augmentation policies from data,” CoRR, vol. abs/1805.09501, 2018. [13] L. Maaten, M. Chen, S. Tyree, and K. Weinberger, “Learning with marginalized corrupted features,” in ICML, 2013, pp. 410–418. [14] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman, “Reading text in the wild with convolutional neural networks,” International Journal of Computer Vision, vol. 116, no. 1, pp. 1–20, 2016. [15] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan, “Unsupervised pixel-level domain adaptation with generative adversarial networks,” in CVPR, 2017, pp. 3722–37"
"Liquid Warping GAN: A Unified Framework for Human Motion Imitation, Appearance Transfer and Novel View Synthesis Wen Liu1∗ Zhixin Piao1∗ Jie Min1 Wenhan Luo2 Lin Ma2 1 ShanghaiTech University 2 Tencent AI Lab  Shenghua Gao1  arXiv:1909.12224v1 [cs.CV] 26 Sep 2019  {liuwen,piaozhx,minjie,gaoshh}@shanghaitech.edu.cn {whluo.china,forest.linma}@gmail.com  1. Introduction Human image synthesis, including human motion imitation [1, 19, 31], appearance transfer [26, 37] and novel ∗ Contributed equally and work done while Wen Liu was a Research Intern with Tencent AI Lab.  Motion Imitation  + Source Image  Reference Pose  Synthesized Image  Novel View  Synthesized Image  Reference Appearance  Synthesized Image  Novel View Synthesis  + Source Image  Appearance Transfer  + Source Image  Figure 1. Illustration of human motion imitation, appearance transfer and novel view synthesis. The first column is the source image and the second column is reference condition, such as image or novel view of camera. The third column is the synthesized results.  view synthesis [40, 42], has huge potential applications in re-enactment, character animation, virtual clothes try-on, movie or game making and so on. The definition is that given a source human image and a reference human image, i) the goal of motion imitation is to generate an image with texture from source human and pose from reference human, as depicted in the top of Fig. 1; ii) human novel view synthesis aims to synthesize new images of the human body, captured from different viewpoints, as illustrated in the middle of Fig. 1; iii) the goal of appearance transfer is to generate a human image preserving reference identity with clothes, as shown in the bottom of Fig. 1 where different parts might come from different people. In the realm of human image synthesis, previous works separately handle these tasks [19, 26, 42] with task-specific  pipeline, which seems to be difficult to extend to other tasks. Recently, generative adversarial network (GAN) [6] achieves great successes on these tasks. Taking human motion imitation as an example, we summarize recent approaches in Fig. 2. In an early work [19], as shown in Fig. 2 (a), source image (with its pose condition) and target pose condition are concatenated which thereafter is fed into a network with adversarial training to generate an image with desired pose. However, direct concatenation does not take the spatial layout into consideration, and it is ambiguous for the generator to place the pixel from source image into a right position. Thus, it always results in a blurred image and loses the source identity. Later, inspired by the spatial transformer networks (STN) [10], a texture warping method [1], as shown in Fig. 2 (b), is proposed. It firstly fits a rough affine transformation matrix from source and reference poses, uses an STN to warp the source image into reference pose and generates the final result based on the warped image. Texture warping, however, could not preserve the source information as well, in terms of the color, style or face identity, because the generator might drop out source information after several down-sampling operations, such as stride convolution and pooling. Meanwhile, contemporary works [4, 31] propose to warp the deep features of the source images into target pose rather than that in image space, as shown in Fig 2 (c), named as feature warping. However, features extracted by an encoder in feature warping cannot guarantee to accurately characterize the source identity and thus consequently produce a blur or lowfidelity image in an inevitable way. The aforementioned existing methods encounter with challenges in generating unrealistic-looking images, due to three reasons: 1) diverse clothes in terms of texture, style, color, and high-structure face identity are difficult to be captured and preserved in their network architecture; 2) articulated and deformable human bodies result in a large spatial layout and geometric changes for arbitrary pose manipulations; 3) all these methods cannot handle multiple source inputs, such as in appearance transfer, different parts might come from different source people. In this paper, to preserve the source information, including details of clothes and face identity, we propose a Liquid Warping Block (LWB) to address the loss of source information from three aspects: 1) a denoising convolutional auto-encoder is used to extract useful features that preserve source information, including texture, color, style and face identity; 2) source features of each local part are blended into a global feature stream by our proposed LWB to further preserve the source details; 3) it supports multiple-source warping, such as in appearance transfer, warping the features of head from one source and those of body from another, and aggregating into a global feature stream. This will further enhance the local identity of each source part.  source output condition flow warped image G 𝒲𝒲  𝑑𝑑  generator texture warping feature warping  𝑃𝑃𝑥𝑥  G  𝐼𝐼𝑥𝑥  𝑃𝑃𝑦𝑦 𝑃𝑃𝑥𝑥 𝐼𝐼𝑥𝑥  𝒲𝒲  𝑃𝑃𝑦𝑦 𝑇𝑇𝑥𝑥  (b) texture warp  G  G  𝑤𝑤  𝑑𝑑  𝑃𝑃𝑥𝑥  𝑆𝑆𝑦𝑦  (a) concatenation 𝑤𝑤  𝐼𝐼𝑥𝑥  𝑆𝑆𝑦𝑦  concat  𝑃𝑃𝑦𝑦  𝑆𝑆𝑦𝑦  (c) feature warp  Figure 2. Three existing approaches of propagating source information into target condition. (a) is early concatenation, and it concatenates the source image and source condition, as well as target condition, into the color channel. (b) and (c) are texture and feature warping, respectively, and the source image or its features are propagated into target condition under a fitted transformation flow.  In addition, existing approaches mainly rely on 2D pose [1, 19, 31], dense pose [22] and body parsing [4]. These methods only take care of the layout locations and ignore the personalized shape and limbs (joints) rotations, which are even more essential than layout location in human image synthesis. For example, in an extreme case, a tall man imitates the actions of a short person and using the 2D skeleton, dense pose and body parsing condition will unavoidably change the height and size of the tall one, as shown in the bottom of Fig. 6. To overcome their shortcomings, we use a parametric statistical human body model, SMPL [2, 18, 12] which disentangles human body into pose (joint rotations) and shape. It outputs 3D mesh (without clothes) rather than the layouts of joints and parts. Further, transformation flows can be easily calculated by matching the correspondences between two 3D triangulated meshes, which is more accurate and results in fewer misalignments than previous fitted affine matrix from keypoints [1, 31]. Based on SMPL model and Liquid Warping Block (LWB), our method can be further extended into other tasks, including human appearance transfer and novel view synthesis for free and one model can handle these three tasks. We summarize our contributions as follows: 1) we propose a LWB to propagate and address the loss of the source information, such as texture, style, color, and face identity, in both image and feature space; 2) by taking advantages of both LWB and the 3D parametric model, our method is a unified framework for human motion imitation, appearance transfer, and novel view synthesis; 3) we build a dataset for these tasks, especially for human motion imitation in video, and all codes and datasets are released for further research convenience in the community.  2. Related Work Human Motion Imitation. Recently, most methods are based on conditioned generative adversarial networks (CGAN) [1, 3, 19, 20, 22, 30] or Variational AutoEncoder [5]. Their key technical idea is to combine target  HMR  source  𝐶𝐶𝑠𝑠  𝐼𝐼𝑓𝑓𝑓𝑓  T  HMR  reference  𝐾𝐾𝑠𝑠 , 𝛽𝛽𝑠𝑠 , 𝜃𝜃𝑠𝑠 , 𝑀𝑀𝑠𝑠  𝐼𝐼𝑏𝑏𝑏𝑏  ℱ  𝐾𝐾𝑟𝑟 , 𝛽𝛽𝑟𝑟 , 𝜃𝜃𝑟𝑟 , 𝑀𝑀𝑟𝑟  (a) Body Mesh Recovery  𝐶𝐶𝑡𝑡  𝐼𝐼𝑠𝑠𝑠𝑠𝑠𝑠  𝐼𝐼̂𝑏𝑏𝑏𝑏  𝐺𝐺𝐵𝐵𝐵𝐵 𝐺𝐺𝑆𝑆𝑆𝑆𝑆𝑆 𝐿𝐿𝐿𝐿𝐿𝐿  𝐺𝐺𝑇𝑇𝑇𝑇𝑇𝑇  (b) Flow Composition  +  X  𝐴𝐴𝑠𝑠  𝑃𝑃𝑠𝑠  +  X  𝐴𝐴𝑡𝑡  recons 𝐼𝐼̂𝑠𝑠  𝑃𝑃𝑡𝑡  (c) Liquid Warping GAN  syn 𝐼𝐼̂𝑡𝑡  Figure 3. The training pipeline of our method. We randomly sample a pair of images from a video, denoting one of them as source image, named Is and the other as reference image named Ir . (a) A body mesh recovery module will estimate the 3D mesh of each image, and render their correspondence map, Cs and Ct ; (b) The flow composition module will first calculate the transformation flow T based on two correspondence maps and their projected vertices in image space. Then it will separate source image Is into foreground image If t and masked background Ibg . Finally it warps the source image based on transformation flow T , and produces a warped image Isyn ; (c) In the last GAN module, the generator consists of three streams, which separately generates the background image Iˆbg by GBG , reconstructs the source image Iˆs by GSID and synthesizes the target image Iˆt under reference condition by GT SF . To preserve the details of source image, we propose a novel Liquid Warping Block (LWB, shown in Fig. 4) which propagates the source features of GSID into GT SF at several layers and preserve the source information, in terms of texture, style and color.  image along with source pose (2D key-points) as inputs and generate realistic images by GANs using source pose. The difference of those approaches are merely in network architectures and adversarial losses. In [19], a U-Net generator is designed and a coarse-to-fine strategy is utilized to generate 256 × 256 images. Si et al. [1, 30] propose a multistage adversarial loss and separately generate the foreground (or different body parts) and background. Neverova et al. [22] replace the sparse 2D key-points with the dense correspondences between image and surface of the human body by DensePose [27]. Chan et al. [3] use pix2pixHD [35] framework together with a specialized Face GAN to learn a mapping from 2D skeleton to image and generate a more realistic target image. Furthermore, Wang et al. [34] extend it to video generation and Liu et al. [16] propose a neural renderer of human actor video. However, their works just train a mapping from 2D pose (or parts) to image of each person — in other words, every body need to train their own model. This shortcoming might limit its wide application. Human Appearance Transfer. Human appearance modeling or transfer is a vast topic, especially in the field of virtual try-on applications, from computer graphics pipelines [24] to learning based pipelines [26, 37]. Graphics based methods first estimate the detailed 3D human mesh with clothes via garments and 3d scanners [38] or multiple camera arrays [15] and then human appearance with clothes is capable to be conducted from one person to another based on the detailed 3D mesh. Although these methods can produce high-fidelity result, their cost, size and controlled environment are unfriendly and inconvenient to customers. Recently, in the light of deep generative models,  SwapNet [26] firstly learns a pose-guided clothing segmentation synthetic network, and then the clothing parsing results with texture features from source image feed into an encoder-decoder network to generate the image with desired garment. In [37], the authors leverage a geometric 3D shape model combined with learning methods, swap the color of visible vertices of the triangulated mesh and train a model to infer that of invisible vertices. Human Novel View Synthesis. Novel view synthesis aims to synthesize new images of the same object, as well as the human body, from arbitrary viewpoints. The core step of existing methods is to fit a correspondence map from the observable views to novel views by convolutional neural networks. In [41], the authors use CNNs to predict appearance flow and synthesize new images of the same object by copying the pixel from source image based on the appearance flow, and they have achieved decent results of rigid objects like vehicles. Following work [23] proposes to infer the invisible textures based on appearance flow and adversarial generative network (GAN) [6], while Zhu et al. [42] argue that appearance flow based method performs poorly on articulated and deformable objects, such as human bodies. They propose an appearance-shape-flow strategy for synthesizing novel views of human bodies. Besides, Zhao et al. [40] design a GAN based method to synthesize highresolution views in a coarse-to-fine way.  3. Method Our Liquid Warping GAN contains three stages, body mesh recovery, flow composition and a GAN module with  Liquid Warping Block (LWB). The training pipeline is the same for different tasks. Once the model has been trained on one task, it can deal with other tasks as well. Here, we use motion imitation as an example, as shown in Fig. 3. Denoting the source image as Is and the reference image Ir . The first body mesh recovery module will estimate the 3D mesh of Is and Ir , and render their correspondence maps, Cs and Ct . Next, the flow composition module will first calculate the transformation flow T based on two correspondence maps and their projected mesh in image space. The source image Is is thereby decomposed as front image If t and masked background Ibg , and warped to Isyn based on transformation flow T . The last GAN module has a generator with three streams. It separately generates background image by GBG , reconstructs the source image Iˆs by GSID and synthesizes the image Iˆt under reference condition by GT SF . To preserve the details of source image, we propose a novel Liquid Warping Block (LWB) and it propagates the source features of GSID into GT SF at several layers.  3.1. Body Mesh Recovery Module As shown in Fig. 3 (a), given source image Is and reference image Ir , the role of this stage is to predict the kinematic pose (rotation of limbs) and shape parameters, as well as 3D mesh of each image. In this paper, we use the HMR [12] as 3D pose and shape estimator due to its good trade-off between accuracy and efficiency. In HMR, an image is firstly encoded into a feature with R2048 by a ResNet-50 [8] and then followed by an iterative 3D regression network that predicts the pose θ ∈ R72 and shape β ∈ R10 of SMPL [18], as well as the weak-perspective camera K ∈ R3 . SMPL is a 3D body model that can be defined as a differentiable function M (θ, β) ∈ RNv ×3 , and it parameterizes a triangulated mesh by Nv = 6, 890 vertices and Nf = 13, 776 faces with pose parameters θ ∈ R72 and β ∈ R10 . Here, shape parameters β are coefficients of a low-dimensional shape space learned from thousands of registered scans and the pose parameters θ are the joint rotations that articulate the bones via forward kinematics. With such process, we will obtain the body reconstruction parameters of source image, {Ks , θs , βs , Ms } and those of reference image, {Kr , θr , βr , Mr }, respectively.  3.2. Flow Composition Module Based on the previous estimations, we first render a correspondence map of source mesh Ms and that of reference mesh Mr under the camera view of Ks . Here, we denote the source and reference correspondence maps as Cs and Ct , respectively. In this paper, we use a fully differentiable renderer, Neural Mesh Renderer (NMR) [13]. We thereby project vertices of source Vs into 2D image space by weakperspective camera, vs = P roj(Vs , Ks ). Then, we calculate the barycentric coordinates of each mesh face, and  𝑋𝑋𝑠𝑠𝑙𝑙1  𝐵𝐵𝑆𝑆(𝑋𝑋𝑠𝑠𝑙𝑙1 , 𝑇𝑇1 ) +  𝑋𝑋𝑡𝑡𝑙𝑙 𝑋𝑋𝑠𝑠𝑙𝑙2  …  𝑇𝑇1  𝑇𝑇2  𝐵𝐵𝐵𝐵(𝑋𝑋𝑠𝑠𝑙𝑙2 , 𝑇𝑇2 )  LWB 𝑋𝑋�  𝑙𝑙 𝑡𝑡  (a) Liquid Warping Block (LWB)  LWB …  LWB  …  (b) Liquid Warping GAN  Figure 4. Illustration of Liquid Warping Block. (a) is the structure of LWB. Xsl 1 and Xsl 2 are the feature maps extracted by GSID of different sources in lth layers. Xtl is the feature map of GT SF at btl aggregate the feature from the lth layer. Final output features X GT SF and warped source features by bilinear sampler (BS) with respect to the flow T1 and T2 . (b) is the architecture of LWB.  obtain fs ∈ RNf ×2 . Next, we calculate the transformation flow T ∈ RH×W ×2 by matching the correspondences between source correspondence map with its mesh face coordinates fs and reference correspondence map. Here H × W is the size of image. Consequently, a front image If t and a masked background image Ibg are derived from masking the source image Is based on Cs . Finally, we warp the source image Is by the transformation flow T , and obtain the warped image Isyn , as depicted in Fig. 3.  3.3. Liquid Warping GAN This stage synthesizes high-fidelity human image under the desired condition. More specifically, it 1) synthesizes the background image; 2) predicts the color of invisible parts based on the visible parts; 3) generates pixels of clothes, hairs and others out of the reconstruction of SMPL. Generator. Our generator works in a three-stream manner. One stream, named GBG , works on the concatenation of the masked background image Ibg and the mask obtained by the binarization of Cs in color channel (4 channels in total) to generate the realistic background image Iˆbg , as shown in the top stream of Fig. 3 (c). The other two streams are source identity stream, namely GSID and transfer stream, namely GT SF . GSID is a denoising convolutional auto-encoder which aims to guide the encoder to extract the features that are capable to preserve the source information. Together with the Iˆbg , it takes the masked source foreground If t and the correspondence map Cs (6 channels in total) as inputs, and reconstructs source front image Iˆs . GT SF stream synthesizes the final result , which receives the warped foreground by bilinear sampler and the correspondence map Ct (6 channels in total) as inputs. To preserve the source information, such as texture, style and color, we propose a novel Liquid Warping Block (LWB) that links the source with target streams. It blends the source features from GSID and fuses them into transfer stream GT SF , as shown in the bottom of Fig. 3 (c). One advantage of our proposed Liquid Warping Block  (LWB) is that it addresses multiple sources, such as in human appearance transfer, preserving the head of source one, and wearing the upper outer garment from the source two, while wearing the lower outer garment from the source three. The different parts of features are aggregated into GT SF by their own transformation flow, independently. Here, we take two sources as an example, as shown in Fig. 4. Denoting Xsl 1 and Xsl 2 as the feature maps extracted by GSID of different sources in the lth layer. Xtl is the feature map of GT SF at the lth layer. Each part of source feature is warped by their own transformation flow, and aggregated into the features of GT SF . We use bilinear sampler (BS) to warp the source features Xsl 1 and Xsl 2 , with respect to the transformation flows, T1 and T2 , respectively. The final output feature is obtained as follows: btl = BS(Xsl , T1 ) + BS(Xsl , T2 ) + Xtl . X 1 2 Please note that we only take two sources an example, which can be easily extended to multiple sources. GBG , GSID and GT SF have the similar architecture, named ResUnet, a combination of ResNet [7] and UNet [28] without sharing parameters. For GBG , we directly regress the final background image, while for GSID and GT SF , we concretely generate an attention map A and a color map P , as illustrated in Fig. 3 (c). The final image can be obtained as follows: Iˆs = Ps ∗ As + Iˆbg ∗ (1 − As ) Iˆt = Pt ∗ At + Iˆbg ∗ (1 − At ). Discriminator. For discriminator, we follow the architecture of Pix2Pix [9]. More details about our network architectures are provided in supplementary materials.  3.4. Training Details and Loss Functions In this part, we will introduce the loss functions, and how to train the whole system. For body recovery module, we follow the network architecture and loss functions of HMR [12]. Here, we use a pre-trained model of HMR. For the Liquid Warping GAN, in the training phase, we randomly sample a pair of images from each video and set one of them as source Is , and another as reference Ir . Note that our proposed method is a unified framework for motion imitation, appearance transfer and novel view synthesis. Therefore once the model has been trained, it is capable to be applied to other tasks and does not need to train from scratch. In our experiments, we train a model for motion imitation and then apply it to other tasks, including appearance transfer and novel view synthesis. The whole loss function contains four terms and they are perceptual loss [11], face identity loss, attention regularization loss and adversarial loss. Perceptual Loss. It regularizes the reconstructed source image Iˆs and generated target image Iˆt to be closer to the  ground truth Is and Ir in VGG [32] subspace. Its formulation is given as follows: Lp = kf (Iˆs ) − f (Is )k1 + kf (Iˆt ) − f (Ir )k1 . Here, f is a pre-trained VGG-19 [32]. Face Identity Loss. It regularizes the cropped face from the synthesized target image Iˆt to be similar to that from the image of ground truth Ir , which pushes the generator to preserve the face identity. It is shown as follows: Lf = kg(Iˆt ) − g(Ir )k1 . Here, g is a pre-trained SphereFaceNet [17]. Adversarial Loss. It pushes the distribution of synthesized images to the distribution of real images. As shown in following, we use LSGAN−110 [21] loss in a way like PatchGAN for the generated target image Iˆt . The discriminator D regularizes Iˆt to be more realistic-looking. We use conditioned discriminator, and it takes generated images and the correspondence map Ct (6 channels) as inputs. X LG D(Iˆt , Ct )2 . adv = Attention Regularization Loss. It regularizes the attention map A to be smooth and to prevent them from saturating. Considering that there is no ground truth of attention map A, as well as color map P , they are learned from the resulting gradients of above losses. However, the attention masks can easily saturate to 1 which prevents the generator from working. To alleviate this situation, we regularize the mask to be closer to silhouettes S rendered by 3D body mesh. Since the silhouettes is a rough map and it contains the body mask without clothes and hair, we also perform a Total Variation Regularization over A like [25], to compensate the shortcomings of silhouettes, and further to enforce smooth spatial color when combining the pixel from the predicted background Iˆbg and the color map P . It is shown as follows: 2 2  La = kAs − Ss k2 + kAt − St k2 + T V (As ) + T V (At ) X T V (A) = [A(i, j) − A(i − 1, j)]2 + [A(i, j) − A(i, j − 1)]2 . i,j  For generator, the full objective function is shown in the following, and λp , λf and λa are the weights of perceptual, face identity and attention losses. LG = λp Lp + λf Lf + λa La + LG adv . For discriminator, the full objective function is X X LD = [D(Iˆt , Ct ) + 1]2 + [D(Ir , Ct ) − 1]2 .  3.5. Inference Once trained model on the task of motion imitation, it can be applied to other tasks in inference. The difference lies in the transformation flow computation, due to the different conditions of various tasks. The remaining modules, Body Mesh Recovery and Liquid Warping GAN modules are all the same. Followings are the details of each task of Flow Composition module in testing phase.  HMR  Camera  Pose  Pose  Shape  SMPL  Camera  Shape  source  Pose Camera  HMR  Pose Shape  reference  Shape  SMPL  Camera  Render  𝐶𝐶𝑠𝑠  Pose  Render  Camera  SMPL  Shape  𝑇𝑇  𝐶𝐶𝑡𝑡  Render  Novel View  SMPL  Render Camera  𝑇𝑇  Render  (0∘ , 40∘ , 0∘ )  (a)Motion Imitation  Pose Shape  𝐶𝐶𝑠𝑠  Pose Shape  𝐶𝐶𝑡𝑡  (b) Novel View Synthesis  SMPL  Render Camera  𝐶𝐶𝑠𝑠ℎ 𝐶𝐶𝑠𝑠𝑏𝑏 𝐶𝐶𝑡𝑡𝑏𝑏  𝑇𝑇1  𝑇𝑇2  (c) Appearance Transfer  Figure 5. Illustration of calculating the transformation flows of different tasks during the testing phase. The left is the disentangled body parameters by Body Recovery module of both source and reference images. The right is the different implementations to calculate the transformation flow in different tasks.  source  reference  pG2  DSC  SHUP  OUR S Better face and cloth details  Better face and cloth details  Preserve shape and cloth details  Figure 6. Comparison of our method with others of motion imitation on the iPER dataset (zoom-in for the best of view). 2D pose-guided methods pG2 [19], DSC [31] and SHUP [1] cannot preserve the clothes details, face identity and shape consistency of source images. We highlight the details by red and blue rectangles.  Motion Imitation. We firstly copy the value of pose parameters of reference θr into that of source, and get synthetic parameters of SMPL, as well as the 3D mesh, Mt = M (θr , βs ). Next, we render a correspondence map of source mesh Ms and that of synthetic mesh Mt under the camera view of Ks . Here, we denote the source and synthetic correspondence map as Cs and Ct , respectively. Then, we project vertices of source into 2D image space by weak-perspective camera, vs = P roj(Vs , Ks ). Next, we calculate the barycentric coordinates of each mesh face, and have fs ∈ RNf ×2 . Finally, we calculate the transformation flow T ∈ RH×W ×2 by matching the correspondences between source correspondence map with its mesh face coordinates fs and synthetic correspondence map. This procedure is shown in Fig. 5 (a). Novel View Synthesis. Given a new camera view, in terms of rotation R and translation t. We firstly calculate the 3D mesh under the novel view, Mt = Ms R + t. The flowing operations are similar to motion imitation. We render a correspondence map of source mesh Ms and that of  novel mesh Mt under the weak-perspective camera Ks and calculate the transformation flow T ∈ RH×W ×2 in the end. This is illustrated in Fig. 5 (b). Appearance Transfer. It needs to “copy” the clothes of torso or body from the reference image while keeping the head (face, eye, hair and so on) identity of source. We split the transformation flow T into two sub-transformation flow, source flow T1 and referent flow T2 . Denoting head mesh as M h = (V h , F h ) and body mesh as M b = (V b , F b ). Here, M = M h ∪ M b . For T1 , We firstly project the head mesh Msh of source into image space, and thereby obtain the silhouettes, Ssh . Then, we create a mesh grid, G ∈ RH×W ×2 . Then, we mask G by S h , and derive T1 = G ∗ S h . Here, ∗ represents element-wise multiplication. For T2 , it is similar to motion imitation. We render the correspondence map of source body Msb and that of reference Mtb , denoting as Csb and Ctb , respectively. Finally, we calculate the transformation flow T2 based on the correspondences between Csb and Ctb . We illustrate it in Fig. 5 (c).  reference video input  source image  output  synthesized video  Figure 7. Examples of motion imitation from our proposed methods on the iPER dataset (zoom-in for the best of view). Our method could produce high-fidelity images that preserve the face identity, shape consistency and clothes details of source, even there are occlusions in source images such as the middle and bottom rows. We recommend accessing the supplementary material for more results in videos. reference image input  source image  output  swapped image  Figure 8. Examples of our method of human appearance transfer in the testing set of iPER (zoom-in for the best of view). Our method could produce high-fidelity and decent images that preserve the face identity and shape consistency of the source image, and keep the clothes details of reference image. We recommend accessing the supplementary material for more results.  4. Experiments  4.1. Evaluation of Human Motion Imitation.  Dataset. To evaluate the performances of our proposed method of motion imitation, appearance transfer and novel view synthesis, we build a new dataset with diverse styles of clothes, named as Impersonator (iPER) dataset. There are 30 subjects of different conditions of shape, height and gender. Each subject wears different clothes and performs an A-pose video and a video with random actions. Some subjects might wear multiple clothes, and there are 103 clothes in total. The whole dataset contains 206 video sequences with 241,564 frames. We split it into training/testing set at the ratio of 8:2 according to the different clothes. Implementation Details. To train the network, all images are normalized to [-1, 1] and resized to 256 × 256. We randomly sample a pair of images from each video. The mini-batch size is 4 in our experiments. λp , λf and λa are set to 10.0, 5.0 and 1.0, respectively. Adam [14] is used for parameter optimization of both generator and discriminator.  Evaluation Metrics. We propose an evaluation protocol of testing set of iPER dataset and it is able to indicate the performance of different methods in terms of different aspects. The details are listed in followings: 1) In each video, we select three images as source images (frontal, sideway and occlusive) with different degrees of occlusion. The frontal image contains the most information, while the sideway will drop out some information, and occlusive image will introduce ambiguity. 2) For each source image, we perform self-imitation that actors imitate actions from themselves. SSIM [36] and Learned Perceptual Similarity (LPIPS) [39] are the evaluation metrics in self-imitation setting. 3) Besides, we also conduct cross-imitation that actors imitate actions from others. We use Inception Score (IS) [29] and Fréchet Distance on a pre-trained person-reid model [33], named as FReID, to evaluate the quality of generated images.  source  30°  60°  90°  120°  150°  180°  210°  240°  270°  300°  330°  Figure 9. Examples of novel view synthesis from our method on the iPER dataset (zoom-in for the best of view). Our method could generate realistic-looking results under different camera views, and it is capable to preserve the source information, even in the self-occlusion case, such as the middle and bottom rows. Table 1. Results of motion imitation by different methods on iPER dataset. ↑ means the larger is better, and ↓ represents the smaller is better. A higher SSIM may not mean a better quality of image [39]. Self-Imitation Cross-Imitation SSIM↑ LPIPS↑ IS↑ FReID↓ PG2 [19] 0.854 0.865 3.242 0.353 0.832 0.901 3.371 0.324 SHUP [1] 0.829 0.871 3.321 0.342 DSC [31] WC 0.821 0.872 3.213 0.341 WT 0.822 0.887 3.353 0.347 WF 0.830 0.897 3.358 0.325 Ours-WLW B 0.840 0.913 3.419 0.317  Comparison with Other Methods. We compare the performance of our method with that of existing methods, including PG2 [19], SHUP [1] and DSC [31]. We train all these methods on iPER dataset, and the evaluation protocol mentioned above is applied to these methods. The results are reported in Table 1. It can be seen that our method outperforms other methods. In addition, we also analyze the generated images and make comparisons between ours and above methods. From Fig. 6, we find that 1) the 2D pose-guided methods, including PG2 [19], SHUP [1] and DSC [31], change the body shape of source. For example, in the 3rd row of Fig. 6, a tall person imitates motion from a short person and these methods change the height of source body. However, our method is capable to keep the body shape unchanged. 2) When source image exhibits selfocclusion, such as invisible face in the 1st row of Fig. 6, our method could generate more realistic-looking content of the ambiguous and invisible parts. 3) Our method is more powerful in terms of preserving source identity, such as the face identity and cloth details of source than other methods, as shown in the 2nd and 3rd row of Fig. 6. 4) Our method also produces high-fidelity images in the cross-imitation setting (imitating actions from others) and we illustrate it in Fig. 7. Ablation Study. To verify the impact of our proposed Liquid Warping Block (LWB), we design three baselines with aforementioned ways to propagate the source information, including early concatenation, texture warping and feature warping. All modules and loss functions are the  same except the propagating strategies among our method and other baselines. Here, we denote early concatenation, texture warping, feature warping, and our proposed LWB as WC , WT , WF and WLW B . We train all these under the same setting on the iPER dataset, then evaluate their performances on motion imitation. From Table 1, we can see that our proposed LWB is better than other baselines. More details are provided in supplementary materials.  4.2. Results of Human Appearance Transfer. It is worth emphasizing that once model has been trained, it is able to directly to be applied in three tasks, including motion imitation, appearance transfer and novel view synthesis. We randomly pick some examples displayed in Fig. 8. The face identity and clothes details, in terms of texture, color and style, are preserved well by our method. It demon"
"Keyphrase Generation for Scientific Articles using GANs Avinash Swaminathan 1 , Raj Kuwar Gupta1 , Haimin Zhang2 , Debanjan Mahata 2 , Rakesh Gosangi2 , Rajiv Ratn Shah1 1  arXiv:1909.12229v1 [cs.CL] 24 Sep 2019  MIDAS, IIIT-Delhi, India 2 Bloomberg s.avinash.it.17@nsit.net.in, rajkuwargupta1996@gmail.com, hzhang449@bloomberg.net, dmahata@bloomberg.net, rgosangi@bloomberg.net, rajivratn@iiitd.ac.in  Introduction Keyphrases are employed to capture the most salient topics of a long document and are indexed in databases for convenient retrieval. Researchers annotate their scientific publications with high quality keyphrases to ensure discoverability in large scientific repositories. Keyphrases could either be extractive (part of the document) or abstractive. Keyphrase generation is the process of predicting both extractive and abstractive keyphrases from a given document. This process is similar to abstractive summarization but instead of a summary the models generate keyphrases. Researchers have achieved considerable success in the field of abstractive summarization using conditional-GANs (Wang and Lee 2018). There has also been growing interest in deep learning models for keyphrase generation (Meng et al. 2017; Chan et al. 2019). Inspired by these advances, we propose a new GAN architecture for keyphrase generation where the generator produces a sequence of keyphrases from a given document and the discriminator distinguishes between human-curated and machine-generated keyphrases.  Proposed Adversarial Model As with most GAN architectures, our model also consists of a generator (G) and discriminator (D), which are trained in an alternating fashion (Goodfellow et al. 2014). Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 Code is available at https://github.com/avinsit123/keyphrasegan  Generator - Given a document d = {x1 , x2 , ..., xn }, where xi is the ith token, the generator produces a sequence of keyphrases: y = {y1 , y2 , ..., ym }, where each keyphrase yi is composed of tokens yi1 , yi2 , ..., yili . We employ catSeq model (Yuan et al. 2018) for the generation process, which uses an encoder-decoder framework: the encoder being a bidirectional Gated Recurrent Unit (bi-GRU) and the decoder a forward GRU. To incorporate the out-of-vocabulary words, we use a copying mechanism (Gu et al. 2016). We also make use of attention mechanism to help the generator identify the relevant components of the source text. Discriminator - We propose a new hierarchical-attention model as the discriminator, which is trained to distinguish between human-curated and machine-generated keyphrases. The first layer of this model consists of m + 1 bi-GRUs. The first bi-GRU encodes the input document d as a sequence of vectors: h = {h1 , h2 , ..., hn }. The other m bi-GRUs, which have the same weight parameters, encode each keyphrase as a vector: {k1 , k2 , ..., km }. We then use an attention-based approach (Luong, Pham, and Manning 2015) to build context vectors cj for each keyphrase, where cj is a weighted average over h. By concatenating cj and kj , we get a contextualized representation ej = [cj ; kj ] of keyphrase yj . The second layer of the discriminator is another bi-GRU which consumes the document representation h and the keyphrase representations e. The final state of this layer is passed through one fully connected layer (Wf ) and sigmoid transformation to get the probability that a given keyphrase sequence is human-curated. ( st =  GRU (ht , st−1 ), for 1 ≤ t ≤ n GRU (et−n , st−1 ), for n + 1 ≤ t ≤ n + m R(yi ) = D(yi ) = σ(Wf si+n )  GAN training - For a given dataset (S), which contain the documents and corresponding keyphrases, we first pretrain the generator (G) using Maximum Likelihood Estimation. We then use this generator to produce machinegenerated keyphrases for all documents in S. These generated keyphrases along with the curated keyphrases are used to train the first version of the discriminator (D). We then employ policy gradient reinforcement learning to train the subsequent versions of G. We freeze the weight parameters of D and use it for reward calculation to train a new version of G. The reward for each keyphrase is obtained  Conclusion In this paper, we propose new GAN architecture for keyphrase generation. The proposed model obtains state-ofthe-art performance in generating abstractive keyphrases. To our knowledge, this is the first work that applies GANs to keyphrase generation problem. Model Catseq(Ex) catSeq-RL(Ex.) GAN(Ex.) catSeq(Abs.) catSeq-RL(Abs.) GAN(Abs.)  Figure 1: Schematic of Proposed Discriminator(D) from the last m states of the second bi-GRU layer in D (see Figure 1). The gradient update is given as: 5RG =  Pm  i=1 [D(yi )  − B] 5 log  Qli  j=1  j  1:j−1  G(yi — yi  , y1:i−1 , x)  where B is a baseline obtained by greedy decoding of keyphrase sequence. The resulting generator is then used to create new training samples for D. This process is continued till G converges.  Score F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M F1@5 F1@M  Inspec 0.2350 0.2864 0.2501 0.3000 0.2481 0.2970 0.0045 0.0085 0.0090 0.0017 0.0100 0.0190  Krapivin 0.2680 0.3610 0.2870 0.3630 0.2862 0.3700 0.0168 0.0320 0.0262 0.0460 0.0240 0.0440  NUS 0.3330 0.3982 0.3750 0.4330 0.3681 0.4300 0.0126 0.0170 0.0190 0.0310 0.0193 0.0340  KP20k 0.2840 0.3661 0.3100 0.3830 0.3002 0.3810 0.0200 0.0360 0.0240 0.0440 0.0250 0.0450  Table 1: Extractive and Abstractive Keyphrase Metrics Model Catseq Catseq-RL GAN  Inspec 0.87803 0.8602 0.891  Krapivin 0.781 0.786 0.771  NUS 0.82118 0.83 0.853  KP20k 0.804 0.809 0.85  Table 2: α-nDCG@5 metrics  References Experiments and Results We trained the proposed GAN model on KP20k dataset (Meng et al. 2017) which consists of 567,830 samples for training, 20,000 each for testing and validation. Each sample consists of an abstract, title, and the corresponding keyphrases of a scientific article. We evaluated the model on four datasets: Inspec, NUS, KP20k, and Krapivin, which contain 600, 211, 20,000, and 800 test samples respectively. For training G, we used Adagrad optimizer with learning rate ≈ 0.0005. We compare our proposed approach against 2 baseline models - catSeq (Yuan et al. 2018), RL-based catSeq Model (Chan et al. 2019) in terms of F1 scores as explained in (Yuan et al. 2018). The results, summarized in Table 1, are broken down in terms of performance on extractive and abstractive keyphrases. For extractive keyphrases, our proposed model performs better than the pre-trained catSeq model on all datasets but is slightly worse than catSeq-RL except for on Krapivin where it obtains the best F1@M of 0.37. On the other hand, for abstractive keyphrases, our model performs better than the other two baselines on three of four datasets suggesting that GAN models are more effective in generation of keyphrases. We also evaluated the models in terms of α-nDCG@5 (Clarke et al. 2008). The results are summarized in Table 2. Our model obtains the best performance on three out of the four datasets. The difference is most prevalent in KP20k, the largest of the four datasets, where our GAN model (at 0.85) is nearly 5% better than both the other baseline models.  [Chan et al. 2019] Chan, H. P.; Chen, W.; Wang, L.; and King, I. 2019. Neural keyphrase generation via reinforcement learning with adaptive rewards. In ACL. [Clarke et al. 2008] Clarke, C.; Kolla, M.; V. Cormack, G.; Vechtomova, O.; Ashkan, A.; Bttcher, S.; and MacKinnon, I. 2008. Novelty and diversity in information retrieval evaluation. Proc. of the 31st ACM SIGIR 659–666. [Goodfellow et al. 2014] Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2014. Generative adversarial nets. In Advances in neural information processing systems, 2672–2680. [Gu et al. 2016] Gu, J.; Lu, Z.; Li, H.; and Li, V. O. 2016. Incorporating copying mechanism in sequence-to-sequence learning. arXiv preprint arXiv:1603.06393. [Luong, Pham, and Manning 2015] Luong, T.; Pham, H.; and Manning, C. D. 2015. Effective approaches to attentionbased neural machine translation. In EMNLP. [Meng et al. 2017] Meng, R.; Zhao, S.; Han, S.; He, D.; Brusilovsky, P.; and Chi, Y. 2017. Deep keyphrase generation. arXiv preprint arXiv:1704.06879. [Wang and Lee 2018] Wang, Y., and Lee, H. 2018. Learning to encode text as human-readable summaries using generative adversarial networks. CoRR abs/1810.02851. [Yuan et al. 2018] Yuan, X.; Wang, T.; Meng, R.; Thaker, K.; He, D.; and Trischler, A. 2018. Generating diverse numbers of diverse keyphrases. ArXiv abs/1810.05241.  "
"Preprint  arXiv:1909.12238v1 [cs.AI] 26 Sep 2019  V-MPO: O N -P OLICY M AXIMUM A P OSTERIORI P OLICY O PTIMIZATION FOR D ISCRETE AND C ONTINUOUS C ONTROL H. Francis Song∗, Abbas Abdolmaleki∗, Jost Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack W. Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas Heess, Dan Belov, Martin Riedmiller, Matthew M. Botvinick DeepMind, London, UK {songf,aabdolmaleki,springenberg,aidanclark, soyer,jwrae,snoury,arahuja,liusiqi,dhruvat, heess,danbelov,riedmiller,botvinick}@google.com I NTRODUCTION  Deep reinforcement learning (RL) with neural network function approximators has achieved superhuman performance in several challenging domains (Mnih et al., 2015; Silver et al., 2016; 2018). Some of the most successful recent applications of deep RL to difficult environments such as Dota 2 (OpenAI, 2018a), Capture the Flag (Jaderberg et al., 2019), Starcraft II (DeepMind, 2019), and dexterous object manipulation (OpenAI, 2018b) have used policy gradient-based methods such as Proximal Policy Optimization (PPO) (Schulman et al., 2017) and the Importance-Weighted Actor-Learner Architecture (IMPALA) (Espeholt et al., 2018), both in the approximately on-policy setting. Policy gradients, however, can suffer from large variance that may limit performance, especially for high-dimensional action spaces (Wu et al., 2018). In practice, moreover, policy gradient methods typically employ carefully tuned entropy regularization in order to prevent policy collapse. As an alternative to policy gradient-based algorithms, in this work we introduce an approximate policy iteration algorithm that adapts Maximum a Posteriori Policy Optimization (MPO) (Abdolmaleki et al., 2018a;b) to the on-policy setting. The modified algorithm, V-MPO, relies on a learned state-value function V (s) instead of the state-action value function used in MPO. Like MPO, rather than directly updating the parameters in the direction of the policy gradient, V-MPO first constructs a target distribution for the policy update subject to a sample-based KL constraint, then calculates the gradient that partially moves the parameters toward that target, again subject to a KL constraint. ∗  Equal contribution  1  Preprint  As we are particularly interested in scalable RL algorithms that can be applied to multi-task settings where a single agent must perform a wide variety of tasks, we show for the case of discrete actions that the proposed algorithm surpasses previously reported performance in the multi-task setting for both the Atari-57 (Bellemare et al., 2012) and DMLab-30 (Beattie et al., 2016) benchmark suites, and does so reliably without population-based tuning of hyperparameters (Jaderberg et al., 2017a). For a few individual levels in DMLab and Atari we also show that V-MPO can achieve scores that are substantially higher than has previously been reported, especially in the challenging Ms. Pacman. V-MPO is also applicable to problems with high-dimensional, continuous action spaces. We demonstrate this in the context of learning to control both a 22-dimensional simulated humanoid from full state observations—where V-MPO reliably achieves higher asymptotic performance than previous algorithms—and a 56-dimensional simulated humanoid from pixel observations (Tassa et al., 2018; Merel et al., 2019). In addition, for several OpenAI Gym tasks (Brockman et al., 2016) we show that V-MPO achieves higher asymptotic performance than has previously been reported.  2  BACKGROUND AND SETTING  We consider the discounted RL setting, where we seek to optimize a policy π for a Markov Decision Process described by states s, actions a, initial state distribution ρenv 0 (s0 ), transition probabilities P env (st+1 |st , at ), reward function r(st , at ), and discount factor γ ∈ (0, 1). In deep RL, the policy πθ (at |st ), which specifies the probability that the agent takes action at in state st at time t, is described by a neural network with parameters θ. We consider problems where both the states s and actions a may be discrete or continuous.  P∞Two kfunctions play a central role in RL: the state-value function V π (st ) = Eat ,st+1 ,a ,... t+1  P∞ k k=0 γ r(st+k  , at+k ) and the state-action  π value  π function Q (st , at ) = Est+1 ,at+1 ,... k=0 γ r(st+k , at+k ) = r(st , at ) + γEst+1 V (st+1 ) , env where s0 ∼ ρenv (st+1 |st , at ). 0 (s0 ), at ∼ π(at |st ), and st+1 ∼ P In the usual formulation of the RL problem, is to find  P∞the goal  a policy π that maximizes the expected t return given by J(π) = Es0 ,a0 ,s1 ,a1 ,... t=0 γ r(st , at ) . In policy gradient algorithms (Williams, 1992; Sutton et al., 2000; Mnih et al., 2016), for example, this objective is directly optimized by estimating the gradient of the expected return. An alternative approach to finding optimal policies derives from research that treats RL as a problem in probabilistic inference, including Maximum a Posteriori Policy Optimization (MPO) (Levine, 2018; Abdolmaleki et al., 2018a;b). Here our objective is subtly different, namely, given a suitable criterion for what are good actions to take in a certain state, how do we find a policy that achieves this goal? As was the case for the original MPO algorithm, the following derivation is valid for any such criterion. However, the policy improvement theorem (Sutton & Barto, 1998) tells us that a policy update performed by exact policy iteration, π(s) = arg maxa [Qπ (s, a) − V π (s)], can improve the policy if there is at least one state-action pair with a positive advantage and nonzero probability of visiting the state. Motivated by this classic result, in this work we specifically choose an exponential function of the advantages Aπ (s, a) = Qπ (s, a) − V π (s). P Notation. In the following we use s,a to indicate both discrete and continuous sums (i.e., integrals) P over states s and actions a depending on the setting. A sum with indices only, such as s,a , denotes P a sum over all possible states and actions, while s,a∼D , for example, denotes a sum over sample states and actions from a batch of trajectories (the “dataset”) D.  3  R ELATED WORK  V-MPO shares many similarities, and thus relevant related work, with the original MPO algorithm (Abdolmaleki et al., 2018a;b). In particular, the general idea of using KL constraints to limit the size of policy updates is present in both Trust Region Policy Optimization (TRPO; Schulman et al., 2015) and Proximal Policy Optimization (PPO) (Schulman et al., 2017); we note, however, that this corresponds to the E-step constraint in V-MPO. Meanwhile, the introduction of the M-step KL constraint and the use of top-k advantages distinguishes V-MPO from Relative Entropy Policy Search (REPS) (Peters et al., 2008). Interestingly, previous attempts to use REPS with neural network function approximators reported very poor performance, being particularly prone to local optima (Duan et al., 2016). In 2  Preprint  contrast, we find that the principles of EM-style policy optimization, when combined with appropriate constraints, can reliably train powerful neural networks, including transformers, for RL tasks. Like V-MPO, Supervised Policy Update (SPU) (Vuong et al., 2019) seeks to exactly solve an optimization problem and fit the parametric policy to this solution. As we argue in Appendix D, however, SPU uses this nonparametric distribution quite differently from V-MPO; as a result, the final algorithm is closer to a policy gradient algorithm such as PPO.  4  M ETHOD  V-MPO is an approximate policy iteration (Sutton & Barto, 1998) algorithm with a specific prescription for the policy improvement step. In general, policy iteration uses the fact that the true state-value function V π corresponding to policy π can be used to obtain an improved policy π 0 . Thus we can 1. Generate trajectories τ from an old “target” policy πθold (a|s) whose parameters θold are fixed. To control the amount of data generated by a particular policy, we use a target network which is fixed for Ttarget learning steps (Fig. 5a in the Appendix). 2. Evaluate the policy πθold (a|s) by learning the value function V πθold (s) from empirical returns and estimating the corresponding advantages Aπθold (s, a) for the actions that were taken. 3. Estimate an improved “online” policy πθ (a|s) based on Aπθold (s, a). The first two steps are standard, and describing V-MPO’s approach to step (3) is the essential contribution of this work. At a high level, our strategy is to first construct a nonparametric target distribution for the policy update, then partially move the parametric policy towards this distribution subject to a KL constraint. Ultimately, we use gradient descent to optimize a single, relatively simple loss, which we provide here in complete form in order to ground the derivation of the algorithm. Consider a batch of data D consisting of a number of trajectories, with |D| total stateaction samples. Each trajectory consists of an unroll of length n of the form τ = (st , at , rt+1 ), . . . , (st+n−1 , at+n−1 , rt+n ), st+n including the bootstrapped state st+n , where rt+1 = r(st , at ). The total loss is the sum of a policy evaluation loss and a policy improvement loss, L(φ, θ, η, α) = LV (φ) + LV-MPO (θ, η, α),  (1)  where φ are the parameters of the value network, θ the parameters of the policy network, and η and α are Lagrange multipliers. In practice, the policy and value networks share most of their parameters in the form of a shared convolutional network (a ResNet) and recurrent LSTM core, and are optimized together (Fig. 5b in the Appendix) (Mnih et al., 2016). We note, however, that the value network parameters φ are considered fixed for the policy improvement loss, and gradients are not propagated. The policy evaluation loss for the value function, LV (φ), is the standard regression to n-step returns and is given by Eq. 6 below. The policy improvement loss LV-MPO (θ, η, α) is given by LV-MPO (θ, η, α) = Lπ (θ) + Lη (η) + Lα (θ, α).  (2)  Here the policy loss is the weighted maximum likelihood loss Lπ (θ) = −  X  ψ(s, a) log πθ (a|s),  ψ(s, a) =  s,a∼D̃  Atarget (s,a)  η , P Atarget (s,a)  exp s,a∼D̃ η  exp  (3)  where the advantages Atarget (s, a) for the target network policy πθtarget (a|s) are estimated according to the standard method described below. The tilde over the dataset, D̃, indicates that we take samples corresponding to the top half advantages in the batch of data. The η, or “temperature”, loss is ""  target # 1 X A (s, a) . (4) Lη (η) = ηη + η log exp η |D̃| s,a∼D̃  The KL constraint, which can be viewed as a form of trust-region loss, is given by   i 1 Xh Lα (θ, α) = α α − sg DKL πθtarget (a|s)kπθ (a|s) + sg[[α]]DKL πθtarget (a|s)kπθ (a|s) , |D| s∈D  (5) 3  Preprint  where sg[[·]] indicates a stop gradient, i.e., that the enclosed term is assumed constant with respect to all variables. Note that here we use the full batch D, not D̃. We used the Adam optimizer (Kingma & Ba, 2015) with default TensorFlow hyperparameters to optimize the total loss in Eq. 1. In particular, the learning rate was fixed at 10−4 for all experiments. 4.1  P OLICY EVALUATION  In the present setting, policy evaluation means learning an approximate state-value function V π (s) given a policy π(a|s), which we keep fixed for Ttarget learning steps (i.e., batches of trajectories). We note that the value function corresponding to the target policy is instantiated in the “online” network receiving gradient updates; bootstrapping uses the online value function, as it is the best available estimate of the value function for the target policy. Thus in this section π refers to πθold , while the value function update is performed on the current φ, which may share parameters with the current θ. We fit a parametric value function Vφπ (s) with parameters φ by minimizing the squared loss 1 X (n) 2 , LV (φ) = Vφπ (st ) − Gt 2|D|  (6)  st ∼D  (n)  where Gt is the standard n-step target for the value function at state st at time t (Sutton & Barto, 1998). This return uses the actual rewards in the trajectory and bootstraps from the value function for Pt+n−1 (n) the rest: for each ` = t, . . . , t + n − 1 in an unroll, G` = k=` γ k−` rk + γ t+n−` Vφπ (st+n ). The advantages, which are the key quantity of interest for the policy improvement step in V-MPO, (n) are then given by Aπ (st , at ) = Gt − Vφπ (st ) for each st , at in the batch of trajectories. PopArt normalization. As we are interested in the multi-task setting where a single agent must learn a large number of tasks with differing reward scales, we used PopArt (van Hasselt et al., 2016; Hessel et al., 2018) for the value function, even when training on a single task. Specifically, the value function outputs a separate value for each task in normalized space, which is converted to actual returns by a shift and scaling operation, the statistics of which are learned during training. We used a scale lower bound of 10−2 , scale upper bound of 106 , and learning rate of 10−4 for the statistics. The lower bound guards against numerical issues when rewards are extremely sparse. Importance-weighting for off-policy data. It is possible to importance-weight the samples using V-trace to correct for off-policy data (Espeholt et al., 2018), for example when data is taken from a replay buffer. For simplicity, however, no importance-weighting was used for the experiments presented in this work, which were mostly on-policy. 4.2  P OLICY IMPROVEMENT IN V-MPO  In this section we show how, given the advantage function Aπθold (s, a) for the state-action distribution pθold (s, a) = πθold (a|s)p(s) induced by the old policy πθold (a|s), we can estimate an improved policy πθ (a|s). More formally, let I denote the binary event that the new policy is an improvement (in a sense to be defined below) over the previous policy: I = 1 if the policy is successfully improved and 0 otherwise. Then we would like to find the mode of the posterior distribution over parameters θ conditioned on this event, i.e., we seek the maximum a posteriori (MAP) estimate   θ∗ = arg max log pθ (I = 1) + log p(θ) , (7) θ  where we have written p(I = 1|θ) as pθ (I = 1) to emphasize the parametric nature of the dependence    on θ. We use the well-known identity log p(X) = Eψ(Z) log p(X,Z) + DKL ψ(Z)kp(Z|X) for ψ(Z) any latent distribution ψ(Z), where DKL (ψ(Z)kp(Z|X)) is the Kullback-Leibler divergence between ψ(Z) and p(Z|X) with respect to Z, and the first term is a lower bound because the KL divergence is always non-negative. Then considering s, a as latent variables, X  pθ (I = 1, s, a) log pθ (I = 1) = ψ(s, a) log + DKL ψ(s, a)kpθ (s, a|I = 1) . (8) ψ(s, a) s,a Policy improvement in V-MPO consists of the following two steps which have direct correspondences to the expectation maximization (EM) algorithm (Neal & Hinton, 1998): In the expectation (E) step, 4  Preprint  we choose the variational distribution ψ(s, a) such that the lower bound on log pθ (I = 1) is as tight as possible, by minimizing the KL term. In the maximization (M) step we then find parameters θ that maximize the corresponding lower bound, together with the prior term in Eq. 7. 4.2.1  E- STEP  In the E-step, our goal is to choose the variational distribution ψ(s, a) such that the lower bound on log pθ (I = 1) is as tight as possible, which is the case when the KL term in Eq. 8 is zero. Given the old parameters θold , this simply leads to ψ(s, a) = pθold (s, a|I = 1), or X pθ (s, a)pθold (I = 1|s, a) , pθold (I = 1) = pθold (s, a)pθold (I = 1|s, a). (9) ψ(s, a) = old pθold (I = 1) s,a Intuitively, this solution weights the probability of each state-action pair with its relative improvement probability pθold (I = 1|s, a). We now choose a distribution pθold (I = 1|s, a) that leads to our desired outcome. As we prefer actions that lead to a higher advantage in each state, we suppose that this probability is given by   πθ A old (s, a) (10) pθold (I = 1|s, a) ∝ exp η for some temperature η > 0, from which we obtain the equation on the right in Eq. 3. This probability depends on the old parameters θold and not on the new parameters θ. Meanwhile, the value of η allows us to control the diversity of actions that contribute to the weighting, but at the moment is arbitrary. It turns out, however, that we can tune η as part of the optimization, which is desirable since the optimal value of η changes across iterations. The convex loss that achieves this, Eq. 4, is derived in Appendix A by minimizing the KL term in Eq. 8 subject to a hard constraint on ψ(s, a). Top-k advantages. We found that learning improves substantially if we take only the samples corresponding to the highest 50% of advantages in each batch for the E-step, corresponding to the use of D̃ rather than D in Eqs. 3, 4. Importantly, these must be consistent between the maximum likelihood weights in Eq. 3 and the temperature loss in Eq. 4, since, mathematically, this is justified by choosing the corresponding policy improvement probability in Eq. 10 to only use the top half of the advantages. This is similar to the technique used in Covariance Matrix Adaptation - Evolutionary Strategy (CMA-ES) (Hansen et al., 1997; Abdolmaleki et al., 2017), and is a special case of the more general feature that any rank-preserving transformation is allowed under this formalism. Importance weighting for off-policy corrections. As for the value function, importance weights can be used in the policy improvement step to correct for off-policy data. While not used for the experiments presented in this work, details for how to carry out this correction are given in Appendix E. 4.2.2  M- STEP : C ONSTRAINED SUPERVISED LEARNING OF THE PARAMETRIC POLICY  In the E-step we found the nonparametric variational state-action distribution ψ(s, a), Eq. 9, that gives the tightest lower bound to pθ (I = 1) in Eq. 8. In the M-step we maximize this lower bound together with the prior term log p(θ) with respect to the parameters θ, which effectively leads to a constrained weighted maximum likelihood problem. Thus the introduction of the nonparametric distribution in Eq. 9 separates the RL procedure from the neural network fitting. We would like to find new parameters θ that minimize X pθ (I = 1, s, a) L(θ) = − ψ(s, a) log − log p(θ). ψ(s, a) s,a  (11)  Note, however, that so far we have worked with the joint state-action distribution ψ(s, a) while we are in fact optimizing for the policy, which is the conditional distribution πθ (a|s). Writing pθ (s, a) = πθ (a|s)p(s) since only the policy is parametrized by θ and dropping terms that are not parametrized by θ, the first term of Eq. 11 is seen to be the weighted maximum likelihood policy loss X Lπ (θ) = − ψ(s, a) log πθ (a|s). (12) s,a  In the sample-based computation of this loss, we assume that any state-action pairs not in the batch of trajectories have zero weight, leading to the normalization in Eq. 3. 5  Preprint  As in the original MPO algorithm, a useful  prior is to keep the new  policy πθ (a|s) close to the old policy πθold (a|s): log p(θ) ≈ −αEs∼p(s) DKL πθold (a|s)kπθ (a|s) . While intuitive, we motivate this more formally in Appendix B. It is again more convenient to specify a bound on the KL divergence instead of tuning α directly, so we solve the constrained optimization problem h X i θ∗ = arg min − ψ(s, a) log πθ (a|s) s.t. E DKL πθold (a|s)kπθ (a|s) < α . (13) θ  s∼p(s)  s,a  Intuitively, the constraint in the E-step expressed by Eq. 19 in Appendix A for tuning the temperature only constrains the nonparametric distribution; it is the constraint in Eq. 13 that directly limits the change in the parametric policy, in particular for states and actions that were not in the batch of samples and which rely on the generalization capabilities of the neural network function approximator. To make the constrained optimization problem amenable to gradient descent, we use Lagrangian relaxation to write the unconstrained objective as   h i J (θ, α) = Lπ (θ) + α α − E DKL πθold (a|s)kπθ (a|s) , (14) s∼p(s)  which we can optimize by following a coordinate-descent strategy, alternating between the optimization over θ and α. Thus, in addition to the policy loss we arrive at the constraint loss   h  h i i Lα (θ, α) = α α − E sg DKL πθold kπθ + sg[[α]] E DKL πθold kπθ . (15) s∼p(s)  s∼p(s)  Replacing the sum over states with samples gives Eq. 5. Since η and α are Lagrange multipliers that must be positive, after each gradient update we project the resulting η and α to a small positive value which we choose to be ηmin = αmin = 10−8 throughout the results presented below. For continuous action spaces parametrized by Gaussian distributions, we use decoupled KL constraints for the M-step in Eq. 15 as in Abdolmaleki et al. (2018b); the precise form is given in Appendix C.  5  E XPERIMENTS  Details on the network architecture and hyperparameters used for each task are given in Appendix F. 5.1  D ISCRETE ACTIONS : DML AB , ATARI  DMLab. DMLab-30 (Beattie et al., 2016) is a collection of visually rich, partially observable 3D environments played from the first-person point of view. Like IMPALA, for DMLab we used pixel control as an auxiliary loss for representation learning (Jaderberg et al., 2017b; Hessel et al., 2018). However, we did not employ the optimistic asymmetric reward scaling used by previous IMPALA experiments to aid exploration on a subset of the DMLab levels, by weighting positive rewards more than negative rewards (Espeholt et al., 2018; Hessel et al., 2018; Kapturowski et al., 2019). Unlike in Hessel et al. (2018) we also did not use population-based training (PBT) (Jaderberg et al., 2017a). Additional details for the settings used in DMLab can be found in Table 5 of the Appendix. Fig. 1a shows the results for multi-task DMLab-30, comparing the V-MPO learning curves to data obtained from Hessel et al. (2018) for the PopArt IMPALA agent with pixel control. We note that the result for V-MPO at 10B environment frames across all levels matches the result for the Recurrent Replay Distributed DQN (R2D2) agent (Kapturowski et al., 2019) trained on individual levels for 10B environment steps per level. Fig. 2 shows example individual levels in DMLab where V-MPO achieves scores that are substantially higher than has previously been reported, for both R2D2 and IMPALA. The pixel-control IMPALA agents shown here were carefully tuned for DMLab and are similar to the “experts” used in Schmitt et al. (2018); in all cases these results match or exceed previously published results for IMPALA (Espeholt et al., 2018; Kapturowski et al., 2019). Atari. The Atari Learning Environment (ALE) (Bellemare et al., 2012) is a collection of 57 Atari 2600 games that has served as an important benchmark for recent deep RL methods. We used the standard preprocessing scheme and a maximum episode length of 30 minutes (108,000 frames), see Table 6 in the Appendix. For the multi-task setting we followed Hessel et al. (2018) in setting the discount to zero on loss of life; for the example single tasks we did not employ this trick, since it 6  Preprint  (a) Multi-task DMLab-30.  (b) Multi-task Atari-57.  Figure 1: (a) Multi-task DMLab-30. IMPALA results show 3 runs of 8 agents each; within a run hyperparameters were evolved via PBT. For V-MPO each line represents a set of hyperparameters that are fixed throughout training. The final result of R2D2+ trained for 10B environment steps on individual levels (Kapturowski et al., 2019) is also shown for comparison (orange line). (b) Multi-task Atari-57. In the IMPALA experiment, hyperparameters were evolved with PBT. For V-MPO each of the 24 lines represents a set of hyperparameters that were fixed throughout training, and all runs achieved a higher score than the best IMPALA run. Data for IMPALA (“Pixel-PopArtIMPALA” for DMLab-30 and “PopArt-IMPALA” for Atari-57) was obtained from the authors of Hessel et al. (2018). Each environment frame corresponds to 4 agent steps due to the action repeat.  Figure 2: Example levels from DMLab-30, compared to IMPALA and more recent results from R2D2+, the larger, DMLab-specific version of R2D2 (Kapturowski et al., 2019). The IMPALA results include hyperparameter evolution with PBT.  can prevent the agent from achieving the highest score possible by sacrificing lives. Similarly, while in the multi-task setting we followed previous work in clipping the maximum reward to 1.0, no such clipping was applied in the single-task setting in order to preserve the original reward structure. Additional details for the settings used in Atari can be found in Table 6 in the Appendix. Fig. 1b shows the results for multi-task Atari-57, demonstrating that it is possible for a single agent to achieve “superhuman“ median performance on Atari-57 in approximately 4 billion (∼70 million per level) environment frames. We also compare the performance of V-MPO on a few individual Atari levels to R2D2 (Kapturowski et al., 2019), which previously achieved some of the highest scores reported for Atari. Again, V-MPO can match or exceed previously reported scores while requiring fewer interactions with the environment. In Ms. Pacman, the final performance approaches 300,000 with a 30-minute timeout (and the maximum 1M without), effectively solving the game. Inspired by the argument in Kapturowski et al. (2019) that in a fully observable environment LSTMs enable the agent to utilize more useful representations than is available in the immediate observation, for the single-task setting we used a Transformer-XL (TrXL) (Dai et al., 2019) to replace the LSTM core. Unlike previous work for single Atari levels, we did not employ any reward clipping (Mnih et al., 2015; Espeholt et al., 2018) or nonlinear value function rescaling (Kapturowski et al., 2019). 7  Preprint  Figure 3: Example levels from Atari. In Breakout, V-MPO achieves the maximum score of 864 in every episode. No reward clipping was applied, and the maximum length of an episode was 30 minutes (108,000 frames). Supplementary video for Ms. Pacman: https://bit.ly/2lWQBy5  (a)  (b)  (c)  (d)  Figure 4: (a) Humanoid “run” from full state (Tassa et al., 2018) and (b) humanoid “gaps” from pixel observations (Merel et al., 2019). Purple curves are the same runs but without parametric KL constraints. Det. eval.: deterministic evaluation. Supplementary video for humanoid gaps: https://bit.ly/2L9KZdS. (c)-(d) Example OpenAI Gym tasks.  5.2  C ONTINUOUS CONTROL  To demonstrate V-MPO’s effectiveness in high-dimensional, continuous action spaces, here we present examples of learning to control both a simulated humanoid with 22 degrees of freedom from full state observations and one with 56 degrees of freedom from pixel observations (Tassa et al., 2018; Merel et al., 2019). As shown in Fig. 4a, for the 22-dimensional humanoid V-MPO reliably achieves higher asymptotic returns than has previously been reported, including for Deep Deterministic Policy Gradients (DDPG) (Lillicrap et al., 2015), Stochastic Value Gradients (SVG) (Heess et al., 2015), and MPO. These algorithms are far more sample-efficient but reach a lower final performance. In the “gaps” task the 56-dimensional humanoid must run forward to match a target velocity of 4 m/s and jump over the gaps between platforms by learning to actuate joints with position-control (Merel et al., 2019). Previously, only an agent operating in the space of pre-learned motor primitives was able to solve the task from pixel observations (Merel et al., 2018; 2019); here we show that V-MPO can learn a challenging visuomotor task from scratch (Fig. 4b). For this task we also demonstrate the importance of the parametric KL constraint, without which the agent learns poorly. In Figs. 4c-d we also show that V-MPO achieves the highest asymptotic performance reported for two OpenAI Gym tasks (Brockman et al., 2016). Again, MPO and Stochastic Actor-Critic (Haarnoja et al., 2018) are far more sample-efficient but reach a lower final performance.  6  C ONCLUSION  In this work we have introduced a scalable on-policy deep reinforcement learning algorithm, V-MPO, that is applicable to both discrete and continuous control domains. For the results presented in this work neither importance weighting nor entropy regularization was used; moreover, since the size of neural network parameter updates is limited by KL constraints, we were also able to use the same learning rate for all experiments. This suggests that a scalable, performant RL algorithm may not require some of the tricks that have been developed over the past several years. Interestingly, both 8  Preprint  the original MPO algorithm for replay-based off-policy learning (Abdolmaleki et al., 2018a;b) and V-MPO for on-policy learning are derived from similar principles, providing evidence for the benefits of this approach as an alternative to popular policy gradient-based methods. ACKNOWLEDGMENTS We thank Lorenzo Blanco, Trevor Cai, Greg Wayne, Chloe Hillier, and Vicky Langston for their assistance and support.  R EFERENCES Abbas Abdolmaleki, Bob Price, Nuno Lau, Luis P Reis, and Gerhard Neumann. Deriving and Improving CMA-ES with Information Geometric Trust Regions. Proceedings of the Genetic and Evolutionary Computation Conference, 2017. Abbas Abdolmaleki, Jost Tobias Springenberg, Jonas Degrave, Steven Bohez, Yuval Tassa, Dan Belov, Nicolas Heess, and Martin Riedmiller. Relative Entropy Regularized Policy Iteration. arXiv preprint, 2018a. URL https://arxiv.org/pdf/1812.02256.pdf. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a Posteriori Policy Optimisation. Int. Conf. Learn. Represent., 2018b. URL https://arxiv.org/pdf/1806.06920.pdf. Anonymous Authors. Off-Policy Actor-Critic with Shared Experience Replay. Under review, Int. Conf. Learn. Represent., 2019. Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Vı́ctor Valdés, Amir Sadik, et al. Deepmind Lab. arXiv preprint arXiv:1612.03801, 2016. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research, 47, 2012. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint, 2016. URL http://arxiv.org/abs/ 1606.01540. Peter Buchlovsky, David Budden, Dominik Grewe, Chris Jones, John Aslanides, Frederic Besse, Andy Brock, Aidan Clark, Sergio Gomez Colmenarejo, Aedan Pope, Fabio Viola, and Dan Belov. TF-Replicator: Distributed Machine Learning for Researchers. arXiv preprint, 2019. URL http://arxiv.org/abs/1902.00465. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. arXiv preprint, 2019. URL http://arxiv.org/abs/1901.02860. DeepMind. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II, 2019. URL https://deepmind.com/blog/alphastar-mastering-real-timestrategy-game-starcraft-ii/. Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking Deep Reinforcement Learning for Continuous Control. arXiv preprint, 2016. URL http://arxiv. org/abs/1604.06778. Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures. arXiv preprint, 2018. URL http://arxiv.org/abs/1802.01561. Google. Cloud TPU, 2018. URL https://cloud.google.com/tpu/. 9  Preprint  Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. arXiv preprint, 2018. URL http://arxiv.org/abs/1801.01290. Nikolaus Hansen, Andreas Ostermeier, and Andreas Ostermeier. Convergence Properties of Evolution Strategies with the Derandomized Covariance Matrix Adaptation: CMA-ES. 1997. URL http: //www.cmap.polytechnique.fr/˜nikolaus.hansen/CMAES2.pdf. Nicolas Heess, Greg Wayne, David Silver, Timothy P. Lillicrap, Yuval Tassa, and Tom Erez. Learning continuous control policies by stochastic value gradients. arXiv preprint, 2015. URL http: //arxiv.org/abs/1510.09142. Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van Hasselt. Multi-task Deep Reinforcement Learning with PopArt. arXiv preprint, 2018. URL https://arxiv.org/"
"1  Data Smashing 2.0: Sequence Likelihood (SL) Divergence For Fast Time Series Comparison  arXiv:1909.12243v1 [stat.ML] 26 Sep 2019  Yi Huang, Ishanu Chattopadhyay Institute of Genomics and Systems Biology and Department of Medicine, University of Chicago, Chicago, IL, 60637, USA {yhuang10, ishanu}@uchicago.edu  F  Introduction  Efficiently learning stochastic processes is a key challenge in analyzing time-dependency in domains where randomness cannot be ignored. For such learning to occur, we need todefine a distance metric to compare and contrast time series. Examples of such metrics from the literature include the classical lp distances and lp distances with dimensionality reduction [18], the short time series distance (STS)[20], which takes into account of irregularity in sampling rates, the edit based distances[21] and generalizations to continuous sequences[6], and the dynamic time warping (DTW)[23], which is used extensively in the speech recognition community. However these distance metrics mentioned all have either or both of the following limitations: first, dimensionality reduction and feature selection heavily relies on domain knowledge and inevitably incur trade-off between precision and computability. Most importantly, it necessitates the attention of human experts and data scientists. Secondly, when dealing with data from non-trivial stochastic process dynamics, state of the art techniques might fail to correctly estimate the similarity or lack thereof between exemplars. For example, suppose two sequences  recording n tosses of a fair coins, use 1 to represent a head and 0, tail. The two sequences are extremely unlikely to share any similarity on the face value, i:e: they have a large pointwise distance, but they are generated by the same process. A good measure of similarity should strive to disambiguate the underlying processes. The Smash2.0 metric introduced here addresses both these limitations. When presented with finite sample paths, the Smash2.0 algorithm is specifically designed to estimate a distance between the generating models of the time series samples. The intuition for the Smash2.0 metric follows from a basic result in information theory: If we know the true distribution p of the random variable, we could construct a code with average description length H (p), where H ( ) is the entropy of a distribution. If, instead, we used the code for a distribution q , we would need H (p) + D(p q ) bits on the average to describe the random variable. Thus, deviation in the distributions show up as KL divergence. If we can generalize the notion of KL divergence to processes, then it might be possible to quantify deviations in process dynamics via an increase in the entropy rate by the corresponding divergence. Our ultimate goal is to design an algorithm that operates on a pair of data streams taking values in a finite data set. Nevertheless, to establish the correctness of our algorithm, we need to decide on a specific scheme for representing stochastic processes taking values in a finite alphabet. We further assume that our processes are ergodic and stationary. The specific modeling paradigm for modeling stochastic processes we use in this paper is called Probabilistic Finite-State Automaton, or PFSA for short, which has been studied in [9], [12], [4], [2]. PFSAs model discrete value stochastic processes, and are strictly more expressive than Markov chain in the sense that it can generate processes that are not Markov of finite order[7]. It is also shown in [12] to be able to approximate any HMM with arbitrary accuracy. Moreover, PFSA has the property that many key statistical quantities of the processes they generate, such as entropy rate[8] and KLdivergence[19], enjoy closed-form formula, which are generally missing for more complicated models. The structural simplicity of PFSA combined with the clarity of its mathematical theory greatly facilitates the construction of efficient inference algorithms and its application as the underlying models to data mining and machine learning tasks with sequential data. Here we want to point out the resemblance of the PFSA model to the variational autoencoder (VAE) [25], [16] framework. The inference of PFSA from the    jj  2 input can be thought as the training of the encoder in a VAE, and the performance of both the VAE and the PFSA model could be then evaluated by the log-likelihood of input as begin generated by the PFSA, as detailed in Sec. 2.2, or the decoder of the VAE. One distance metric that is in the line of construction and exactly where Smash2.0 derives its name from is the Smash proposed in [4]. Smash is also based on PFSA modeling and designed directly to represent the difference or similarity between the generating models of the sequences rather than the sample path themselves. However, as while as both Smash and Smash2.0 enjoy the advantage of not requiring dimensionality reduction or domain knowledge for feature extraction, Smash2.0 is much more computationally efficient than Smash. In Sec. 6.3, we compare explicitly Smash2.0 to two other distance metrics, Smash and fastDTW, a distance metric proposed in [26] that enjoys growing popularity. We show with a simple synthetic dataset that Smash2.0 can surpass the other two algorithms at both computational efficiency and performance. The remaining of the paper is organized as follows. In Sec. 2, we introduce basic concepts of stochastic processes and establish the correspondence between processes and labeled directed graphs via the core concept of causal state. The definition and basic properties of PFSA are introduced by the end of Sec. 2.2. In Sec. 3, with an in-depth study of the set of causal states, we answer the question of when will a stochastic process has a PFSA generator. An inference algorithm, GenESeSs, of PFSA is given in Sec. 4. We introduce the concept of reducedness of PFSA in Sec. 5.1, and the closed form formula for entropy rate and KL divergence of the processes generated by reduced PFSA are given in Sec. 5.2, together with theorem on log-likelihood convergence which is the key in learning with PFSA. In Sec. 6 we introduce the definition of 2.0 together with quantization of continuous sequences and the quality measurement of quantization scheme. We also compare Smash2.0 with Smash and fastDTW in efficiency and performance at end of this section. In Sec. 7, we apply the distance metric proposed to two datasets arising from real world problems.  2 2.1  = i for i = 1; : : : ; n, it is S ! [0; 1] defined by  (1 : : : n ! ) = P r (X1 : : : Xn = 1 : : : n ) ; (1) is a premeasure on S . By Charathéodory extension theorem, the  -finite premeasure  can be extended uniquely to a measure over F = (S ), where (S ) is the -field generated by S . Denoting the measure also by , we see that every stochastic process induces a probability space (! ; F ; ) over ! . In light of Eq. (1) and also for notational brevity, we denote  (x! ) by P r(x) when no confusion arises. We note that P r() =  (! ) =  (! ) = 1. producing a realization with Xi straightforward to verify that  :  Readers who are not familiar with the measure-theoretic terms used here can read Chap. 1 of [17] for reference. Taking one step further, and denoting the collection of all measures over (! ; ) by  , we see that we can get a family of measures in  from a process in addition to .  M  F  M  F  S  ); ( ! ) = PPrr((xy x)  x y  for all y  2 ? .  Now we introduce the concept of Probabilistic Nerode Equivalence, which was first introduced in [5]. Definition 2 (Probabilistic Nerode Equivalence). For any pair of sequences x; y ? , x is equivalent to y, written as x y, if and only if either P r(x) = P r(y ) = 0, or x = y .  2    The interested reader can verify that the relation defined above is indeed an equivalence relation which is also right-invariant in the sense that x y xz yz , for all z ? . We denote the equivalence class of sequence x by [x]. We note that [x] is well-defined because x = y for x; y [x]. An equivalence class is also called a causal state [4] since the distribution of future events preceded by possibly distinct x; y [x] are both determined by [x] . We denote [x] (y ! ) by P r[x] (y ) when no confusion arises. We note that [] = . Remark 1. Since the equivalence class x ? P r(x) = 0 plays no role in our future discussion, we ignore it as a causal state from this point on.   )    2  2  2  f 2 j  Foundation Stochastic Processes and Causal States  In this paper we study the generative model for stationary ergodic stochastic processes [22], [9] over a finite alphabet. Specifically, we consider a set of -valued random variables Xt t2N+ indexed by positive integers representing time steps. By stationary, we mean strictly stationary, i.e. the finite-dimensional distributions[10] are invariant of time. By ergodic, we mean that all finite-dimensional distributions can be approximated with arbitrary accuracy with long enough realization. We are especially interested in processes in which the Xi s are not independent. We denote the alphabet by  and use lower case Greeks (e.g.  or  ) for symbols in . We use lower case Latins (e.g. x or y ) to denote sequences of symbols, x = 1 2 : : : n for example, with the empty sequence denoted by . The length of a sequence x is denoted by x . The set of sequences of length d is denoted by d , and the collection of sequences of finite length is denoted by ? , S1 ? d i.e.  = d=0  . We use ! to denote the set of infinitely long sequences, and x! to denote the collection of infinite sequences with x ? as prefix. We note that, since all sequences can be viewed as prefixed by , we have ! = ! . We note that = x! x ? is a semiring over ! . Let P r (X1 : : : Xn = 1 : : : n ) denote the probability of the process  f g  jj  2  S f  j 2  g  2  Definition 1 (Observation Induced Measures). For x ? with P r(x) > 0, the measure x is the extension to of the premeasure defined on the semiring given by  g  2  Definition 3 (Derivatives). For any d N+ , the d-th order derivative of an equivalence class [x], written as d[x] , is defined to be the marginal distribution of [x] on d , with the entry indexed by y denoted by d[x] (y ). The first-order derivative is also called the symbolic derivative, in [4] since 1 = , and is denoted by [x] for short. The derivative of a sequence is that of its equivalence class, i.e. dx = d[x] . We note that d is the marginal distribution of  on d , and is denoted by d for short. 2.2  From Causal States to Probabilistic Automaton  From now on, we denote the set of causal states of a process by Q when no confusion arises. We start this section by showing that there is a labeled directed graph [1] associated with any stochastic process. For any q Q and   such that P rq ( ) > 0, by rightinvariance of probabilistic Nerode equivalence, there exists a q 0 Q, such that x q 0 for all x q . Whenever the scenario described happens, we can put a directed edge from q to q 0 and label it by  and P rq ( ), and by doing this for all q Q and  2  2  2  2  2  2  3 TABLE 1: Causality table of an order-1 Markov process with causal states Q = q ; q0 ; q1  f  x   0 1 00 01 10 11 000 001 010 011 100 101 110 111  :: :  g  P r ( x) 1 :5 :5 :3 :2 :2 :3 :18 :12 :08 :12 :12 :08 :12 :18  :: :  x  causal state  (:5; :5) (:6; :4) (:4; :6) (:6; :4) (:4; :6) (:6; :4) (:4; :6) (:6; :4) (:4; :6) (:6; :4) (:4; :6) (:6; :4) (:4; :6) (:6; :4) (:4; :6)  q q0 q1 q0 q1 q0 q1 q0 q1 q0 q1 q0 q1 q0 q1 :: :  :: :  0(:6)  q0  0(:5)  q1  1(:4)  q  1(:6) 0(:6)  j jj j  f:(q;)=q0 g  q0  q1  1(:6)  1(:4) 1(:5)    2   , we get a (possibly infinite) labeled directed graph with vertex set Q. Example 1 (An Order-One Markov Process). We now carry out the construction described above on an order-1 Markov process [13] over alphabet  = 0; 1 , in which Xt+1 follows a Bernoulli distribution conditioned on the value of Xt . Specifically we have  f g  = 0jXt = 1) = :4: Together with the specification P r (X1 = 0) = :5, we can check (  = 0jXt = 0) = :6;  (  P r Xt+1  that the process is stationary and ergodic. The reason that we choose this process as our first example is because it has a small set of causal states of size 3. We list the causal states of sequence up to length 3 in Tab. 1. Since it is impossible to write down the infinite-dimensional distribution x , we only show the symbolic derivative x in Tab. 1, but we can verify that x = y if and only if x = y for this process. Now, we conceptualize the labeled directed graph obtained from analyzing the causal states by an automaton structure[29], which we call probabilistic finite-state automaton[4], and show how we can get a stochastic process from it.  Definition 4 (Probabilistic Finite-State Automaton (PFSA)). A probabilistic finite-state automaton G is specified by a quadruple (; Q; ;  e ), where  is a finite alphabet, Q is a finite set of states,  is a partial map from Q  to Q called transition map, and  e , called observation probability, is a map from Q to P , where P is the space of probability distributions over . The entry indexed by  of  e (q ) is written as  e (q;  ). We call the directed graph (not necessarily simple with possible loops and multiedges) with vertices in Q and edges specified      j jj j  Fig. 1: The graph on left is the labeled directed graph constructed from Tab. 1. We note that the graph is not strongly connected since q does not have any incoming edge. The graph on right is the strongly connected component of the graph on left.  P r Xt+1  2  Definition 5 (Observation and Transition Matrices). Given a e is the Q PFSA (; Q; ;  e ), the observation matrix   matrix with the (q;  )-entry given by  e (q;  ), and the transition matrix  is the Q Q matrix with the (q; q 0 )-entry  (q; q 0 ), given by X  (q; q 0 ) =  e (q;  ):  0(:4)  0(:4)  by  the graph of the PFSA and, unless stated otherwise, we assume it to be strongly connected[1], which means for any pair q; q 0 Q, there is a sequence 1 2 k , such that  (qi 1 ; i ) = qi for i = 1; 2; : : : ; k with q0 = q and qk = q 0 . To generate a sequence of symbols, assuming G’s current state is q , it then outputs symbol  with probability  e (q;  ), and moves to state  (q;  ). We see that  is partial because  (q;  ) is undefined when  e (q;  ) = 0.  e are stochastic, It is straightforward to verify that both  and  i.e. nonnegative with rows of sum 1. Remark 2. We borrow the terms observation matrix and transition matrix from the study of HMM [28]. However, we need to point out here that our model differs from the HMM in that, in HMM, the transition from the current state to the next one is independent of the symbol generated by the current state, while in PFSA, the current state and symbol generated together determine the next state the PFSA will be in as specified by the transition map  . The interested reader might have noticed that we haven’t specified the initial distribution on states for the process to start with. The reason is that, unless specified otherwise, we assume the initial distribution to be the stationary distribution[15] of . We denote the stationary distribution of G by pG , or by p if G is understood.  Theorem 1 (PFSA Generates Stationary Ergodic Processes). Stochastic process generated by a PFSA G with distribution on states initialized with pG is stationary and ergodic. proof omitted. Example 1 shows that we may derive a PFSA from a stationary ergodic process, and Thm. 1 shows that the process generated by the PFSA thus obtained is also stationary and ergodic. This motivates us to seek a characterization for stochastic processes that gives rise to a PFSA. Since the process in Example 1 is an order-1 Markov process, which is the simplest non-i.i.d. process, it is legitimate to ask whether a process has to be Markov to have a PFSA generator. This desired characterization may be obtained from analyzing the notion of causal states, which we investigate in the next section. Remark 3. Table 2 compares the three generative models of stochastic processes mentioned above: Markov chain(MC), PFSA, and hidden Markov model(HMM). We note that a Markov chain produces a sequence of states, while sequences produced by PFSA and hidden Markov model take values in their respect output alphabets. We can also see that HMM can be considered as an extension to MC by adding an output alphabet and observation probabilities (in blue in the following example). Quite clealrly, PFSAs are not directly comparable to either MC or HMM.  3  Stochastic Processes with PFSA Generator  3.1  Persistent Causal States  Definition 6 (Persistent and Transient Causal States). Let Q be the set of causal states of a stationary ergodic process. For every q Q and d N, let pd (q ) = d [x] = q , i.e. the probability of length-d sequences who are equivalent to q . A causal state q is  2  2  f  g  4 TABLE 2: Comparing Markov chain, PFSA, and hidden Markov model. Model  Defining variables  MC  Set of states; Transition probabilities.  Example : :  6  q1 :  PFSA  HMM  Set of states; Output alphabet; Transition function; Observation probabilities.  Set of states; Output alphabet; Transition probabilities; Observation probabilities.  6  q0  :  4  4  1(:6) 0(:6)  q0  0(:4)  q1  1(:4)  : :  6  6  q0  :  q1  3  :  0  7  :  4 :  :  8  :  4  2  1  persistent if lim inf d!1 pd (q ) > 0, and transient if otherwise. We denote the subset of all persistent causal states by Q+ . Remark 4. Here we borrow the term transient state from the study of Markov chains, but we should note that the two concepts are not identical. For an introduction to Markov chain, we refer the interested reader to [13]. A Markov chain never revisits transient states as soon it hits a recurrent state. However, although a transient causal state could be like a transient state of a Markov chain, as the q in Example 1, it could also be revisited for infinitely many times, as a recurrent state of a Markov chain does. The reason we borrow the term is because the concepts are also similar in an imporatnt aspect: The probability a Markov chain being in a transient state diminishes as time increases, and a transient causal states also has lim inf d!1 pd (q ) = 0. Since transient causal states can recur, we coin the name persistent causal states for the counterpart of transient causal states instead of borrowing the recurrent state also from Markov chain. P For any pair q; q 0 Q, let q;q0 = f:q=q0 g q ( ), where the expression q = q 0 is a shorthand for [x ] = q 0 for all [x] = q. The following proposition shows that q;q0 captures the flow of probability over causal states as sequence length increases. P Proposition 1. We have pd (q 0 ) = q2Q q;q0 pd 1 (q ) for each q0 Q and d N+ . Furthermore, there is no flow from a persistent state to a transient one, i.e. q;q0 = 0 for q Q+ and q 0 Q Q+ . proof omitted.  2  2  2  2  2 n  Theorem 2. Let Q+ be the set of persistent causal states of a stationary ergodic process P . Then, p(q ) = limd!1 pd (q ) exists for every q Q+ . Furthermore, if Q+ is finite and P p ( q ) = 1 , the process generated by the PFSA G = q 2Q+ + (; Q ; ; e) with  (q; ) = q and e(q; )+= q () is exactly P . In fact, we have pG q = p(q ) for q Q . proof omitted.  2  j  2  Example 2 (An Order-Two Markov Process). Now, let us consider an order-2 Markov process over alphabet  = 0; 1 , in which Xt+2 follows a Bernoulli distribution conditioned on the value of Xt Xt+1 . More specifically, denoting P r (Xt+2 = 0 Xt Xt+1 = ij ) by pij for i; j 0; 1 , we have  f g  j  2f g  TABLE 3: Causality table of an order-2 Markov process with causal states Q = q ; q0 ; q1 ; q00 ; q01 ; q10 ; q11 . The reader can see that the causal states q00 , q01 , q10 , and q11 are named after the last two symbols of the corresponding sequences, which is a demonstration of the order-2 Markovity of the process, i.e. the distribution of future events is determined completely by length-2 immediate history.  f  g  x   P r(x)  :: :  :: :  0 1 00 01 10 11 000 001 010 011 100 101 110 111  1 1=2 1=2 4=15 7=30 7=30 4=15 2=25 14=75 7=150 14=75 14=75 7=150 14=75 2=25  x  (1=2; 1=2) (8=15; 7=15) (7=15; 8=15) (3=10; 7=10) (1=5; 4=5) (4=5; 1=5) (7=10; 3=10) (3=10; 7=10) (1=5; 4=5) (4=5; 1=5) (7=10; 3=10) 3=10; 7=10 (1=5; 4=5) (4=5; 1=5) (7=10; 3=10)  :: :  causal state  q q0 q1 q00 q01 q10 q11 q00 q01 q10 q11 q00 q01 q10 q11 :: :  p00 = :3, p01 = :2, p10 = :8, p11 = :7. Together with the specification Pr (X2 = 0 X1 = 0) = 8=15, P r (X2 = 0 X1 = 1) = 7=15, and P r (X1 = 0) = :5, we can check that the process is stationary and ergodic. We list the causal states of sequence up to length 3 in Tab. 3. Since it is impossible to write down the infinite-dimensional distribution x , we only show x in Tab. 3, but we can check that x = y if and only if x = y for this process. Since q , q0 , q1 only show up once, while q00 , q01 , q10 , q11 appear repeatedly, we have Q+ = q00 ; q01 ; q10 ; q11 . With more detailed calculation, we can show that p (q00 ) = 4=15, p (q01 ) = 7=30, p (q10 ) = 7=30, and p (q11 ) = 4=15, which sum up to 1. According to Thm. 2, we can construct a PFSA with state set Q+ that generates exactly the same process. We demonstrate the labeled directed graph constructed on Q in Fig. 2, and the PFSA is exactly the induced subgraph[24] on Q+ , which is also the unique strongly connected component of the graph. We can show that the stationary distribution of the PFSA is exactly (4=15; 7=30; 7=30; 4=15).  j  j  f  g  Example 3 (A PFSA on Three States). In this example, we analyze the stochastic process generated by the PFSA on the left of Fig. 3. We nickname the PFSA as T . We show that Q of this process is infinite, while Q+ is of size 3. We first notice that, no matter what state the PFSA resides, the sequence 11, and hence any sequence ending in 11, will take it to state r, which generates symbol 0 with probability :8, and 1 with probability :2. We also note that, whenever there are two consecutive 1s in a given sequence in ? , we know for sure the state the PFSA resides. For example, sequence 110 will take the PFSA to s, and 1101, to q . On the left of Fig. 4, we show the probabilities of causal states [11], [110], [1101], and the sum of probabilities of all other causal states for sequence length d = 0; : : : ; 25. We see from the bar plots that the sum of concentrations of [11], [110], and [1101] approaches 1 as d increases. We also point out that, with all numbers rounded up to three decimal places, p25 ([11]) = 0:182, p25 ([110]) = 0:474, and p25 ([1101]) = 0:279, while the stationary distributions of the states r, s, and q are 0:190; 0:506; 0:304, respectively. However, we also note that Q of the process is actually infinite by observing the fact that [0d ] , where  d means  repeated d  5  q0  1(7=15)  0(8=15)  0(:3)    q01  1(:7)  q00  1(:2)  0(:8)  0(:2)  1(:3) 1(:5)  q11  0(:7)  q10  1(8=15)  1(1=3) q  r  0(1=3)  q0  q1  0(2=3)  1(2=3)  s  0(:8)  Accumulation Causal States  We see from Example 4 that we can have a PFSA that generates stochastic process with empty Q+ . In such a case, can we still get the PFSA structure back by studying the the set of causal states of the process? The answer is yes. Definition 7 (Epsilon-Ball of Measure). Denote the collection all measures on (! ; ) by  , and let   , the ""-ball of order d centered at  is defined by 8 9 < = X  F  0(:5)  1(:2)  Fig. 4: The bar plot on left shows the probabilities of [11], [110], and [1101] for sequence length d = 0; 1; : : : ; 25 as discussed in Example 3. The curves on right show the contour of pd (qn ) against n for sequence length d = 10; 20; : : : ; 150 as discussed in Example 4. 3.2  q1  Fig. 2: Labeled directed graph obtained from Tab. 3. The edges from transient causal states are dotted, while those from persistent states are solid.  1(:3)  0(:7)  Fig. 3: The PFSA T on left generates a stochastic process with infinite Q and Q+ of size 3. The PFSA S on right generates a + stochastic process with empty Q+ but Q (defined in Sec. 3.2) of size 2. times, are all distinctive. We note that the process generated by this PFSA is not Markov, as implied by the infinity of Q. However, the fact that there are only three persistent causal states whose sum of probabilities approaches 1 allows it to have a PFSA generator. Example 4 (A Stochastic Process with Empty Q+ ). In this example, we analyze the stochastic process generated by the PFSA on the left of Fig. 3. We nickname the PFSA as S . We show that Q of this process is infinite while Q+ is empty. Without run into details of the computation, we point out the fact that causal states of this process are also uniquely characterized by their symbolic derivatives, and the set (q ) q Q is in one-toone correspondence with Z. More specifically, we have   Q = qn qn :5n+1 + 1; :5n + :5 ; n Z ; (2)  f  /  j 2 g  2  / means being proportional to, and 2 :5n+1 + 1 ;  1 :5n 1 + 1 ; qn ;qn+1 = = q ;q n n +1 3 :5n + 1 3 :5n + 1 with qn ;qn+1 + qn ;q n+1 = 1, for all n 2 Z. We demonstrate on the left of Fig. 4 the contour of pd (qn ) against n for sequence length d = 10; 20; : : : ; 150. It takes some more work to show rigorously, but we can speculate that, for any fixed n 2 Z, pd (qn ) approaches 0 as d approaches infinity, as the curves flatten out  ( ) = : 0 2 M  Bd;""   M  2  x  2M  d  j 0 (x! )  ( ! )j < "" ; :   x  In another words, Bd;"" is the collection of all measures that is no more than "" away from  with respect to total variation distance over d . Definition 8 (Accumulation Causal States). Let Q be the set of causal states of a stochastic process P , a measure   is an accumulation causal state of P if  l  2M  ( ) =  [x] 2 Bd;"" ( ) satisfies pd;"" ( ) = lim inf l!1 pl;d;"" ( ) > 0 for all d 2 N+ and "" > 0. That is, a measure  is accumulation causal state if, no matter how large d is and how small "" is, the sum of probabilities of length-l sequences falling in Bd;"" ( ) does not pl;d;""   vanish as l approaches infinity. The collection of accumulation causal states is denoted by Q. Since pd;"" ( ) is monotonically decreasing as d and "" 0, p( ) = limd!1 lim""!0 pd;"" ( ) is well-defined. A measure  with p( ) > 0 is called an atomic accumulation causal state, and the collection of all atomic accumulation causal states is + denoted by Q .  !1  !  2M  Definition 9 (Translation Measure). Let   , the translation of  by  for  ( ! ) > 0, denoted by  , is the extension to of the premeasure on the semiring given by  where  with increasing d.  n as in Eq. (2)  sequence length  1(:8)  0(7=15)  1(:5)  frequency  q  frequency  0(:5)  +  F  S  (x! )  (x! ) = :  ( ! )  Proposition 2. Q is closed under translation. proof omitted. Theorem 3. Let P be a stationary ergodic stochastic process P + with finite Q and  2Q+ p( ) = 1. Then the process generated   + by the PFSA G = ; Q ; ;  e with  (;  ) =  and  e (;  ) =  ( ! ) is exactly P . In fact, we have pG  = p( ).  j  6  cumulative density  Definition 10 (Observation induced distributions). Let x = 1 : : : n be a sequence observed, the distribution over states induced by x is defined inductively by X 1 pG (1 : : : i ) q =  e (q 0 ;  )pG (1 : : : i 1 ) q0 ;  j  q (0)  Fig. 5: Cumulative probability density function of q (0). proof omitted.  +  Example 5 (Example 4 Revisited). We demonstrate that Q of the process in Example 4 has two elements, again by observation. We plot the cumulative probability density functions of q (0) for each sequence length d = 10; 20; 30; 40 in Fig. 5. More specifically,  for each fixed d, the x-coordinates of the dots are in d = [x] (0) x d , whilethe y -coordinate of a dot with x-coordinate h0 d equals d [x] (0) h0 . We can see clearly that, the cumulative function converges to a step function with steps at 1=3 and 2=3 as d increases. The fact implies that +  Q satisfies that  (0! ) is either 1=3 or 2=3. We see from + (2) that the two measures in Q are exactly q 1 and q1 . Fig. 5 also implies that that p (q 1 ) = 1=3 and p (q1 ) = 2=3, which is exactly the stationary distribution on the state stet of the PFSA.  2  2    2  4  Inference algorithm of PFSA  From the discussion in Sec. 3, we see that a stochastic process enjoys an PFSA generator if either finitely many causal states get all the probability in the limit, as described in Sec. 3.1, or there exists finitely many measures in  whose arbitrarily small neighborhoods are going to be populated by all the causal states in the limit, as described in Sec. 3.2. The implications of these observation goes beyond a theoretical development of PFSA, but also guide us through the designing of inference algorithms of the model. In fact, a valid heuristic of the inference algorithm of PFSA would be to apply any clustering algorithm to the set of causal states corresponding to sequences up to a certain length, and use the center of the clusters to serve as estimates to the states. However, this  primitive heuristic has a drawback since the cluster structure of [x] x d may not be clear enough to facilitate a clustering algorithm. In order to get better estimates of the states, we need something to fine tune our view into the set of causal states, which we call ""-synchronizing sequence[2].  M  j 2  4.1  Before introducing ""-synchronizing sequence, we first introduce the concept of observation induced distributions over the state set. Let G be a PFSA, we know that the initial distribution over states is exactly the stationary distribution pG . Let us assume that the first symbol generated by G is  , denote by pG ( ) the distribution over states after G producing  , we have  where Z  =  is the normalizer.  = Z1  X  X  fq0 j(q0 ;)=qg X  2 f j(  q Q q 0  q 0 ;  )=qg   e q 0 ;  pG q0 ;  (  ) j  =  X  X  2 f j(  q Q q 0  q 0 ;i  )=qg   e q 0 ;  pG 1 : : : i  (  ) (  ) j  1 ) jq0 ;  for i = 1; : : : ; n, with the base case pG () = pG . Definition 11 (""-synchronizing sequence). Let G be a strongly connected PFSA on state set Q and over alphabet . A sequence x ? is called an ""-synchronizing sequence for some "" > 0 if there exists a q Q such that pG (x) eq 1 < "", where eq is the base probability vector with the entry indexed by q equalling 1.  2  2  k  k  Without running into details, we want to point out here that the concept of ""-synchronizing sequence is the center piece for the theoretic development of PFSA, including the formula for entropy rate and KL divergence, which we are going to discuss in Sec. 5. Although we omit the proofs in this paper and hence wouldn’t see ""-synchronizing sequences in real play of proving theorems, we will soon see the its role in PFSA inference algorithm, and the reason the ""-synchronizing sequences are important to inference is  because that [x"" x] x d tends to have a much clearer cluster structure than [x] x d for an ""-synchronizing sequence x"" .  j 2 j 2  4.2  GenESeSs Algorithm  We give a brief review to the algorithm called GenESeSs proposed in [3] in this section. By a sub-sequence, we mean a consecutive sub-sequence.  2  Definition 12 (Empirical Symbolic Derivative). Let x ? , the empirical symbolic derivative ^xy of a sub-sequence y of x is given by ^xy () = number of sub-sequence y in x ;  number of sub-sequence y in x for all   2 .  Our inference algorithm is called GenESeSs for Generator Extraction Using Self-similar Semantics, With the input of a long enough observed sequence x, GenESeSs takes the following three steps to infer a PFSA: Step one: Approximate ""-synchronizing sequence: Calculate   1 x x ^ logjj ; "" = y y is a sub-sequence of x with y  j j  ""  Then, select a sequence x"" with ^xx"" being a vertex of the convex hull of ""x . Step Two: Identify transition structure: For each state q , we associate a sequence identifier xq x"" ? , and a probability distribution dq on . We extend the structure recursively: Initialize the state set as Q = q0 , find xq0 and set dq0 = ^xxq ; Calculate 0 the empirical symbolic derivative of xq  for each state q Q and  D  2  f g     e q 0 ;  pG q0 ;  (  Z  j  fq0 j(q0 ;i )=qg  D  Epsilon-synchronizing Sequences  pG ( )jq  where  Z  2 . If )=  ^xxq    dq 0  1   "" for some  q0  2  2 Q, then define   q;  q 0 . However, if no such q 0 exists in Q, add a new state 0 q to Q, and define xq 0 xq  , and dq 0 x xq  . The process terminates when no more states can be added to Q. The inferred  (  =  =^  PFSA is the strongly conn"
"RLBench: The Robot Learning Benchmark & Learning Environment  arXiv:1909.12271v1 [cs.RO] 26 Sep 2019  Stephen James1 , Zicong Ma2 , David Rovick Arrojo2 , Andrew J. Davison1  I. INTRODUCTION Robot manipulation systems broadly fall somewhere on a spectrum ranging from traditional, modular methods, that include object recognition, state estimation, and planning, to fully end-to-end approaches that leverage deep learning and large-scale data to learn a mapping from input observations directly to motor actions, with the intuition that the ‘traditional’ modules are embedded in the weights of a deep neural network. Driven by the successful combination of large-scale data [1] and deep learning algorithms in the field of computer vision [2], there is now a large body of work looking at increasing the capabilities of robotic agents through the use of reinforcement learning [3], [4], meta-learning [5], [6], [7], multi-task learning [8], [9], etc. However, there is currently no standard in place for comparing manipulation methods in these respective areas. Although there exist benchmarks such as OpenAI Gym [10] and DeepMind Control Suite [11] for evaluating continuous-control reinforcement learning algorithms, their focus is not on real-world problems, and it is often the case that algorithms in these toy-benchmarks do not scale to more complex, real-world tasks. Few-shot learning methods for robotics also suffer from a lack of well defined tasks; for example, in Finn et al. [5] and James et al. 1 Dyson  Robotics Lab, Imperial College London Imperial College London 1 https://sites.google.com/view/rlbench 2 UROP,  [6] there is a very narrow distribution of tasks, where the task of “placing a peach into a red bowl” would be considered a different task to “placing an apple in to a green bowl”. Despite the increase in these data-driven approaches, it is not clear where the ideal location on this ‘learning’ spectrum lies for complex robotics tasks that we may one day want robots performing in our homes. Given all of these problems, there seems to be a need for a benchmark that evaluates not only the diverse range of robot learning fields that are now emerging, but also a range of visually-guided manipulation approaches from both sides of the spectrum. This motivates the need for a one-size-fits-all benchmark that allows the capability to utilise large-scale data, whilst also allowing classical systems to be compared. To that end, we present RLBench, which is an ambitious large-scale benchmark and learning environment designed to facilitate research in a number of both classical and deep-learning based robot manipulation areas. RLBench is deliberately highly challenging and forward looking. The benchmark includes 100 completely unique, hand-designed tasks ranging in difficulty (shown in Figure 1), which share a common Franka Emika Panda robot arm, featuring a range of sensor modalities, including joint angles, velocities and forces, an eye-in-hand camera and an over-the-shoulder stereo camera setup. Each of the 100 tasks comes with a number of textual descriptions and an infinite set of demonstrations made possible through our task building tools that use waypointbased motion planning. In this paper, we discuss a host of research areas that would benefit from this benchmark, including, but not restricted to, reinforcement learning, imitation learning, fewshot learning, multi-task learning, and geometric based methods, such as SLAM. In addition to the benchmark, we also contribute an open-source set of tools that will allow rapid development of new tasks (through the use of PyRep [12]) in order to improve the size and scope of the benchmark over time. To summarise, RLBench has the following 3 key aims: • Provide a benchmark and learning environment for both ‘robot learning’ and ‘traditional’ methods. • Provide the a large-scale few-shot challenge, where given M training tasks and N unseen tasks, a system must take K different demonstrations of each of the N unseen tasks, and then be able to perform these tasks in new configurations. • Provide a set of tools to allow easy task creation. II. R ELATED W ORK We review existing datasets, benchmarks, and learning environments that could be considered similar to ours in  Fig. 1: RLBench is a large-scale benchmark consisting of 100 completely unique, hand-designed tasks. In this figure we show a sample of 24 tasks that feature in the benchmark. Example tasks include stacking a set of 6 colored blocks in a pyramid (top left), inserting a shape onto a peg (top right), finish setting up a checkers board (bottom left), and watering a plant (bottom right). To get a better understanding of the variety of tasks, please watch the video.  an effort to further motivate RLBench. Firstly we cover reinforcement learning benchmarks, followed by benchmarks designed specifically for manipulation. a) Reinforcement Learning: Largely as a consequence of the seminal work that saw an algorithm learn to play a range of Atari 2600 video games to superhuman level directly from image pixels [13], deep reinforcement learning (DRL) has increasingly become prevalent in the literature, leading to a number of recent further success in the games of Go [14], Chess [15], StarCraft [16], and Dota [17]. With the success of these approaches, there has been a surge in developing DRL algorithms to solve continuous control environments [18], [19], [20], [21], [22]. These learned (continuous control) agents are usually tested on benchmarks such as OpenAI Gym [10] or the DeepMind Control Suite [11]. However, apart from a small number of robotic tasks in OpenAI Gym, these benchmarks feature only toy tasks that often do not resemble real-world problems that robots will need to overcome. To combat this, many projects create their own manipulation tasks to evaluate their approach, making comparisons difficult. As a direct consequence of this, these created tasks can often succumb to unintentionally introducing another hyperparameter into the method in the form of the task design itself. For example, a method could fail on a more challenging task, and so results would only be presented for a simpler set of tasks. This is something a standard benchmark of tasks could alleviate. (We should mention the very recently announced Meta-World project [23], a multi-task benchmark for meta-learning research in manipulation, though full documentation describing the aims of that project is not available at the time of writing.) b) Manipulation: Most related work in benchmarking robot manipulation algorithms often concentrates on solving only one of the manipulation sub-problems, focusing on either perception, grasping, or planning. But first, we look  at benchmarks that evaluate the system as a whole. The Amazon Robotics Challenge (ARC) [24] was an attempt to create a benchmark for robotic picking and stowing. Although it was a successful challenge that drew many conclusions, such as the usefulness of a dual gripper and suction cup end-effector [25], it was difficult to reproduce in a lab setup. The ACRV Picking Benchmark [26] aimed to solve this by creating a similar, but reproducible setup to the ARC. The issue with picking and stowing is that it is but one of many possible tasks; RLBench on the other hand comes with 100 unique tasks, many of which involve some aspect of picking and placing. Similarly to ARC, the RoboCup@Home competition [27] is run annually, but has a greater range of tasks that must be completed. However, given that no largescale data is given beforehand, this makes reinforcement learning and other end-to-end approaches difficult to apply in the competition. RLBench is a platform that can unify both old and new methods and compare them on an even playing field. For evaluating imitation learning systems in particular, RoboTurk [28] was a recent attempt to leverage crowd sourcing to obtain data for tasks, but because of this the system has only three tasks. Whilst RoboTurk is entirely in simulation, Simitate [29] on the other hand is a hybrid approach, where real world observations (RGB-D camera calibrated against a motion capturing system) are combined with a simulated environment for benchmarking. In contrast to RoboTurk, we do not crowd source our demonstrations, but instead rely on an infinite supply of generated demonstrations collected via motion planners. Although Simitate offers the benefit of being partially a real-world dataset, the addition of new tasks requires time-consuming calibration and motion capturing; our system on the other hand sacrifices the real-world aspect, but in exchange we receive the ability generate a diverse range of tasks in a scalable way.  Fig. 2: The V-REP scene consists of a Franka Panda affixed to a wooden table, surrounded by 3 directional lights. Observations include rgb, depth, and segmentation masks from an over-the-shoulder stereo camera and a eye-in-hand monocular camera, along with robot proprioceptive data, which includes joint angles, velocities, and torques, and the gripper pose. The arm can be easily swapped out for another arm if required.  Moving on from whole-system benchmarks, there are a host of benchmarks that focus on sub-problems, for example perception datasets, from both the computer vision community (such as ILSVRC [1], COCO [30], Pascal-VOC [31], etc), and the robotics community (such as BigBIRD [32], YCB-Video [33], etc). For grasping, both OpenGrasp [34] and VisGraB [35] are popular simulation-based benchmarks, whilst the YCB dataset [36] focuses on real-world objects. In comparison to these, RLBench allows robotic systems to be evaluated on the complete robotic pipeline, rather than limited to sub-problems such as object detection, state estimation, grasp selection, and planning. III. B ENCHMARK P ROPERTIES When designing RLBench, we have prioritised several key properties: a) Diversity: Algorithms we develop should be general. In order to effectively learn inter-task relationships, a truly diverse range of tasks is needed to help avoid over-fitting. b) Reproducibility: Reproducibility is challenging in robotics as each lab has their own robotic setup. Moving to simulation solves this, but at the risk of developing solutions that may not run as well in the real-world. However, with the rise of deep-learning methods becoming more prominent in robotics, we believe it is important to find the potential and limits of these methods in a controlled, reproducible environment. c) Scale: Given the amount of data modern machine learning methods need, it is important to not only have a large collection of tasks, but also the ability to produce a large number of demonstrations from these tasks.  Fig. 3: A sample of the visual observations given from both the over-the-shoulder stereo and eye-in-hand monocular cameras, which supply rgb, depth, and mask images.  d) Extensibility: Following on from the previous point, we hope to continue to grow this repository of tasks. Therefore it is crucial that the task building system is as easy as possible to use. By leveraging the recently released robotics toolkit, PyRep [12], we are able to make a broad range of tasks in a short amount of time. e) Tiered Difficulty: Attempting to get robots to do a single task can be challenging let alone expecting them to do numerous tasks. We therefore wanted to have a range of tasks, including both easy tasks, such as reaching, which would be well suited to new and emerging methods, to more challenging, long-time-horizon tasks that can stress-test well known state-of-the-art algorithms in use today. f) Realism: Although we cannot claim full photorealism in our rendering system, or general realistic physics, we have put substantial effort into high quality components such as using a realistic robot model, graphics with lighting and shadows and a domain randomisation rendering option in order to maximise the potential for research on sim-to-real transfer. IV. RLB ENCH RLBench is an ambitious project which we hope to grow over many years. The benchmark and learning environment is built around a V-REP [37] and PyRep [12] interface. PyRep is a toolkit for robot learning research, built on top of V-REP that features a number of improvements, including speed, rendering, and flexible a API for robot control and scene manipulation. Using the combination of these two libraries, we have been able to build this ambitious benchmark, which we now describe in greater detail.  Fig. 4: An example showing the distinction between task, variation, and episode. In this case, the ‘stack blocks’ task has V variations, each with E episodes. Each variation comes with a list of textual descriptions that describes the objective. Across variations, usually target objects or colours are changed, whereas across episodes positions are changed.  A. Scene The V-REP scene, shown in Figure 2, remains constant across all tasks and contains the Franka Emika Panda 7 DoF arm affixed to a wooden table, surrounded by 3 directional lights. As shown in Figure 3, visual observations can be perceived from a stereo camera, and a monocular wrist camera, which supply rgb, depth, and segmentation mask data on each frame. In addition to visual observations, robot proprioceptive data can be retrieved, which includes joint angles, velocities, and torques, along with the end-effector pose. Tasks are loaded into the scene and placed at the centre of the workspace. Every task starts with the same assumption that no objects are held, therefore, unlike many works in the literature, tasks that involve tools will first need to grasp the object appropriately in order to accomplish the task. Although this makes the environments considerably harder to complete, we believe it is an important assumption to make given that household robots will one day work under such conditions. B. Tasks, Variations & Episodes RLBench employs 3 keys terms: Task, Variation, and Episode. Each task consists of one or more variations, and from each variation, an infinite number of episodes can be drawn. Each variation of a task comes with a list of textual descriptions that verbally summarise this variation of the task, which could prove useful for human robot interaction (HRI) and natural language processing (NLP) research. A summary of this can be seen in Figure 4. Formally, we define an episode trajectory τ to consist of a series of observations o and actions a: τ = [(o1 , a1 ), . . . , (oT , aT )]. These episodes are sampled from a variation τ ∼ ν. Finally, we define each task to be a set of variations, T = {ν1 , · · · , νN }. We now motivate the need for the concept of a ‘variation’ with an example. It is naturally difficult to come up with a precise way to differentiate between tasks given their subjective nature. For example, one could argue that “pick up the apple” and “pick up the banana” are different tasks,  whilst one could also equally argue that they are the same “pick up the X” task. We therefore introduce the variation concept, which allows cases like the above to be grouped as very similar tasks. Moreover, given the way the task building tools are designed (discussed in Section IV-E), the variation concept allows a convenient way of getting as much from a task definition as possible, given that there is usually only a small amount of additional work needed to generate a large number of variations for a given task. C. Environment Users will interface with the benchmark and learning environment through the Environment class. The Environment is the entry point and can spawn child environments, called TaskEnvironment, for the tasks you are interested in solving. The environment API, which Figure 5 demonstrates, is modelled after a typical agent-environment reinforcement learning setup. Each task has a completely sparse reward of +1 which is given only on task completion. Users have a wide variety of action spaces at their disposal, which include absolute or delta joint velocities, absolute or delta joint positions, absolute or delta joint torque, absolute or delta end-effector velocities, and finally absolute or delta end-effector poses. D. Demonstrations RLBench, through the task building tool mentioned in Section IV-E, provides expert algorithm π ∗ for each different task and their corresponding variations, allowing for demonstration episodes to be generated The episodes produced via π ∗ come from using the Open Motion Planning Library [38]. E. Task Builder Two common simulation environments in the literature today are Bullet [39] and MuJoCo [40]. However, given that these are physics engines rather than robotics frameworks, it can often be cumbersome to build rich environments and integrate standard robotics tooling such as inverse and  1 2 3  from rlbench.environment import Environment from rlbench.action_modes import ActionMode from rlbench.tasks import ReachTarget  4 5  1 2  3  DATASET = ’path/to/demo/dataset’  4  env = Environment( DATASET, ActionMode.ABS_JOINT_VELOCITY) env.launch()  5  6 7 8 9 10 11 12  15 16 17 18 19 20 21 22 23 24 25  9 10 11  agent = Agent() agent.ingest(demos)  12 13 14  training_steps = 100 episode_length = 100 obs = None for i in range(training_steps): if i % episode_length == 0: descriptions, obs = task.reset() action = agent.act(obs) obs, reward, terminate = task.step(action) env.shutdown()  Fig. 5: Example usage of the RLBench Environment for training a reinforcement learning agent. When using demonstrations, users can either point to a set of saved demonstrations (as shown here), or alternatively generate demonstrations on the fly.  forward kinematics, user interfaces, motion libraries, and path planners. Given the scale of RLBench, we needed a tool for designing tasks as easily as possible. The task building tool is the interface for users who wish to create new tasks to be added to the RLBench task repository. Each task has 2 associated files: a V-REP model file (.ttm), which holds all of the scene information and demo waypoints, and a python (.py) file, which is responsible for wiring the scene objects to the RLBench backend, applying variations, defining success criteria, and adding other more complex task behaviours. Figure 6 shows an example of how simple many tasks files can be. In order to use the task creator, users must understand how tasks are initialised and placed in the scene. When a user asks for a new task from RLBench, the task is initialised by calling init task(), and is only called once. Following that, init variation(int i) is called at the beginning of each variation, and gets passed the variation number, which should be less than or equal to the number of variations for that task (which can be obtained by calling variation count()). This function returns a list of strings which provide descriptions that could be associated with this variation of the task; an analysis of the frequency of words in these descriptions can be seen in top of Figure 7. Finally, init episode() is called each time a new episode (of the same variation) is requested. Once a task has been created, we provide a task validation tool, that attempts to collect a number of demonstrations of the designed task in order to ensure that the path planning aspect of the task only fails a small number of times. Once the validator passes, the user will be free to perform a  class TakeLidOffSaucepan(Task):  7 8  task = env.sample_task() demos = task.get_demos(2)  13 14  6  from rlbench.backend.task import Task from rlbench.backend.conditions import DetectedCondition, GraspedCondition from pyrep.objects.shape import Shape from pyrep.objects.proximity_sensor import ProximitySensor  15 16  def init_task(self): lid = Shape(’saucepan_lid’) success_detector = ProximitySensor(’success’) self.register_graspable_objects([lid]) cond_set = [ GraspedCondition(self.robot.gripper, lid), DetectedCondition(lid, success_detector) ] self.register_success_conditions([cond_set])  17 18 19  def init_episode(self, index): return [’take lid off the saucepan’]  20 21 22  def variation_count(self): return 1  Fig. 6: An example of a task python file. When using the task building tool, users are able to simultaneously edit the V-REP scene whilst also changing the various behaviour of a task. In this example, the task is to take a lid off of a saucepan. By interfacing with the scene using PyRep, we register that the episode should terminate and be considered a success only if the saucepan lid is detected by a proximity sensor and that the lid is being held. The backend handles the randomisation of the position of the task at the beginning of each episode.  GitHub pull request in order to contribute to the growing task repository. V. T HE RLB ENCH F EW-S HOT C HALLENGE (v 1.0) A big gap in the literature today is a means to evaluate and compare few-shot learning methods for robotics. We place particular emphasis on the few-shot regime, because much like humans, robots should have the ability to leverage knowledge from previously learned tasks in order to learn new ones quickly in new and unfamiliar environments. Despite this, most approaches in manipulation have focused on learning a single task, with a limited notion of generalisation, and no way of leveraging the knowledge to learn other tasks more efficiently. The few pieces of work that perform few-shot learning in robotics [5], [6], [7] focused on a very narrow definition of task and often treat a variation of the same task as another task; for example, placing a peach into a red bowl would be considered a different task to placing an apple into a green bowl. In order to develop truly general algorithms, we feel that it is important to have a diverse range of tasks to train and test on. To that end, we propose the following challenge: Given N unseen tasks, provide the system with K different demonstrations of each of the N tasks, and then evaluate the systems ability to perform these tasks in new configurations. Specifically, we suggest the following procedure:  Fig. 7: Top shows the frequency of words in the variation descriptions with function words removed, leaving only content words. Bottom shows the average length of 5 demonstrations from a sample of 75 tasks (taken from the first variation). The tasks lengths vary from 100 to 1000 timesteps. Longer tasks usually involve many composed sets of actions, for example, the ‘empty dishwasher’ task involves opening the washer door, sliding out the tray, grasping a plate, and then lifting the plate out of the tray. These long-horizon tasks can facilitate interesting research in reinforcement learning in robotic tasks.  •  •  •  Of the 100 unique tasks, 10% of the tasks have been selected for the test set (meta-test) which span a range of difficulties, while the rest are chosen for training (metatrain). These train-test splits will be made available on the benchmark’s webpage. The training tasks can be used in any way desired by the user. RLBench supplies a large number of pre-generated demos for each task that can be downloaded, although there is also the option to generate demos on the fly (or for users to create their own). During test time, the system is given K demonstrations of the unseen task (K-shot), and then success should be reported on new episodes of that same task. The only information available to the system should be the number of demos N and their corresponding observations. There must be no prior knowledge of the unseen tasks given to the system that are not included in the training tasks. Users report 1-shot, 5-shot, and 20-shot results for their method.  We purposefully call this challenge v 1.0 as we expect the number of tasks to grow considerably over the years; as this happens, we will create newer versions that span a broader range of tasks; therefore, we hope this versioning will ensure results remain meaningful and reproducible as the benchmark grows. State-of-the-art few-shot learning methods such as recurrent methods [41], [42], [43], metric learning methods [44], [45], and gradient based methods [46], [47] have not been tested on such a grand scale, and we look forward to seeing how they perform on this benchmark.  VI. OTHER A PPLICATIONS & C HALLENGES Further to the few-shot learning challenge highlighted in Section V, we briefly overview other areas of research that could benefit from RLBench. a) Reinforcement Learning: There is a large body of work in continuous control reinforcement learning that evaluate their algorithms on benchmarks such as OpenAI Gym [10] or DeepMind Control Suite [11]. Unlike these benchmarks, RLBench has been tailored for visually-guided manipulation, which makes this an ideal platform for evaluating current and future reinforcement learning algorithms on real-world based tasks. Moreover, given the large number of demonstrations provided, it opens up the space to accelerate and facilitate research in bootstrapping reinforcement learning policies with demonstrations in order to reduce sample complexity. In addition, with the provided eye-in-hand camera observations, we open research in partial observability or incremental estimation for continuous control tasks. b) Imitation Learning: Almost all imitation learning work design their own tasks for evaluating their method, making reproducibility difficult. A set number of demonstrations are shipped with RLBench, but there is also the option in the framework to generate demonstrations on-thefly, meaning that you cam generate an infinite amount for your imitation learning algorithm. c) Sim-to-Real Transfer: Recently there has been a large amount of work in learning control policies in simulation and then transferring these to the real world [48], [49], [50], [51], [52], [53]. The simulated Franka Panda within  RLBench can be easily swapped out, with one line of code, for another arm that researchers may have in their lab; this means that sim-to-real methods could be compared more easily on a standard set of tasks. Moreover, given the taskbuilding tool and demonstration generation that RLbench has to offer, new tasks can easily be designed to demonstrate particular features in novel sim-to-real methods. d) Multi-task Learning: In contrast to few-shot learning, multi-task learning concerns itself with learning several tasks simultaneously without particularly being expected to generalise to radically different tasks at test time. In this setup, all tasks from both meta-training and meta-testing can be used during training, and then during testing, the system must be able to generalise to unseen examples of those tasks. Given the difficulty of the challenge laid out in Section V, tackling the multi-task problem could provide valuable insights to increasing performance in the few-shot domain. e) SLAM: Simultaneous Localisation and Mapping (SLAM) is concerned with constructing a map of an unknown environment while simultaneously keeping track of an agent’s location within it. Traditionally SLAM has been limited to navigation, virtual reality and augmented reality domains; but ultimately we can envision SLAM systems playing a key role in robots interacting with the world, i.e. a focus on more task-based SLAM. However, if we would like a manipulation system to make use of a SLAM map, it is not currently clear what the best way to represent this map is: whether it be sparse [54], [55], dense [56], [57], or semidense [58]. Moreover, it is not clear what level accuracy the map would need in order to achieve a desired task. RLBench could facilitate research in unifying SLAM and manipulation more tightly. VII. S UMMARY AND F UTURE W ORK We have presented RLBench, an attempt to accelerate research in robotic manipulation that can be used in a broad range of robotic related research. We have posed the few-shot learning challenge for manipulation, and have highlighted a number of research areas that could benefit from this large scale benchmark and learning environment. Given the scale of this project, we envision that there may be teething problems as people begin using the platform, and so we aim to maintain and continuously improve the benchmark during launch. Further to that, we hope, along with the help of the community, to continuously expand the tasks available for both training and evaluation. We hope RLBench will become a key resource for a broad range of robot manipulation related research, and look forward to seeing what the community achieves with this diverse range of tasks. ACKNOWLEDGMENTS We thank Juxi Leitner, Ankur Handa and Eugene Valassakis for insightful feedback on an early draft of this paper. Research presented here has been supported by Dyson Technology Ltd.  R EFERENCES [1] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large Scale Visual Recognition Challenge,” International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211–252, 2015. [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” in Advances in neural information processing systems, 2012, pp. 1097–1105. [3] S. James and E. Johns, “3d simulation for robot arm control with deep q-learning,” arXiv preprint arXiv:1609.03759, 2016. [4] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke et al., “Qtopt: Scalable deep reinforcement learning for vision-based robotic manipulation,” arXiv preprint arXiv:1806.10293, 2018. [5] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine, “One-shot visual imitation learning via meta-learning,” Conference on Robot Learning, 2017. [6] S. James, M. Bloesch, and A. J. Davison, “Task-embedded control networks for few-shot imitation learning,” Conference on Robot Learning, 2018. [7] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine, “One-shot imitation from observing humans via domain-adaptive meta-learning,” Robotics: Science and Systems, 2018. [8] C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine, “Learning modular neural network policies for multi-task and multi-robot transfer,” in 2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017, pp. 2169–2176. [9] K. Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. Riedmiller, “Learning an embedding space for transferable robot skills,” International Conference on Learning Representations, 2018. [10] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, “Openai gym,” arXiv preprint arXiv:1606.01540, 2016. [11] Y. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq et al., “Deepmind control suite,” arXiv preprint arXiv:1801.00690, 2018. [12] S. James, M. Freese, and A. J. Davison, “Pyrep: Bringing v-rep to deep robot learning,” arXiv preprint arXiv:1906.11176, 2019. [13] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski et al., “Human-level control through deep reinforcement learning,” Nature, vol. 518, no. 7540, p. 529, 2015. [14] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, p. 484, 2016. [15] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton et al., “Mastering the game of go without human knowledge,” Nature, vol. 550, no. 7676, p. 354, 2017. [16] DeepMind, “Alphastar: Mastering the real-time strategy game starcraft ii,” https://deepmind.com/blog/article/ alphastar-mastering-real-time-strategy-game-starcraft-ii, 2019. [17] OpenAI, “Openai five,” https://blog.openai.com/openai-five/, 2018. [18] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,” arXiv preprint arXiv:1509.02971, 2015. [19] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International conference on machine learning, 2015, pp. 1889–1897. [20] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv preprint arXiv:1707.06347, 2017. [21] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor,” arXiv preprint arXiv:1801.01290, 2018. [22] S. Fujimoto, H. van Hoof, and D. Meger, "
"Lower Bounds on Adversarial Robustness from Optimal Transport  arXiv:1909.12272v1 [cs.LG] 26 Sep 2019  Arjun Nitin Bhagoji ∗ Department of Electrical Engineering Princeton University abhagoji@princeton.edu  Daniel Cullina ∗,† Department of Electrical Engineering Pennsylvania State University cullina@psu.edu  Prateek Mittal Department of Electrical Engineering Princeton University pmittal@princeton.edu  1 Introduction Machine learning (ML) has become ubiquitous due to its impressive performance in a wide variety of domains such as image recognition [47, 70], natural language and speech processing [22, 25, 36], game-playing [12, 57, 69] and aircraft collision avoidance [41]. This ubiquity, however, provides adversaries with both the opportunity and incentive to strategically fool machine learning systems during both the training (poisoning attacks) [5,9,39,58,65] and test (evasion attacks) [8,17,33,55,56, 61, 75] phases. In an evasion attack, an adversary adds imperceptible perturbations to inputs in the test phase to cause misclassification. A large number of adversarial example-based evasion attacks have been proposed against ML algorithms used for tasks such as image classification [8, 17, 19, 33, 61, 75], object detection [21, 52, 81], image segmentation [2, 30] and speech recognition [18, 84]; ∗ †  Equal contribution. Work done while at Princeton University  33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.  generative models for image data [44] and even reinforcement learning algorithms [37, 45]. These attacks have been carried out in black-box [7, 11, 20, 51, 59, 60, 75] as well as in physical settings [28, 48, 68, 72]. A wide variety of defenses based on adversarial training [33, 53, 76], input de-noising through transformations [6, 24, 27, 67, 82], distillation [63], ensembling [1, 4, 73] and feature nullification [79] were proposed to defend ML algorithms against evasion attacks, only for most to be rendered ineffective by stronger attacks [3, 14–16]. Iterative adversarial training [53] is a current state-of-the-art empirical defense. Recently, defenses that rely on adversarial training and are provably robust to small perturbations have been proposed [34, 43, 64, 71] but are unable to achieve good generalization behavior on standard datasets such as CIFAR-10 [46]. In spite of an active line of research that has worked to characterize the difficulty of learning in the presence of evasion adversaries by analyzing the sample complexity of learning classifiers for known distributions [66] as well as in the distribution-free setting [23, 54, 83], fundamental questions remain unresolved. One such question is, what is the behavior of the optimal achievable loss in the presence of an adversary? In this paper, we derive bounds on the 0−1 loss of classifiers while classifying adversarially modified data at test time, which is often referred to as adversarial robustness. We first develop a framework that relates classification in the presence of an adversary and optimal transport with an appropriately defined adversarial cost function. For an arbitrary data distribution with two classes, we characterize optimal adversarial robustness in terms of the transportation distance between the classes. When the classifier comes from a restricted hypothesis class, we obtain a lower bound on the minimum possible 0 − 1 loss (or equivalently, an upper bound on the maximum possible classification accuracy). We then consider the case of a mixture of two Gaussians and derive matching upper and lower bounds for adversarial robustness by framing it as a convex optimization problem and proving the optimality of linear classifiers. For an ℓ∞ adversary, we also present the explicit solution for this optimization problem and analyze its properties. Further, we derive an expression for sample complexity with the assumption of a Gaussian prior on the mean of the Gaussians which allows us to independently match and extend the results from Schmidt et al. [66] as a special case. Finally, in our experiments, we find transportation costs between the classes of empirical distributions of interest such as MNIST [49], Fashion-MNIST [80] and CIFAR-10 [46] for adversaries bounded by ℓ2 and ℓ∞ distance constraints, and relate them to the classification loss of state-of-theart robust classifiers. Our results demonstrate that as the adversarial budget increases, the gap between current robust classifiers and the lower bound increases. This effect is especially pronounced for the CIFAR-10 dataset, providing a clear indication of the difficulty of robust classification for this dataset. What do these results imply? First, the effectiveness of any defense for a given dataset can be directly analyzed by comparing its robustness to the lower bound. In particular, this allows us to identify regimes of interest where robust classification is possible. Our bound can be used to decide whether a particular adversarial budget is big or small. Second, since our lower bound does not require any distributional assumptions on the data, we are able to directly apply it to empirical distributions, characterizing whether robust classification is possible. Further, in the Gaussian setting, the optimal classifier in the adversarial case depends explicitly on the adversary’s budget. The optimal classifier in the benign case (corresponding to a budget of 0), differs from that for non-zero budgets. This immediately establishes a trade-off between the benign accuracy and adversarial robustness achievable with a given classifier. This raises interesting questions about which classifier should actually be deployed and how large the trade-off is. From the explicit solution we derive in the Gaussian setting, we observe that non-robust features occur during classification due to a mismatch between the norms used by the adversary and that governing the data distribution. We expand upon this observation in Section 4.1, which was also made independently by Ilyas et al. [38]. Contributions: We summarize our contributions in this paper as follows: i) we develop a framework for finding general lower bounds for classification error in the presence of an adversary (adversarial robustness) using optimal transport, ii) we show matching upper and lower bounds for adversarial robustness as well as the sample complexity of attaining it for the case of Gaussian data and a convex,  2  origin-symmetric constraint on the adversary and iii) we determine lower bounds on adversarial robustness for empirical datasets of interest and compare them to those of robustly trained classifiers.  2 Preliminaries and Notation In this section, we set up the problem of learning in the presence of an evasion adversary. Such an adversary presents the learner with adversarially modified examples at test time but does not interfere with the training process [17, 33, 75]. We also define notation for the rest of the paper and explain how other work on adversarial examples fits into our setting. Symbol  Usage  X Space of natural examples Space of examples produced by the adversary X˜ N : X → 2X̃ Neighborhood constraint function for adversary P Distribution of labeled examples (on X × {−1, 1}) Table 1: Basic notation for the adversarial learning problem We summarize the basic notation in Table 1. We now formally describe the learning problem. There is an unknown P ∈ P(X × {−1, 1}). The learner receives labeled training data (x, y) = ((x0 , y0 ), . . . , (xn−1 , yn−1 )) ∼ P n and must select a hypothesis h. The evasion adversary receives a labeled natural example (xTest , yTest ) ∼ P and selects x̃ ∈ N (xTest ), the set of adversarial examples in the neighborhood of xTest . The adversary gives x̃ to the learner and the learner must estimate yTest . Their performance is measured by the 0-1 loss, ℓ(yTest , h(x̃)). Examples produced by the adversary are elements of a space X˜ . In most applications, X = X̃ , but we find it useful to distinguish them to clarify some definitions. We require N (x) to be nonempty so some choice of x̃ is always available. By taking X = X˜ and N (x) = {x}, we recover the standard problem of learning without an adversary. If N1 , N2 are neighborhood functions and N1 (x) ⊆ N2 (x) for all x ∈ X , N2 represents a stronger adversary. When X = X̃ , one way to produce a neighborhood function N is from a distance d on X and an adversarial budget constraint β: N (x) = {x̃ : d(x, x̃) ≤ β}. This provides an ordered family of adversaries of varying strengths used in previous work [17, 33, 66]. The learner’s error rate under the data distribution P with an adversary constrained by the neighborhood function N is L(N, P, h) = E(x,y)∼P [maxx̃∈N (x) ℓ(h(x̃), y)].  3 Adversarial Robustness from Optimal transport In this section, we explain the connections between adversarially robust classification and optimal transport. At a high level, these arise from the following idea: if a pair of examples, one from each class, are adversarially indistinguishable, then any hypothesis can classify at most one of the examples correctly, By finding families of such pairs, one can obtain lower bounds on classification error rate. When the set of available hypotheses is as large as possible, the best of these lower bounds is tight. Section Roadmap: We will first review some basic concepts from optimal transport theory [78]. Then, we will define a cost function for adversarial classification as well as its associated potential functions that are needed to establish Kantorovich duality. We show how a coupling between the conditional distributions of the two classes can be obtained by composing couplings derived from the adversarial strategy and the total variation distance, which links hypothesis testing and transportation costs. Finally, we show that the potential functions have an interpretation in terms of classification, which leads to our theorem connecting adversarial robustness to the optimal transport cost. 3.1 Basic definitions from optimal transport In this section, we use capital letters for random variables and lowercase letters for points in spaces. Couplings A coupling between probability distributions PX on X and PY on Y is a joint distribution on X × Y with marginals PX and PY . Let Π(PX , PY ) be the set of such couplings. 3  Definition 1 (Optimal transport cost). For a cost function c : X × Y → R ∪ {+∞} and marginal distributions PX and PY , the optimal transport cost is C(PX , PY ) =  inf  PXY ∈Π(PX ,PY )  E(X,Y )∼PXY [c(X, Y )].  (1)  Potential functions and Kantorovich duality There is a dual characterization of optimal transport cost in terms of potential functions which we use to make the connection between the transport and classification problems. Definition 2 (Potential functions). Functions f : X → R and g : Y → R are potential functions for the cost c if g(y) − f (x) ≤ c(x, y) for all (x, y) ∈ X × Y. A pair of potential functions provide a one-dimensional representation of the spaces X and Y. This representation must be be faithful to the cost structure on the original spaces: if a pair of points (x, y) are close in transportation cost, then f (x) must be close to g(y). In the dual optimization problem for optimal transport cost, we search for a representation that separates PX from PY as much as possible: (2) C(PX , PY ) = sup EY ∼PY [g(Y )] − EX∼PX [f (X)]. f,g  For any choices of f , g, and PXY , it is clear that E[g(Y )] − E[f (X)] ≤ E[c(X, Y )]. Kantorovich duality states that there are in fact choices for f and g that attain equality. Define the dual of f relative to c to be f c (y) = inf x c(x, y) + f (x). This is the largest function that forms a potential for c when paired with with f . In (2), it is sufficient to optimize over pairs (f, f c ). Compositions The composition of cost functions c : X × Y → R and c′ : Y × Z → R is (c ◦ c′ ) : X × Z → R  (c ◦ c′ )(x, z) = inf c(x, y) + c′ (y, z). y∈Y  The composition of optimal transport costs can be defined in two equivalent ways: (C ◦ C ′ )(PX , PZ ) = inf C(PX , PY ) + C ′ (PY , PZ ) = inf E[(c ◦ c′ )(X, Z)] PY  Total variation distance  PXZ  The total variation distance between distributions P and Q is CTV (P, Q) = sup P (A) − Q(A).  (3)  A  We use this notation because it is the optimal transport cost for the cost function cTV : X × X → R, cTV (x, x′ ) = 1[x 6= x′ ]. Observe that (3) is equivalent to (2) with the additional restrictions that f (x) ∈ {0, 1} for all x, i.e. f is an indicator function for some set A and g = f cTV . For binary classification with a symmetric prior on the classes, a set A that achieves the optimum in Eq. (3) corresponds to an optimal test for distinguishing P from Q.  3.2 Adversarial cost functions and couplings We now construct specialized version of costs and couplings that translate between robust classification and optimal transport. Cost functions for adversarial classification The adversarial constraint information N can be encoded into the following cost function cN : X × X˜ → R: cN (x, x̃) = 1[x̃ 6∈ N (x)]. The composition of cN and c⊤ N (i.e. cN with the arguments flipped) has simple combinatorial interpretation: ′ (cN ◦ c⊤ )(x, x ) = 1[N (x) ∩ N (x′ ) = ∅]. N  Perhaps the most well-known example of optimal transport is the earth-mover’s or 1-Wasserstein distance, where the cost function is a metric on the underlying space. In general, the transportation ′ ′ ⊤ cost cN ◦ c⊤ N is not a metric on X because (cN ◦ cN )(x, x ) = 0 does not necessarily imply x = x . ′ However, when (cN ◦ c⊤ N )(x, x ) = 0, we say that the points are adversarially indistinguishible. 4  Couplings from adversarial strategies Let a : X → X˜ be a function such that a(x) ∈ N (x) for all x ∈ X . Then a is an admissible adversarial perturbation strategy. The adversarial expected risk can be expressed as a maximization over adversarial strategies: L(N, P, h) = supa1 ,a−1 E(x,c)∼P [ℓ(h(ac (x)), c)]. Let X̃1 = a1 (X1 ), so a1 gives a coupling PX1 X̃1 between PX1 and PX̃1 . By construction, CN (PX1 , PX̃1 ) = 0. A general coupling between PX1 and PX̃1 with CN (PX1 , PX̃1 ) = 0 corresponds to a randomized adversarial strategy. We define PX̃−1 and PX−1 X̃−1 analogously. By composing the adversarial strategy coupling PX1 X̃1 , the total variation coupling of PX̃1 and PX̃−1 , and PX̃−1 X−1 , we obtain a coupling PX1 X−1 . Potential functions from classifiers Now we can explore the relationship between transport and classification. Consider a given hypothesis h : X˜ → {−1, 1}. A labeled adversarial example (x̃, y) is classified correctly if x̃ ∈ h−1 (y). A labeled example (x, y) is classified correctly if N (x) ⊆ h−1 (y). Following Cullina et al. [23], we define degraded hypotheses h̃ : X → {−1, 1, ⊥},  y : N (x) ⊆ h−1 (y) h̃(x) = ⊥ : otherwise.  PX̃−1 X˜  h(x) = −1  h(x) = 1  PX−1 X 1  This allows us to express the adversarial classification accuracy of h, 1 − L(N, h, P ), as 1 (E[1[h̃(X1 ) = 1]] + E[1[h̃(X−1 ) = −1]]). 2  PX̃1  h̃(x) = −1  PX1 h̃(x) = ⊥  h̃(x) = 1  f  g  0  Figure 1: The relationships between a classifier h : X → {1, −1}, a degraded classifier h̃ : X˜ → {1, −1, ⊥}, and potential functions f, g : X → R.  Observe that 1[h̃(x) = 1] + 1[h̃(x′ ) = −1] ≤ ′ (cN ◦c⊤ N )(x, x )+1. Thus the functions f (x) = 1 − 1[h̃(x) = 1] and g(x) = 1[h̃(x) = −1] are admissible potentials for cN ◦ c⊤ N . This is illustrated in Figure 3.  Our first theorem characterizes optimal adversarial robustness when h is allowed to be any classifier. Theorem 1. Let X and X̃ be Polish spaces. Let N : X → 2X̃ be a neighborhood function such that the set {(x, x̃) : x̃ ∈ N (x)} is closed. For any pair of distributions PX1 ,PX−1 on X , ⊤ (CN ◦ CN )(PX1 , PX−1 ) = 1 − 2 inf L(N, h, P ) h  where h : X̃ → {1, −1} can be any measurable function. In the case of finite spaces, this theorem is essentially equivalent to the König-Egerváry theorem on size of a maximum matching in a bipartite graph. The full proof is in Section A of the Appendix. If instead of all measurable functions, we consider h ∈ H, a smaller hypothesis class, Theorem 1 provides a lower bound on inf h∈H L(N, h, P ).  4 Gaussian data: Optimal loss In this section, we consider the special case when the the data is generated from a mixture of two Gaussians with identical covariances and means that just differ in sign. Directly applying (1) or (2), requires optimization over either all classifiers or all transportation plans. However, a classifier and a coupling that achieve the same cost must both be optimal. We exploit this to show that optimizing over linear classifiers and ‘translate and pair’ transportation plans is sufficient to characterize adversarial robustness in this case. Problem setup: Consider a labeled example (X, Y ) ∈ Rd × {−1, 1} such that the example X has a Gaussian conditional distribution, X|(Y = y) ∼ N (yµ, Σ), and Pr(Y = 1) = Pr(Y = −1) = 12 . 5  Let B ⊆ Rd be a closed, convex, absorbing, origin-symmetric set. The adversary is constrained to add perturbations to a data point x contained within βB, where β is an adversarial budget parameter. That is, for all x, N (x) = x + βB. This includes ℓp -constrained adversaries as the special case B = {z : kzkp ≤ 1}. For N and P of this form, we will determine inf h L(N, P, h) where h can be any measurable function. We first define the following convex optimization problem in order to state Theorem 2. In the proof of Theorem 2, it will become clear how it arises. Definition 3. Let α∗ (β, x) be the solution to the following convex optimization problem: (z, y, α) ∈ Rd+d+1  min α s.t. kykΣ ≤ α kzkB ≤ β z+y =x p where we use the seminorms kykΣ = y ⊤ Σ−1 y and kzkB = inf{β : z ∈ βB}.  (4)  ⊤ Theorem 2. Let N (x) = x + βB. Then (CN ◦ CN )(N (µ, Σ), N (−µ, Σ)) = 1 − 2Q(α∗ (β, µ)), where Q is the complementary cumulative distribution function for N (0, 1).  The crucial properties of the solution to (4) are characterized in the following lemma. Lemma 1. Let x ∈ Rd , β ≥ 0, and α = α∗ (β, x). There are y, z, w ∈ Rd such that y + z = x and kykΣ = α  kzkB = β  kwkΣ∗ = 1  kwkB∗ = γ  w⊤ y = α  w⊤ z = βγ.  The proof of Lemma 1 is in Section B.1 of the Appendix. Proof of Theorem 2. We start from the definition of optimal transport cost and consider the restricted class of “translate and pair in place” couplings to get an upper bound. In these couplings, the adversarial attacks are translations by a constant: X̃1 = X1 + z and X̃−1 = X−1 − z. The total variation coupling between X̃1 and X̃−1 does “pairing in place”.  ⊺  w z − w⊺ µ ⊤ √ )(PX1 , PX−1 ) ≤ inf CT V (PX̃1 , PX̃−1 ) = inf sup 2Q (CN ◦ CN − 1. z∈βB w z∈βB w⊺ Σw The full computation of the total variation between Gaussiansp is in Section B.2 of the Appendix.. The infimum is attained at w∗ = 2Σ−1 (z − µ) and its value is (z − µ)⊺ Σ−1 (z − µ). The choice of z from Lemma 1 makes the upper bound 2Q(−α∗ (β, µ)) − 1 = 1 − 2Q(α∗ (β, µ)). Now we consider the lower bounds on optimal transport cost from linear classification functions of the form fw (x) = sgn (w⊺ x). In the presence of an adversary, the classification problem becomes maxw P(x,y)∼P [fw (x + aw,y (x)) = y] . When y = 1, the correct classification event is fw (x + aw,1 (x)) = 1, or equivalently w⊺ x − βkwkB∗ > 0. This ultimately gives the lower bound   βkwkB∗ − w⊺ µ ⊤ (CN ◦ CN )(PX1 , PX−1 ) ≥ sup 1 − 2Q . (5) kwkΣ∗ w  The full calculation appears in the supplementary material (Section B.3). From Lemma 1, there is a choice of w that makes the bound in (5) equal to 1 − 2Q(α∗ (β, µ)). The proof of Theorem 2 shows that linear classifiers are optimal for this problem. The choice of w provided by Lemma 1 specifies the orientation of the optimal classifier. 4.1 Special cases  Matching norms for data and adversary: When B is the unit ball derived from Σ, the optimization 1 problem (4) has a very simple solution: α∗ (β, x) = kxkΣ −β, y = αx, z = βx, and w = kxk Σ−1 x. Σ Thus the same classifier is optimal for all adversarial budgets. In general, α∗ (0, x) = kxkΣ and α∗ (kxkB , x) = 0, but α∗ (β, x) can be nontrivially convex for 0 ≤ β ≤ kxkB . When there is a difference between the two seminorms, the adversary can exploit this and the optimal modification is not proportional to x. The optimal classifier varies with the adversarial budget, so there is a trade-off between accuracy and robust accuracy. ℓ∞ adversaries: In Figure 2, we illustrate this phenomenon for an ℓ∞ adversary. We plot α(β, x) for Σ = I (so k · kΣ = k · k2 ) and taking B to be the ℓ∞ unit ball (so k · kB = k · k∞ ). In 6  1  α∗ (β, x), d = 10  0.6 0.4  α∗ (β, x), d = 1000  0.8  α∗ (β, x)  α∗ (β, x)  1 0.8  0.6 0.4 0.2  0.2  0  0 0  0.1  0.2  0.3  0.4  0.5  0  0.6  0.02  0.04  β  0.06  0.08  0.1  0.12  β  Figure 2: Variation in α∗ w.r.t. β for an ℓ∞ adversary with d = 10 (left) and d = 1000 (right). α∗ is the point at which the primal transport problem and the dual classification problem have matching solutions, given by 1 − 2Q(α∗ ). The classification loss at this point is simply Q(α∗ ). this case (4) has an explicit solution. For each coordinate zi , set zi = min(xi , β), which gives yi = xi − min(xi , β), which makes the constraints tight. Thus, as β increases, more components of z equal those of x, reducing the marginal effect of an additional increase in β. Due to the mismatch between the seminorms governing the data and adversary, the value of β determines which features are useful for classification, since features less than β can be completely erased. Without an adversary, all of these features would be potentially useful for classification, implying that human-imposed adversarial constraints, with their mismatch from the underlying geometry of the data distribution, lead to the presence of non-robust features that are nevertheless useful for classification. A similar observation was made in concurrent work by Ilyas et al. [38].  5 Gaussian data: Sample complexity lower bound In this section, we use the characterization of the optimal loss in the Gaussian robust classification problem to establish the optimality of a rule for learning from a finite number of samples. This allows for precise characterization of sample complexity in the learning problem. Consider the following Bayesian learning problem, which generalizes a problem considered by Schmidt et al. [66]. We start from the classification problem defined in Section 4. There, the choice 1 of the classifier h could directly depend on µ and Σ. Now we give µ the distribution N (0, m I). A learner who knows this prior but not the value of µ is provided with n i.i.d. labeled training examples samples. The learner selects any measurable classification function ĥn : Rd → {−1, 1} by applying some learning algorithm to the training data with the goal of minimizing E[L(N, P, ĥn )]. The optimal transport approach allows us to determine the exact optimal loss for this problem for each n as well as the optimal learning algorithm. To characterize this loss, we need the following definitions. Let A be the ℓ2 unit ball: {y ∈ Rd : kyk2 ≤ 1}. Let S(α, β) = {(x, t) ∈ Rd × R : x ∈ tαA + βB}.  Theorem 3. In the learning problem described above, the minimum loss of any learning rule is PrV ∼N (0,I) [V ∈ S(ρ, βρ)], where ρ2 = m(m+n) . n  x S(ρ, βρ) S(ρ, 0) S(0, βρ) t  The proof is in Section C of the Appendix. The special case where B is an ℓ∞ ball was considered by Schmidt et al. [66]. They obtained a lower bound on Figure 3: S(ρ, βρ) is the set appearing loss that can be expressed in our notation as Pr[V ∈ in the statement of Theorem 3. S(ρ, 0) S(0, ρβ)]. This bound essentially ignores the random corresponds to the loss lower bound obnoise in the problem and computes the probability that tained by Schmidt et al.. S(0, βρ) correafter seeing n training examples, the posterior distribu- sponds to the loss in the non-adversarial tions for Xn+1 |(Yn+1 = 1) and Xn+1 |(Yn+1 = −1) are version of this classification problem. adversarially indistinguishable. The true optimal loss takes into account the intermediate case in which these posterior distributions are difficult but not impossible to distinguish in the presence of an adversary. Figure 3 depicts the sets involved in these loss expressions. 7  Optimal minimum loss Robust classifier loss  0.4 0.3 0.2 0.1 0 0  1  2  3  4  5  β  (a) MNIST  0.5  Classification loss  Classification loss  Classification loss  0.5  Optimal minimum loss Robust classifier loss  0.4 0.3 0.2 0.1 0 0  1  2  3  4  β  5  6  (b) Fashion MNIST  7  0.5 0.4 0.3 0.2 Optimal minimum loss Robust classifier loss  0.1 0 0  2  4  6  8  10  β  (c) CIFAR-10  Figure 4: Variation in minimum 0 − 1 loss (adversarial robustness) as β is varied for ‘3 vs. 7’. For all datasets, the loss of a robustly classifier (trained with iterative adversarial training [53]) is also shown for a PGD adversary with an ℓ2 constraint. 1  Schmidt et al. investigate sample complexity in the following parameter regime: m = c1 d 2 which by design is a low noise regime. In this regime, they establish upper and lower bounds on sample β2d ′ 2 complexity of learning an adversarially robust classifier: C log d ≤ n ≤ C β d. By taking into account the effect of the random noise, our characterization of the loss loses this gap. For larger values of m, the difference between Pr[Y ∈ S(0, ρβ)] and Pr[Y ∈ S(ρ, ρβ)] becomes more significant, so our analysis is useful over a much broader range of parameters.  6 Experimental Results In this section, we use Theorem 1 to find lower bounds on adversarial robustness for empirical datasets of interest. We also compare these bounds to the performance of robustly trained classifiers on adversarial examples and find a gap for larger perturbation values. For reproducibility purposes, our code is available at https://github.com/inspire-group/robustness-via-transport. 6.1 Experimental Setup We consider the adversarial classification problem on three widely used image datasets, namely MNIST [49], Fashion-MNIST [80] and CIFAR-10 [46], and obtain lower bounds on the adversarial robustness for any classifier for these datasets. For each dataset, we use data from classes 3 (PX1 ) and 7 (PX−1 ) to obtain a binary classification problem. This choice is arbitrary and similar results are obtained with other choices, which we omit for brevity. We use 2000 images from the training set of each class to compute the lower bound on adversarial robustness when the adversary is constrained using the ℓ2 norm. For the ℓ∞ norm, these pairs of classes are very well separated, making the lower bounds less interesting (results in Section D of the Appendix). For the MNIST and Fashion MNIST dataset, we compare the lower bound with the performance of a 3-layer Convolutional Neural Network (CNN) that is robustly trained using iterative adversarial training [53] with the Adam optimizer [42] for 12 epochs. This network achieves 99.9% accuracy on the ‘3 vs. 7’ binary classification task on both MNIST and Fashion-MNIST. For the CIFAR-10 dataset, we use a ResNet-18 [35] trained for 200 epochs, which achieves 97% accuracy on the binary classification task. To generate adversarial examples both during the training process and to test robustness, we use Projected Gradient Descent (PGD) with an ℓ2 constraint, random initialization and a minimum of 10 iterations. We note that since more powerful heuristic attacks may be possible against these robustly trained classifiers, the ‘robust classifier loss’ reported here represents a lower bound on the adversarial risk. 6.2 Lower bounds on adversarial robustness for empirical distributions Now, we describe the steps we follow to obtain a lower bound on adversarial robustness for empirical distributions through a direct application of Theorem 1. We first create a k × k matrix D whose entries are kxi − xj kp , where k is the number of samples from each class and p defines the norm. Now, we threshold these entries to obtain Dthresh , the matrix of adversarial costs (cN ◦ c⊤ N )(xi , xj ) (recall Section 3.2), whose (i, j)th entry is 1 if Dij > 2β and 0 otherwise, where β is the constraint ⊤ on the adversary. Finally, optimal coupling cost (CN ◦ CN )(PX1 , PX−1 ) is computed by performing minimum weight matching over the bipartite graph defined by the cost matrix Dthresh using the Linear Sum Assignment module from Scipy [40]. 8  In Figure 4, we show the variation in the minimum possible 0 − 1 loss (adversarial robustness) in the presence of an ℓ2 constrained adversary as the attack budget β is increased. For the MNIST and Fashion MNIST data, we compare this loss value to that of a robustly trained classifier [53] when the PGD attack is used (on the same data). For both datasets, until a certain β value, robust training converges and the model attains a non-trivial adversarial robustness value. Nevertheless, there is a gap between the empirically obtained and theoretically predicted minimum loss values. Further, after β = 3.8 (MNIST) and β = 4.8 (Fashion MNIST), we observe that robust training is unable to converge 3 . We believe this occurs due to the fact that around 40% or more of the total data at that value of β is close to the boundary when adversarially perturbed, making the classification problem very challenging. Figure 5c shows the variation in the minimum possible 0 − 1 loss for the CIFAR-10 dataset. We note that in order to reduce the classification accuracy to random for CIFAR-10, a much larger ℓ2 budget is needed compared to either MNIST or Fashion-MNIST, implying that the classes are better separated. Further, for this dataset, the model used is not able to converge beyond β = 1.5.  7 Related work and Concluding Remarks We only discuss the closest related work that analyzes evasion attacks theoretically. Extensive recent surveys [10, 50, 62] provide a broader overview. Distribution-specific generalization analysis: Schimdt et al. [66] studied the sample complexity of learning a mixture of Gaussians as well as Bernoulli distributed data in the presence of ℓ∞ -bounded adversaries, which we recover as a special case of our framework in 5. Gilmer et al. [32] analyzed the robustness of classifiers for a distribution consisting of points distributed on two concentric spheres. In contrast to these papers, our framework applies for any binary classification problem as our lower bound applies to arbitrary distributions. Sample complexity in the PAC setting: Cullina et al. [23], Yin et al. [83] and Montasser et al. [54] derive the sample complexity needed to PAC-learn a hypothesis class in the presence of an evasion adversary. These approaches do not provide an analysis of the optimal loss under a given distribution, but only of the number of samples needed to get ǫ-close to it, i.e. to learn the best empirical hypothesis. Optimal transport for bounds on adversarial robustness: Sinha et al. [71] constrain the adversary using a Wasserstein distance bound on the distribution that results from perturbing the benign distribution and study the sample complexity of SGD for minimizing the relaxed Lagrangian formulation of the learning problem with this constraint. In contrast, we directly analyze the optimal cost, use a cost function that characterizes sample-wise adversarial perturbation exactly, which aligns with current practice and provide a lower bound on the 0 − 1 loss with an adversary, while Sinha et al. minimize an upper bound to perform robust training. Dohmatob [26] uses the ‘blowup’ property of certain data distributions to provide bounds on adversarial risk, given some level of ordinary risk. In comparison, we require very mild assumptions on the example space, distribution, and adversarial constraints while the assumptions in Dohmatob’s paper are more restrictive. Even in regimes where the framework of Dohmatob is applicable, our approach provides two key advantages. First, our bounds explicitly concern the adversarial robustness of the optimal classifier, while Dohmatob’s relate adversarial the adversarial robustness to the benign classification error of a classifier. Thus, our bounds can still be nontrivial even when there is a classifier with a benign classification error of zero, which is exactly the case in our MNIST experiments. Second, our bounds apply for any adversarial budget while Dohmatob’s become non-trivial only when the adversarial budget exceeds a critical threshold depending on the properties of the space. Possibility of robust classification: Bubeck et al. [13] show that there exist classification tasks in the statistical query model for which there is no efficient algorithm to learn robust classifiers. Tsipras et al. [77], Zhang et al. [85] and Suggala et al. [74] study the trade-o"
"Non-Invasive Fuhrman Grading of Clear Cell Renal Cell Carcinoma Using Computed Tomography Radiomics Features and Machine Learning Mostafa Nazari1, Isaac Shiri*2, Ghasem Hajianfar3, Niki Oveisi5, Hamid Abdollahi4, Mohammad Reza Deevband1, Mehrdad Oveisi6 1. Department of Medical Physics, School of Medicine, Shahid Beheshti University of Medical Sciences, Tehran, Iran. 2. Division of Nuclear Medicine and Molecular Imaging, Department of Medical Imaging, Geneva University Hospital, CH-1211 Geneva 4, Switzerland 3. Rajaie Cardiovascular Medical and Research Center, Iran University of Medical Science, Tehran, Iran 4. School of Population and Public Health, The University of British Columbia, BC, V6T 1Z4, Canada. 5. Department of Radiologic Sciences and Medical Physics, Faculty of Allied Medicine, Kerman University, Kerman, Iran 6. Department of Computer Science, University of British Columbia, Vancouver BC, Canada  Corresponding Author: Isaac Shiri Address: Division of Nuclear Medicine and Molecular Imaging, Department of Medical Imaging, Geneva University Hospital, CH-1211 Geneva 4, Switzerland Email: Isaac.Shiri@etu.unige.ch Tel: +41766269359  Key words: RCC, Radiomics, Machine Learning, Computed Tomography  Introduction Renal cell carcinoma (RCC) is the seventh most common cancer in the world, with a mortality rate of 140,000 per year (1). The most common types of renal cancer cells consists of clear cells RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC) (2, 3). Approximately 70% of kidney cancers are made up of ccRCC, pRCC accounts for 10-15% of kidney cancers, and chRCC is the least common type with only 5% of kidney cancer cases (4). A ccRCC diagnosis has a survival rate of less than 5-years and a higher risk of metastasis compared to pRCC and chRCC (5). The most important step for a physician during cancer diagnosis and treatment is tumor staging and grading. Tumor grading is a description of the differentiation of tumor tissue cells relative to normal tissue cells. It is an indicator of how quickly a tumor is expected to grow and spread. The Fuhrman grading system is widely accepted (6) and is based on the assessment of the following cell nucleus characteristics: nuclear size, nuclear shape, and nucleolar prominence. Based on these assessments, the tumor will be classified into one of four different grades (I-IV). Grades I and II are considered as low-grade tumors with a favorable prognosis, while grades III and IV accounted for high-grade tumors and have an unfavorable prognosis (7). Currently, fine needle aspiration (FNA) and imaging-guided biopsies are the gold standard methods for preoperative kidney tumor grading. However, these techniques have some drawbacks including infection, bleeding, tumor cells spreading, and provide limited information regarding the whole tumor. For example, a pathologist may diagnose a ductal carcinoma in situ (DCIS) as a noninvasive breast cancer based on the FNA breast sample obtained, when in fact, the patient has infiltrating ductal carcinoma (IDC) in a nearby area (8, 9).  In the recent years, several new non-invasive therapeutic methods for RCC have been developed, including radiofrequency ablation, cryoablation, and active surveillance(10). However, there lacks proper criterion for patient management with these non-invasive/minimally invasive treatment methods, and are often treated surgically post-diagnosis (11, 12). Therefore, it is desirable to produce individualized treatment strategies, where radical approaches (e.g. surgery) are taken for aggressive or high-grade tumors (III, IV) ccRCC, and conservative management (e.g. active surveillance) is provided for low-grade (I, II) lesions (13). To guide this decision, a noninvasive and accurate method for Fuhrman grading of renal cell carcinoma tumors preoperatively is desirable. For this purpose, two approaches are attractive for researchers at present. One is the Apparent diffusion coefficient (ADC) value in MRI imaging (14), and the other is CT-based semiquantitative and quantitative techniques (15, 16). Radiomics refers to the comprehensive quantification of tumor phenotypes in order to uncover disease characteristics that fail to be explained by the naked eye (17-20). In fact, it can be said that radiomics serves as the bridge between medical imaging and personalized medicine (21). Radiomics is a new era of science which faces many challenges, including image acquisition (22), reconstruction (23, 24), processing (25), and model development(26, 27) to provide robust and reproducible models. Previous studies have shown that the radiomics signature is valuable for differentiating high/low grade ccRCC tumors (28, 29). This study aims to construct a radiomics feature–based machine learning model to predict the Fuhrman Grade of ccRCC patients preoperatively.  Methods and Materials Flow chart of the current study is illustrated in Figure 1  Figure 1. Illustrates the process flow followed in the paper.    Data  Data was collected from cancer image archive databases from 1980 to 2016 (30). (Table 1) Table 1.Clinical characteristics of Clear Cell Renal Cell Carcinoma Characteristic Patient (N=71) Gender Male Female Age, Y Grade Low Grade (I ,II) High Grade (III,IV)    51 (71%) 20 (29%) 60.3 ± 11.7 31 40  Image acquisition technique  All patients had undergone a three phasic CT scan, including 1) a routine unenhanced CT scan, 2) a corticomedullary phase (CMP) contrast enhanced scan starting 40 seconds after the contrast material injection, and 3) a nephrographic phase (NP) contrast-enhanced scan performed 70–90 seconds after intravenous injection of iodinated contrast material. The iodine content (300 mg/mL) was infused at an infusion rate of 3 mL/s at an infusion dose of 80–100 mL. All subjects were scanned using a GE and Siemens CT machine with a tube voltage of 120 kV and a tube current of 150–300 mA with daily clinical reconstruction parameters.   Tumor Segmentation  In this study manual volume of interest (VOI) segmentation was performed and verified by a radiologist with 3D slicers.   Image pre-processing  Prior to feature extraction, the voxel size resampling method was applied on the images to create an isotropic dataset. This allowed comparisons between image data from different samples and scanners (31). Laplacian of Gaussian (LOG), wavelet decomposition (WAV), and discretization into 32, 64, and 128 bins preprocessing was performed to generate different set of features. For the LOG filter, different sigma values were used to extract fine, medium, and coarse features. Wavelet filtering yields 8 decompositions per level: all possible combinations of applying either a High or a Low pass filter in each of the three dimensions including HHH, HHL, HLH, HLL, LHH, LHL, LLH, and LLL. Preprocessing steps (including discretization, LOG, and wavelet) were also performed on all intensity, histogram, and textural features.   Feature extraction  Radiomics features were extracted through the PyRadiomics open source python library. Extracted features were then categorized into the following subgroups. Firstly, shape features depict the shape of the tumor volume (VOI) and geometric properties such as volume, maximum surface, tumor compactness, and sphericity. Furthermore, first-order statistics features describe the distribution of voxel intensities within tumor volumes. This includes mean, median, maximum, and minimum values of the voxel intensities on the image. Second-order statistics features (known as textural features) are used as a method to measure inter-relationships between voxel distributions in tumor volumes. This is reflective of changes in image space gray levels. These feature groups include: gray-level co-occurrence matrix (GLCM), gray-level run-length matrix (GLRLM), gray-level size-zone matrix (GLSZM), and gray-level dependence matrix (GLDM) features (see Table 2).  Table 2.Radiomics Features   First Order Statistics (FOS)                      Energy Total Energy Entropy Minimum 10th percentile 90th percentile Maximum Mean Median Interquartile Range Range Mean Absolute Deviation (MAD) Robust Mean Absolute Deviation (rMAD) Root Mean Squared (RMS) Standard Deviation Skewness Kurtosis Variance 19. Uniformity                   Gray Level Co-occurrence Matrix (GLCM)                         Shape Features   Volume Surface Area Surface Area to Volume ratio Sphericity Spherical Disproportion Maximum 3D diameter Maximum 2D diameter (Slice) Maximum 2D diameter (Column) Maximum 2D diameter (Row) Major Axis Minor Axis Least Axis Elongation Flatness   Autocorrelation Joint Average Cluster Prominence Cluster Shade Cluster Tendency Contrast Correlation Difference Average Difference Entropy Difference Variance Joint Energy Joint Entropy Informal Measure of Correlation (IMC) 1 Informal Measure of Correlation (IMC) 2 Inverse Difference Moment (IDM) Inverse Difference Moment Normalized (IDMN) Inverse Difference (ID) Inverse Difference Normalized (IDN) Inverse Variance Maximum Probability Sum Average Sum Entropy 23. Sum of Squares                    Gray Level Size Zone Matrix (GLSZM)  Small Area Emphasis (SAE) Large Area Emphasis (LAE) Gray Level Non-Uniformity (GLN) Gray Level Non-Uniformity Normalized (GLNN) Size-Zone Non-Uniformity (SZN) Size-Zone Non-Uniformity Normalized (SZNN) Zone Percentage (ZP) Gray Level Variance (GLV) Zone Variance (ZV) Zone Entropy (ZE) Low Gray Level Zone Emphasis (LGLZE) High Gray Level Zone Emphasis (HGLZE) Small Area Low Gray Level Emphasis (SALGLE) Small Area High Gray Level Emphasis (SAHGLE) Large Area Low Gray Level Emphasis (LALGLE) 16. Large Area High Gray Level Emphasis (LAHGLE)   Gray Level Run Length Matrix (GLRLM)                                        Short Run Emphasis (SRE) Long Run Emphasis (LRE) Gray Level Non-Uniformity (GLN) Gray Level Non-Uniformity Normalized (GLNN) Run Length Non-Uniformity (RLN) Run Length Non-Uniformity Normalized (RLNN) Run Percentage (RP) Gray Level Variance (GLV) Run Variance (RV) Run Entropy (RE) Low Gray Level Run Emphasis (LGLRE) High Gray Level Run Emphasis (HGLRE) Short Run Low Gray Level Emphasis (SRLGLE) Short Run High Gray Level Emphasis (SRHGLE) Long Run Low Gray Level Emphasis (LRLGLE) 16. Long Run High Gray Level Emphasis (LRHGLE)  Gray Level Dependence Matrix (GLDM)  Small Dependence Emphasis (SDE) Large Dependence Emphasis (LDE) Gray Level Non-Uniformity (GLN) Dependence Non-Uniformity (DN) Dependence Non-Uniformity Normalized (DNN) Gray Level Variance (GLV) Dependence Variance (DV) Dependence Entropy (DE) Low Gray Level Emphasis (LGLE) High Gray Level Emphasis (HGLE) Small Dependence Low Gray Level Emphasis (SDLGLE) Small Dependence High Gray Level Emphasis (SDHGLE) Large Dependence Low Gray Level Emphasis (LDLGLE) 14. Large Dependence High Gray Level Emphasis (LDHGLE)  Neighboring Gray Tone Difference Matrix (NGTDM)  1-Coarseness 2-Contrast 3-Busyness 4-Complexity 5- Strength    Univariate analysis  For univariate analysis, early Pearson correlation tests between features were used to eliminate highly correlated features. Student’s t-tests were then used for comparisons between two groups. To control for the False Discovery Rate (FDR) in multiple hypothesis testing, the Benjamini-  Hochberg (FDR) correction method was applied on p-values, and the ultimately reported q-value (32).   Feature Set preprocessing  Due to the different ranges of radiomics features, without feature normalization, some features might appear as a larger weight, while others might appear as a lower weight. This depends on the distribution of feature values. To eradicate this, z-score normalization was applied to the feature values (33).   Feature Selection  Three different feature selections methods (Table 3) were implemented in this framework: enhanced variable selection algorithms based on the least absolute shrinkage and selection operator methods (34), student's t-test (26, 27), and MRMR (Minimum Redundancy Maximum Relevance) algorithm.   Multivariate Machine Learning Classifier  The following three classifiers (Table 3) were implemented and compared: logistic regression, random forest, and support vector machines (SVM).   Model evaluation The cross validation (CV) technique was applied in order to tune the model parameters.  Furthermore, bootstrapped datasets were used for model evaluations. The predictive power of all  models was investigated using the area under the receiver operator characteristic (ROC) curve (AUC). All analysis and evaluation were performed by using the programming software R (version 3.5.2). Table 3. Feature selection and Classification methods Feature Selection Methods  Abbreviation  Classification Methods Logistic Regression  Abbreviation  MRMR  Random Forest  RF  LASSO  Support Vector Machine  SVM  T student test Minimum Redundancy Maximum Relevance least absolute shrinkage and selection operator  LR  Results After applying inclusion/exclusion criteria, 71 (31 low-grade and 40 high-grade) patients were selected. The mean ages of the low- and high-grade groups were 60.05 and 60.08 years old, respectively. In total, there are 51 male and 20 female participants. Univariate analysis demonstrated that among filtered and non-filtered images only the 128 bin discretized images have a statistically significant difference (q-value < 0.05) in texture parameters with a mean AUC of 0.74±3 (q-value < 0.05). These features include Long Run High Gray Level Emphasis from GLRLM (AUC: 77, q-value: 0.0002), Cluster Tendency from GLCM (AUC: 72, q-value: 0.001), Contrast from NGTDM (AUC: 74, q-value: 0.03), and Dependence Non-Uniformity from GLDM (AUC: 72, q-value: 0.04) (Figure 2). Table 4 shows the AUC (95% CI) of three different ML-based classifiers. As shown in the table, there is a wide performance range from 0.5 to 0.86. Three different feature selection methods were applied prior to the implementation of each ML-based classifier to determine which is the  best for that model. The results demonstrated that the lasso method performs the best for logistic regression. Furthermore, the student’s t-test proved to be the best for random forest and SVM classifier models. Results for logistic regression suggested that 128 bin discretized images and fine LoG features have the highest performance with a mean of AUC 0.75. According to the results, the predictive performance of the random forest model has a range of 0.48 to 0.67. Among these, wavelet filtered images showed lowest performance and 128 bin discretized images showed highest performance. Among the three classifiers, SVM with a student's t-test feature selection presented the best predictive performance. SVM with Coarse LoG features demonstrated a mean AUC 0.83 (Figure 2).  a. Univariate Analysis  c. Coarse LoG filter  b.  d. Wavelet filter  Figure 2. AUC for discrimination of the high from low grade ccRCC. a. Univariate analysis of best predictor, b. LR model with 128 bin discretizing, c. SVM model with Coarse LoG filter, d. RF model with Wavelet filter. AUC: Area under receiver operating characteristic curve, LR: logistic regression, SVM: Support Vector Machine, RF: Random Forest.  Table 3. Classifiers performance with different pre-processing LR Original image 32_bin 64 _bin 128_bin LoG LoG_ sigma.0.5 LoG_sigma.1.0 LoG_sigma.1.5 LoG_sigma.2.0 LoG_sigma.2.5 LoG_sigma.3.0 LoG_sigma.3.5 LoG_sigma.4.0 LoG_sigma.4.5 LoG_sigma.5.0 WAVELET Wav_HHL Wav_HLH Wav_LHH Wav_HLL Wav_LLH Wav_LHL Wav_LLL  AUC (95%CI) RF  SVM  0.68 ±0.08 0.73 ±0.09 0.70 ±0.07 0.75 ±0.08  0.62 ±0.07 0.56 ±0.06 0.55 ±0.06 0.60 ±0.08  0.76 ±0.07 0.70 ±0.08 0.65 ±0.07 0.77 ±0.08  0.69 ±0.08 0.74 ±0.11 0.65 ±0.09 0.68 ±0.09 0.73 ±0.11 0.74 ±0.10 0.62 ±0.06 0.65 ±0.09 0.62 ±0.06 0.70 ±0.08  0.53 ±0.06 0.53 ±0.02 0.56 ±0.02 0.55 ±0.06 0.54 ±0.05 0.55 ±0.09 0.57 ±0.04 0.60 ±0.12 0.62 ±0.06 0.56 ±0.05  0.62 ±0.06 0.64 ±0.04 0.72 ±0.07 0.74 ±0.08 0.76 ±0.07 0.77 ±0.06 0.79 ±0.08 0.81 ±0.06 0.83 ±0.08 0.78 ±0.06  0.62 ±0.05 0.65 ±0.06 0.67 ±0.08 0.62 ±0.06 0.68 ±0.07 0.62 ±0.06 0.62 ±0.06  0.58 ±0.04 0.55 ±0.08 0.53 ±0.03 0.57 ±0.05 0.56 ±0.06 0.53 ±0.05 0.55 ±0.08  0.71 ±0.07 0.62 ±0.06 0.65 ±0.07 0.76 ±0.08 0.59 ±0.08 0.65 ±0.08 0.75 ±0.10  Discussion There is an important association between the Fuhrman grade and patient’s prognosis. There are several non-invasive methods proposed to predict the ccRCC Fuhrman grade preoperatively. In MR imaging, the apparent diffusion coefficient (ADC) value is known to be an indicator of tumor activity. Several studies have assessed the utility of the apparent diffusion coefficient (ADC) in distinguishing low- and high-grade clear cell RCC (14, 35). These studies showed that magnetic resonance imaging (MRI) has an acceptable predictive accuracy in the preoperative detection of the high grade RCC (AUC was 0.80) (36). However, MRI is a costly process and a wide range of ADC values for ccRCC have been reported in the literature (37, 38). Therefore, their repeatability needs to be validated further. Furthermore, a large number of CTbased semi-quantitative and quantitative studies have attempted to classify low- and high-grade ccRCC (15, 16). These studies showed that CT is a promising method for this work. Radiomics approach convert medical images into quantitative, high-dimensional, and mineable features to predict tumor status. However, the abundance of predictive modeling techniques means that it is important to choose the correct one for predicting tumor status. As some previous radiomics studies (16, 28), for Fuhrman grade prediction did not include shape features in their analyses, this study combined shape features and texture features to differentiate low- and high-grades of ccRCC. It was observed that shape features cannot be ignored from multivariate machine learning models. Univariate analysis of extracted radiomics features demonstrated that among filtered and non-filtered images only the 128 bin discretized images showed statistically significant texture parameters. In a similar univariate analysis, Zhan Feng et al (29) analyzed CT texture parameters and found effective quantitative parameters to evaluate the heterogeneity of ccRCC. After applying  the LoG filter, they reported that only entropy has a statistically significant difference after FDR corrections in all image phases. In this study four features showed statistically significant differences between two groups. These features include: Long Run High Gray Level Emphasis from GLRLM, Cluster Tendency from GLCM Contrast from NGTDM, and Dependence NonUniformity from GLDM matrix. Among these features, the Long Run High Gray Level Emphasis demonstrated the highest AUC (AUC: 77, q-value: 0.0002). The first machine learning model applied in this study was logistic regression. It is a machine learning classification algorithm used to predict the class probability of a categorical dependent variable. It was observed that among three different features selection methods, the best results for the Logistic Regression model was obtained when using the lasso algorithm. These results suggest that the AUC logistic regression model is approximately similar to results obtained in previous studies. Juice Dinga et al (16) used a texture-score based logistic regression model on a training cohort and the AUC was 0.878. When predictive models were applied on the validation cohort, good results were still obtained (AUC > 0.670). Jun Shu1 et al (39) extracted radiomics features from the corticomedullary (CMP) and nephrographic phases (NP) of CT images of all patients. They constructed logistic regression classification models to discriminate high and low grades ccRCC. Application of the model on CMP and NP showed an AUC of 0.766 (95% CI:0.709-0.816) and 0.818 (95% CI:0.765-0.838), respectively. Another machine learning model applied was random forest, which is an ensemble learning method that consists of a collection of decision trees. It uses a weighted average of those trees for the final decision (40). It works correctly for a large range of data, but is susceptible to overfitting. In this study, applying the random forest model on the dataset yielded unsatisfactory results (table 1). The SVM was the bestperforming classifier in this study. SVM creates a decision boundary between two classes that  enables the prediction of labels from one or more feature vectors. After applying the SVM model on filtered and unfiltered images, the best classification result was obtained when coarse LoG features were used with a mean AUC of 0.81. LoG filtering is an advanced image-filtering method that combines Laplacian filtering and Gaussian filtering. In a similar single-center retrospective study (28) , the performance of quantitative CT texture analysis combined with different ML based classifier methods is evaluated for discriminating low and high grade ccRCC. Despite differences in procedure, they also determined that highest predictive performance is achieved by an SVM classifier. In summary, both of these studies support each other with a similar conclusion that CT texture analysis is a useful and promising non-invasive method to predict the Fuhrman grades of ccRCCs preoperatively. The limitations of this study were as follows. (1) This study was a retrospective study with no external data validation, (2) the sample size was relatively small, (3) since the tumor boundary is manually drawn, the interference of the volume effect cannot be completely avoided. Conclusion The results of this study show that CT based SVM classifier with t-test features selection can be a useful and promising non-invasive method for the prediction of low and high Fuhrman nuclear grade ccRCCs. Additionally, the results demonstrated that 128 bin discretized preprocessing is an effective method under these conditions. Large, multicenter and externally prospective studies are needed for further validation of CT base machine learning models.  Reference 1. Capitanio U, Montorsi F. Renal cancer. The Lancet. 2016;387(10021):894-906. 2. Srigley JR, Delahunt B, Eble JN, Egevad L, Epstein JI, Grignon D, et al. The International Society of Urological Pathology (ISUP) vancouver classification of renal neoplasia. The American journal of surgical pathology. 2013;37(10):1469-89. 3. Marconi L, Dabestani S, Lam TB, Hofmann F, Stewart F, Norrie J, et al. Systematic review and meta-analysis of diagnostic accuracy of percutaneous renal tumour biopsy. European urology. 2016;69(4):660-73. 4. Muglia VF, Prando A. Renal cell carcinoma: histological classification and correlation with imaging findings. Radiologia brasileira. 2015;48(3):166-74. 5. Delahunt B, Cheville JC, Martignoni G, Humphrey PA, Magi-Galluzzi C, McKenney J, et al. The International Society of Urological Pathology (ISUP) grading system for renal cell carcinoma and other prognostic parameters. The American journal of surgical pathology. 2013;37(10):1490-504. 6. Lohse CM, Blute ML, Zincke H, Weaver AL, Cheville JC. Comparison of standardized and nonstandardized nuclear grade of renal cell carcinoma to predict outcome among 2,042 patients. American journal of clinical pathology. 2002;118(6):877-86. 7. Delahunt B. Advances and controversies in grading and staging of renal cell carcinoma. Modern Pathology. 2009;22(S2):S24. 8. Lechevallier E, André M, Barriol D, Daniel L, Eghazarian C, De Fromont M, et al. Fine-needle percutaneous biopsy of renal masses with helical CT guidance. Radiology. 2000;216(2):506-10. 9. Al Nazer M, Mourad WA. Successful grading of renal‐cell carcinoma in fine‐needle aspirates. Diagnostic cytopathology. 2000;22(4):223-6. 10. Kutikov A, Kunkle DA, Uzzo RG. Focal therapy for kidney cancer: a systematic review. Current opinion in urology. 2009;19(2):148-53. 11. Volpe A, Panzarella T, Rendon RA, Haider MA, Kondylis FI, Jewett MA. The natural history of incidentally detected small renal masses. Cancer. 2004;100(4):738-45. 12. Bratslavsky G, Kirkali Z. The changing face of renal-cell carcinoma. Journal of endourology. 2010;24(5):753-7. 13. Kunkle DA, Egleston BL, Uzzo RG. Excise, ablate or observe: the small renal mass dilemma—a meta-analysis and review. The Journal of urology. 2008;179(4):1227-34. 14. Yoshida R, Yoshizako T, Hisatoshi A, Mori H, Tamaki Y, Ishikawa N, et al. The additional utility of apparent diffusion coefficient values of clear-cell renal cell carcinoma for predicting metastasis during clinical staging. Acta radiologica open. 2017;6(1):2058460116687174. 15. Ishigami K, Leite LV, Pakalniskis MG, Lee DK, Holanda DG, Kuehn DM. Tumor grade of clear cell renal cell carcinoma assessed by contrast-enhanced computed tomography. SpringerPlus. 2014;3(1):694. 16. Ding J, Xing Z, Jiang Z, Chen J, Pan L, Qiu J, et al. CT-based radiomic model predicts high grade of clear cell renal cell carcinoma. European journal of radiology. 2018;103:51-6. 17. Shiri I, Maleki H, Hajianfar G, Abdollahi H, Ashrafinia S, Hatt M, et al. Next Generation Radiogenomics Sequencing for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients Using Multimodal Imaging and Machine Learning Approaches. arXiv preprint arXiv:190702121. 2019. 18. Hajianfar G, Shiri I, Maleki H, Oveisi N, Haghparast A, Abdollahi H, et al. Non-Invasive MGMT Status Prediction in GBM Cancer Using Magnetic Resonance Images Radiomics Features: Univariate and Multivariate Radiogenomics Analysis. World Neurosurgery. 2019.  19. Shiri I, Maleki H, Hajianfar G, Abdollahi H, Ashrafinia S, Oghli MG, et al., editors. PET/CT Radiomic Sequencer for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients. 2018 IEEE Nuclear Science Symposium and Medical Imaging Conference Proceedings (NSS/MIC); 2018: IEEE. 20. Abdollahi H, Mahdavi SR, Shiri I, Mofid B, Bakhshandeh M, Rahmani K. Magnetic resonance imaging radiomic feature analysis of radiation-induced femoral head changes in prostate cancer radiotherapy. Journal of cancer research and therapeutics. 2019;15(8):11. 21. Lambin P, Leijenaar RT, Deist TM, Peerlings J, De Jong EE, Van Timmeren J, et al. Radiomics: the bridge between medical imaging and personalized medicine. Nature Reviews Clinical Oncology. 2017;14(12):749. 22. Abdollahi H, Shiri I, Heydari M. Medical Imaging Technologists in Radiomics Era: An Alice in Wonderland Problem. Iranian journal of public health. 2019;48(1):184. 23. Shiri I, Rahmim A, Ghaffarian P, Geramifar P, Abdollahi H, Bitarafan-Rajabi A. The impact of image reconstruction settings on 18F-FDG PET radiomic features: multi-scanner phantom and patient studies. European radiology. 2017;27(11):4498-509. 24. Shiri I, Ghafarian P, Geramifar P, Leung KH-Y, Ghelichoghli M, Oveisi M, et al. Direct attenuation correction of brain PET images using only emission data via a deep convolutional encoder-decoder (Deep-DAC). European radiology. 2019:1-13. 25. Shiri I, Abdollahi H, Shaysteh S, Mahdavi SR. Test-retest reproducibility and robustness analysis of recurrent glioblastoma MRI radiomics texture features. Iranian Journal of Radiology. 2017(5). 26. Abdollahi H, Mofid B, Shiri I, Razzaghdoust A, Saadipoor A, Mahdavi A, et al. Machine learningbased radiomic models to predict intensity-modulated radiation therapy response, Gleason score and stage in prostate cancer. La radiologia medica. 2019;124(6):555-67. 27. Abdollahi H, Mostafaei S, Cheraghi S, Shiri I, Mahdavi SR, Kazemnejad A. Cochlea CT radiomics predicts chemoradiotherapy induced sensorineural hearing loss in head and neck cancer patients: a machine learning and multi-variable modelling study. Physica Medica. 2018;45:192-7. 28. Bektas CT, Kocak B, Yardimci AH, Turkcanoglu MH, Yucetas U, Koca SB, et al. Clear cell renal cell carcinoma: machine learning-based quantitative computed tomography texture analysis for prediction of Fuhrman nuclear grade. European radiology. 2019;29(3):1153-63. 29. Feng Z, Shen Q, Li Y, Hu Z. CT texture analysis: a potential tool for predicting the Fuhrman grade of clear-cell renal carcinoma. Cancer Imaging. 2019;19(1):6. 30. Akin O, Elnajjar P, Heller M, Jarosz R, Erickson B, Kirk S, et al. Radiology data from the cancer genome atlas kidney renal clear cell carcinoma [TCGA-KIRC] collection. The Cancer Imaging Archive. 2016. 31. Shafiq‐ul‐Hassan M, Zhang GG, Latifi K, Ullah G, Hunt DC, Balagurunathan Y, et al. Intrinsic dependencies of CT radiomic features on voxel size and number of gray levels. Medical physics. 2017;44(3):1050-62. 32. Haynes W. Benjamini–hochberg method. Encyclopedia of systems biology. 2013:78-. 33. Kickingereder P, Götz M, Muschelli J, Wick A, Neuberger U, Shinohara RT, et al. Large-scale radiomic profiling of recurrent glioblastoma identifies an imaging predictor for stratifying antiangiogenic treatment response. Clinical Cancer Research. 2016;22(23):5765-71. 34. Guo P, Zeng F, Hu X, Zhang D, Zhu S, Deng Y, et al. Improved variable selection algorithm using a LASSO-type penalty, with an application to assessing hepatitis B infection relevant factors in community residents. PloS one. 2015;10(7):e0134151. 35. Rosenkrantz AB, Niver BE, Fitzgerald EF, Babb JS, Chandarana H, Melamed J. Utility of the apparent diffusion coefficient for distinguishing clear cell renal cell carcinoma of low and high nuclear grade. American Journal of Roentgenology. 2010;195(5):W344-W51.  36. Maruyama M, Yoshizako T, Uchida K, Araki H, Tamaki Y, Ishikawa N, et al. Comparison of utility of tumor size and apparent diffusion coefficient for differentiation of low-and high-grade clear-cell renal cell carcinoma. Acta Radiologica. 2015;56(2):250-6. 37. Zhang J, Mazaheri Tehrani Y, Wang L, Ishill NM, Schwartz LH, Hricak H. Renal masses: characterization with diffusion-weighted MR imaging—a preliminary experience. Radiology. 2008;247(2):458-64. 38. Goyal A, Sharma R, Bhalla AS, Gamanagatti S, Seth A, Iyer VK, et al. Diffusion-weighted MRI in renal cell carcinoma: a surrogate marker for predicting nuclear grade and histological subtype. Acta Radiologica. 2012;53(3):349-58. 39. Shu J, Tang Y, Cui J, Yang R, Meng X, Cai Z, et al. Clear cell renal cell carcinoma: CT-based radiomics features for the prediction of Fuhrman grade. European journal of radiology. 2018;109:8-12. 40. Breiman L. Random Forests, Vol. 45. Mach Learn. 2001;1.  20  "
"Published as a conference paper at ICLR 2020  ATTENTION F ORCING M ODEL T RAINING  FOR  S EQUENCE - TO - SEQUENCE  arXiv:1909.12289v1 [cs.LG] 26 Sep 2019  Qingyun Dou, Yiting Lu, Joshua Efiong & Mark J. F. Gales Cambridge University Engineering Department Trumpington St, Cambridge CB2 1PZ {qd212,ytl28,je369,mjfg100}@cam.ac.uk  1  I NTRODUCTION  Auto-regressive sequence-to-sequence (seq2seq) models with attention mechanism are widely used in a variety of areas including Neural Machine Translation (NMT) (Neubig, 2017; Huang et al., 2016) and speech synthesis (Shen et al., 2018; Wang et al., 2018), also known as Text-To-Speech (TTS). These models excel at connecting sequences of different length, but can be difficult to train. A standard approach is teacher forcing, which guides a model with reference output history during training. This makes the model unlikely to recover from its mistakes during inference, where the reference output is replaced by generated output. One alternative is to train the model in free running mode, where the model is guided by generated output history. This approach often struggles to converge, especially for attention-based models, which need to infer the correct output and align it with the input at the same time. Several approaches are introduced to tackle the above problem, namely scheduled sampling (Bengio et al., 2015) and professor forcing (Lamb et al., 2016). Scheduled sampling randomly decides, for each time step, whether the reference or generated output token is added to the output history. The probability of choosing the reference output token decays from 1 to 0 with a heuristic schedule. A natural extension is sequence-level scheduled sampling, where the decision is made for each sequence instead of token. Professor forcing views the seq2seq model as a generator. During training, the generator operates in both teacher forcing mode and free running mode. In teacher forcing mode, it tries to maximize the standard likelihood. In free running mode, it tries to fool a discriminator, which is trained to tell if the model is running in teacher forcing mode. To make training stable, the above approaches require either a well tuned schedule, or a well trained discriminator. This paper introduces attention forcing, which guides the model with generated output history and reference attention. This approach makes training stable by decoupling the learning of the output 1  Published as a conference paper at ICLR 2020  and that of the alignment. There is no need for a schedule or a discriminator. Furthermore, for cascaded systems like many TTS systems, attention forcing can be particularly useful. A model trained with attention forcing can generate (in attention forcing mode) output sequences aligned with the references. These output sequences can be used to train a downstream model, enabling it to fix some upstream errors. The TTS experiments show that attention forcing yields significant gain in speech quality. The NMT experiments show that for tasks where various re-orderings of the output are valid, guiding the model with generated output history can be problematic, while guiding the model with reference attention yields slight but consistent gain in BLEU score (Papineni et al., 2002).  2  S EQUENCE - TO - SEQUENCE GENERATION  Sequence-to-sequence generation can be defined as the problem of mapping an input sequence x1:L to an output sequence y1:T . From a probabilistic perspective, a model θ estimates the distribution of y1:T given x1:L , typically as a product of distributions conditioned on output history: QT p(y1:T |x1:L ; θ) = t=1 p(yt |y1:t−1 , x1:L ; θ) (1) Ideally, the model is trained through minimizing the KL-divergence between the true distribution p(y1:T |x1:L ) and the estimated distribution:  θ̂ = argmin Ex1:L ∼p(x1:L ) KL p(y1:T |x1:L )||p(y1:T |x1:L ; θ) θ (2)  = argmin Ex1:L ∼p(x1:L ) Ey1:T ∼p(y1:T |x1:L ) log p(y1:T |x1:L )/p(y1:T |x1:L ; θ) θ  In practice, this is approximated by minimizing the Negative Log-Likelihood (NLL) of some training (n) (n) data {y1:T , x1:L }N 1 , sampled from the true distribution: PN (n) (n) θ̂ = argmin − n=1 log p(y1:T |x1:L ; θ) (3) θ  While L and T are functions of n, the subscripts are omitted to simplify notations, i.e. Ln and Tn are written as L and T . At inference stage, given an input x∗1:L , the output ŷ1:T can be obtained through searching for the most probable sequence from the estimated distribution: ŷ1:T = argmax p(y1:T |x∗1:L ; θ̂) (4) y1:T  The exact search is computationally expensive, and is often approximated by greedy search if the output space is continuous, or beam search if the output space is discrete (Bengio et al., 2015). 2.1  ATTENTION - BASED SEQ 2 SEQ MODEL  Attention mechanisms (Bahdanau et al., 2014; Chorowski et al., 2015) are commonly used to connect sequences of different length. This paper focuses on attention-based encoder-decoder models. For these models, the probability p(yt |y1:t−1 , x1:L ; θ) is estimated as: p(yt |y1:t−1 , x1:L ; θ) ≈ p(yt |y1:t−1 , αt , x1:L ; θ) ≈ p(yt |st , ct ; θy ) (5) st = f (y1:t−1 ; θs ) (6) ct = f (αt , x1:L ; θc ) (7) θ = {θy , θs , θc }. αt is an alignment vector (a set of attention weights). st is a state vector representing the output history y1:t−1 , and ct is a context vector summarizing x1:L for the prediction of yt . The following equations, as well as figure 1, give a more detailed illustration of how αt , st and ct can be computed: h1:L = f (x1:L ; θh ) (8) st = f (st−1 , yt−1 ; θs ) (9) αt = f (st , h1:L ; θα ) (10) PL ct = l=1 αt,l hl (11) ŷt ∼ p(yt |st , ct ; θy ) (12) First the encoder maps x1:L to an encoding sequence h1:L . For each decoder time step, st is updated with yt−1 . Based on h1:L and st , the attention mechanism computes αt , and then ct as the weighted sum of h1:L . Finally, the decoder estimates a distribution based on st and ct , and optionally generates an output token ŷt by either sampling or taking the most probable token. Note that the output history y1:t−1 plays an important role, as it impacts p(yt |st , ct ; θy ) through both st and ct . Also note that there are many forms of attention-based encoder-decoder models. While attention forcing is illustrated with this particular form, it is not limited to it. 2  Published as a conference paper at ICLR 2020  Figure 1: Illustration of an attention-based encoder-decoder model 2.2  T RAINING APPROACHES  As shown in equations 2 and 3, minimizing the KL-divergence between the true distribution and the model distribution can be approximated by minimizing the NLL. This motivates the approach to train the model in teacher forcing mode, where p(yt |y1:t−1 , x1:L ; θ) is computed with the correct output history y1:t−1 , as shown in equations 5 and 6. In this case, the loss can be written as: PN PN PT (n) (n) (n) (n) (n) (13) L(T) y (θ) = − n=1 log p(y1:T |x1:L ; θ) = − n=1 t=1 log p(yt |y1:t−1 , x1:L ; θ) This approach yields the correct model (zero KL-divergence) if the following assumptions hold: 1) the model is powerful enough ; 2) the model is optimized correctly; 3) there is enough training data to approximate the expectation shown in equation 2. In practice, these assumptions are often not true, hence the model is prone to make mistakes. To illustrate the problem, suppose there is a ∗ for the test input x∗1:L . Due to data sparsity in high-dimensional space, x∗1:L reference output y1:T ∗ , x∗1:L ; θ) is wrongly estimated is likely to be unseen during training. If the probability p(yt∗ |y1:t−1 ∗ |x∗1:L ; θ) will also to be small at time step t, the probability of the reference output sequence p(y1:T ∗ be small, i.e. it will be unlikely for the model to generate y1:T . ∗ , ŷ1:T ) between the reference outIn practice, the model can be assessed by some loss D(y1:T ∗ put y1:T and the generated output ŷ1:T . Taking the expected value yields the Bayes risk: ∗ , ŷ1:T ). This motivates training the model with the following loss: Eŷ1:T ∼p(y1:T |x∗1:L ;θ) D(y1:T PN (n) L(B) D(y1:T , ŷ1:T ) y (θ) = n=1 Eŷ1:T ∼p(y1:T |x(n) 1:L ;θ) (14) PN PM (n,m) (n) (n) (n,m) ≈ n=1 m=1 p(ŷ1:T |x1:L ; θ)D(y1:T , ŷ1:T ) (n)  ŷ (n,m) is sampled from the estimated distribution p(y1:T |x1:L ; θ). D is minimal when the two sequences are equal. So the model is trained to not only assign high probability to the reference sequences in the training data, but also assign low probability to other sequences. This makes minimum Bayes risk training prone to overfitting. Very often, D is computed at sub-sequence level. Examples include BLEU score for NMT, word error rate for speech recognition and root mean square error for TTS. So if an approach trains the model to predict the reference output, based on erroneous output history, it will indirectly reduce the Bayes risk. One example is to train the model in free running mode, where p(yt |y1:t−1 , x1:L ; θ) is estimated with the generated output history: p(yt |y1:t−1 , x1:L ; θ) ≈ p(yt |ŷ1:t−1 , x1:L ; θ) ≈ p(yt |st , ct ; θy ) (15) st = f (ŷ1:t−1 ; θs ) (16) ŷt is obtained from the estimated distribution p(yt |st , ct ; θy ), as shown in equation 12. (The approaches discussed in this section are designed for all auto-regressive models, with or without attention mechanism. So the realization ct is not shown.) The corresponding loss function is: PN PT (n) (n) (n) (17) L(F) y (θ) = − t=1 log p(yt |ŷ1:t−1 , x1:L ; θ) n=1 Note that if there is enough data and modeling power, and the model is optimized correctly, the QT distribution t=1 p(yt |ŷ1:t−1 , x1:L ; θ) can be the same as the true distribution p(y1:T |x1:L ). The problem with this approach is that training often struggles to converge. One concern is that the model needs to learn to infer the correct output and align that with the input at the same time. Therefore, several approaches, namely scheduled sampling and professor forcing, are proposed to train the model in a mode between teacher forcing and free running. Scheduled sampling (Bengio et al., 2015) randomly decides, for each time step, whether the reference or generated output token is added to the output history ye1:t−1 . For this approach, 3  Published as a conference paper at ICLR 2020  p(yt |y1:t−1 , x1:L ; θ) is estimated as: p(yt |y1:t−1 , x1:L ; θ) ≈ p(yt |e y1:t−1 , x1:L ; θ) ≈ p(yt |st , ct ; θy ) st = f (e y1:t−1 ; θs )  yt with probability  yet = ŷt with probability 1 −   (18) (19) (20)   gradually decays from 1 to 0 with a heuristic schedule. Considering that during training, ye1:t−1 is mostly an inconsistent mixture of the reference output and the generated output, a natural extension is sequence-level scheduled sampling (Bengio et al., 2015), where the decision is made for each sequence instead of token:  y1:t−1 with probability  ye1:t−1 = (21) ŷ1:t−1 with probability 1 −  This type of training improves the results of many experiments, but sometimes leads to worse results (Wang et al., 2017; Bengio et al., 2015). One concern is that the decay schedule does not fit the learning pace of the model. Professor forcing (Lamb et al., 2016) is an alternative trade-off. During training, the model θ is viewed as a generator, which generates two output sequences for each input sequence, respectively 0(n) (n) (n) in teacher forcing mode and free running mode1 . For the training example {y1:T , x1:L }, let y1:T (n) denote the output generated in teacher forcing mode, and ŷ1:T the output generated in free running forcing mode, this can be expressed as: 0(n)  ∼ p(yt |y1:t−1 , x1:L ; θ)  (n) ∀t ŷt  (n) (n) p(yt |ŷ1:t−1 , x1:L ; θ)  ∀t yt  (n)  ∼  (n)  (22) (23) 0(n) β1:T  (n)  In addition to the final output, some intermediate output sequences are saved. Let and β̂1:T denote the intermediate output sequences generated respectively in teacher forcing and free running 0(n) 0(n) (n) (n) mode. These generated sequences form a dataset {y1:T , β1:T , ŷ1:T , β̂1:T }N 1 that is used to train a discriminator ψ. ψ is trained to predict the probability that a group of sequences is generated in teacher forcing mode, and the loss function is:   PN  0(n) 0(n) (n) (n) Lψ (ψ|θ) = − n=1 log f (y1:T , β1:T ; ψ) + log 1 − f (ŷ1:T , β̂1:T ; ψ) (24) While this loss function is optimized w.r.t. ψ, it depends on θ, hence the notation ψ|θ. For the generator θ, there are three training objectives. The first one is the standard likelihood shown in equation 13. The second one is to fool the discriminator in free running mode:  PN (F) (n) (n) Lβ (θ|ψ) = − n=1 log f (ŷ1:T , β̂1:T ; ψ) (25) The third one, which is optional, is to fool the discriminator in teacher forcing mode:  PN (T) 0(n) 0(n) Lβ (θ|ψ) = − n=1 log 1 − f (y1:T , β1:T ; ψ)  (26)  This approach makes the distribution p(yt |ŷ1:t−1 , x1:L ; θ) estimated in free running mode similar to the corresponding distribution p(yt |y1:t−1 , x1:L ; θ) estimated in teacher forcing mode. In addition, it regularizes some hidden layers, encouraging them to behave as if in teacher forcing mode. The disadvantage is that it requires designing and training the discriminator.  3 3.1  ATTENTION FORCING G UIDING THE MODEL WITH ATTENTION  For attention-based seq2seq generation, we propose a new algorithm: attention forcing. The basic idea is to use reference attention (i.e. reference alignment) and generated output to guide the model during training. In attention forcing mode, the model does not need to learn to simultaneously infer 1 The term ”teacher forcing”, as well as ”attention forcing”, can refer to either an operation mode, or the approach to train a model in that operation mode. An operation mode can be used not only to train a model, but also to generate from it. For example, in teacher forcing mode, given the reference output y1:T , a model can 0 0 generate a guided output y1:T , without evaluating the loss. y1:T is likely to be different but similar to y1:T , and can be useful for training the discriminator.  4  Published as a conference paper at ICLR 2020  Figure 2: Illustration of attention forcing the output and align it with the input. As the reference alignment is known, the decoder can focus on inferring the output, and the attention mechanism can focus on generating the correct alignment. Let θ̂ denote the model that is trained in attention forcing mode, and later used for inference. In attention forcing mode, p(yt |y1:t−1 , x1:L ; θ̂) is estimated with the generated output ŷ1:t−1 and the reference alignment αt , and equation 5 becomes: p(yt |y1:t−1 , x1:L ; θ̂) ≈ p(yt |ŷ1:t−1 , αt , x1:L ; θ̂) ≈ p(yt |ŝt , ĉt ; θ̂y ) (27) ŝt and ĉt denote the state vector and context vector generated by θ̂. Details of attention forcing can be illustrated by figure 2, as well as the following equations: h1:L = f (x1:L ; θh ) ĥ1:L = f (x1:L ; θ̂h ) (28) st = f (st−1 , yt−1 ; θs ) αt = f (st , h1:L ; θα ) ĉt =  PL  l=1  ŝt = f (ŝt−1 , ŷt−1 ; θ̂s )  (29)  α̂t = f (ŝ1:t−1 , ĥ1:L ; θ̂α )  (30) (31)  αt,l ĥl  ŷt ∼ p(yt |ŝt , ĉt ; θ̂y )  (32)  The right side of the equations 28 to 30, as well as equations 31 and 32, show how the attention forcing model θ̂ operates. ĥl and α̂t denote the encoding and alignment vectors generated by θ̂. ŝt is computed with ŷ1:t−1 . While an alignment α̂t is generated by θ̂, it is not used by the decoder, because ĉt is computed with the reference alignment αt . In most cases, αt is not available. One option of obtaining it is shown by the left side of equations 28 to 30, which is the same as equations 8 to 10. The option is to generate αt from a teacher forcing model θ. θ is trained in teacher forcing mode, as described in section 2.2. Once trained, it can generate αt , again in teacher forcing mode. During inference, the attention forcing model operates in free running mode. In this case, equation PL 31 becomes ĉt = l=1 α̂t,l ĥl . The decoder is guided by α̂t , instead of αt . During training, there are two objectives: to infer the reference output and to imitate the reference alignment. For the first objective, the loss function is: PN PT (n) (n) (n) (n) (33) L(A) y (θ, θ̂) = − n=1 t=1 log p(yt |ŷ1:t−1 , αt , x1:L ; θ, θ̂) For the second objective, as an alignment corresponds to a categorical distribution, the loss function is the average KL-divergence between the reference alignment and the generated alignment: (n) PN PT PL PN PT αt,l (n) (n) (n) L(A) (34) α (θ, θ̂) = t=1 l=1 αt,l log (n) n=1 t=1 KL(αt ||α̂t ) = n=1 α̂t,l  (A)  (A)  (A)  The two losses can be jointly optimized as Ly,α = Ly + γLα . γ is a scaling factor that should be set according to the dynamic range of the two losses, which roughly indicates the norm of the (A) gradient. The alignment loss Lα can be interpreted as a regularization term, which encourages the attention mechanism of θ̂ to behave like that of θ. Our default optimization option is as follows. θ is (T) trained in teacher forcing mode, with the loss Ly shown in equation 13, and then fixed to generate (A) the reference attention. θ̂ is trained with the joint loss Ly,α . In our experiments, this option makes training more stable, most probably because the reference attention is the same from epoch to epoch. There are several alternative options. One example is to tie θ and θ̂, i.e. use only one set of model (A) parameters, and train it with the joint loss Ly,α . This option is less stable, but more efficient. 5  Published as a conference paper at ICLR 2020  Figure 3: Illustration of a speech synthesis system  3.2  C OMPARISON WITH RELATED APPROACHES  Intuitively, attention forcing, as well as scheduled sampling and professor forcing, is in the middle of teacher forcing and free running. Unlike scheduled sampling, attention forcing does not require a decay schedule, which can be difficult to tune. While the scaling factor γ is hyper parameter, it can be set according to the dynamic ranges of the two losses, as described in section 3.1. In addition, it can be tuned according the alignment vector, which is an interpretable indicator of how well the attention mechanism works. In terms of regularization, attention forcing is similar to professor forcing. The output layer of the attention mechanism, which can be viewed as a special hidden layer, is encouraged to behave as if in teacher forcing mode. The difference is that attention forcing does not require a discriminator to learn a loss function, as the KL-divergence is natural loss function for the alignment vector. A limitation of attention forcing is that it is less general than the approaches described in section 2.2, which are well defined for all auto-regressive models, with or without attention mechanism. To apply attention forcing to a model without attention mechanism, attention needs to be defined first. For convolutional neural networks, for example, attention maps can be defined based on activation or gradient (Zagoruyko & Komodakis, 2016).  4  A PPLICATION TO SPEECH SYNTHESIS  Attention forcing has a feature that is essential for many cascaded systems: when the reference alignment is available, the output can be generated in attention forcing mode, and will be aligned with the reference. TTS is a typical example. For TTS, the task is to map a sequence of characters x1:L to a sequence of waveform samples w1:J . Directly mapping x1:L to w1:J is difficult because the two sequences are not aligned and are orders of magnitude different in length. (10 characters can correspond to more than 1000 waveform samples.) As shown in figure 3, TTS is often realized by first mapping x1:L to a vocoder feature sequence y1:T , and then mapping y1:T to w1:J . The vocoder feature sequence is a compact and interpretable representation of the waveform; a vocoder can be used to map vocoder features to waveform or reversely, with a series of signal processing techniques. Each feature frame corresponds to a window of waveform samples, i.e. each time step in the feature sequence corresponds to a fixed number of time steps in the waveform sequence. The model mapping x1:L to y1:T can be referred to as the frame-level model θ, and the model mapping y1:T to w1:J can be referred to as the waveform-level model φ. Conventionally, φ is a vocoder, and is not learnable. θ contains a text processing frontend, a duration model and a feature model (Li et al., 2018). The text processing frontend extracts linguistic features from x1:L ; the duration model predicts the duration of each linguistic feature; the feature model maps the linguistic features to y1:T . This paper focuses on the state-of-the-art approach, where θ, as well as φ, is a neural network. φ can be considered a neural vocoder, which is not limited by the assumptions made by the conventional vocoders (Lorenzo-Trueba et al., 2018; Kalchbrenner et al., 2018). θ is an attention-based seq2seq model, as described in section 2.1. Compared with the conventional approach, the attention-based model has several advantages, such as performance gain and less need for data labeling (Wang et al., 2017). Note that as shown in figure 3, θ learns not only to map a character sequence to a feature sequence, but also to align them. In contrast, φ does not align its input and output (Shen et al., 2018; Oord et al., 2016). 6  Published as a conference paper at ICLR 2020  (n)  (n)  (n)  (n)  The training dataset {w1:J , x1:L }N 1 usually contains pairs of waveform w1:J and text x1:L . (To simplify notations, the superscript (n) is be omitted by default in the following discussion.) For each w1:J , a vocoder feature sequence y1:T can be extracted. The frame-level model θ is trained with {y1:T , x1:L }. The waveform-level model φ can be trained with {w1:J , y1:T }, or {w1:J , ŷ1:T }, where ŷ1:T is generated by θ. Training with ŷ1:T allows φ to fix some mistakes made by θ, but this is only possible when ŷ1:T is aligned with w1:J . To ensure the alignment, the standard approach is to train θ in teacher forcing mode, and then generate from it in the same mode. This paper proposes an alternative approach: to use attention forcing instead of teacher forcing. As analyzed in section 3.1, training θ with attention forcing improves its performance. Furthermore, in attention forcing mode, each output ŷt is predicted based on ŷ1:t−1 (instead of y1:t−1 ), hence ŷ1:T is more likely (than in teacher forcing mode) to contain errors that θ makes at inference stage. Training φ with ŷ1:T can enable it to correct the errors, improving the quality of the waveform. Note that if θ is trained with scheduled sampling or professor forcing, it is often not possible to predict, based only on generated output history, a vocoder feature sequence aligned with the reference waveform. Also note that φ is trained in teacher forcing mode, as it does not have attention mechanism. Hence the rest of this section focuses on discussing θ at training stage and inference stage. During training, it is often assumed that the output tokens follow a certain type of distribution, so that (A) minimizing the loss Ly shown in equation 33 can be approximated by minimizing some distance metric between y1:T and ŷ1:T . For example, assuming that the distribution shown in equation 27 is (A) a Laplace distribution, minimizing Ly is equivalent to minimizing the average `1 distance: PN PT (n) (n) (A) (35) argmin Ly (θ, θ̂) ≈ argmin n=1 t=1 ||yt − ŷt ||1 θ,θ̂  θ,θ̂  ŷt = argmax p(yt |ŷ1:t−1 , αt , x1:L ; θ̂)  (36)  yt  The notation is the same as in section 3.1. θ̂ denotes the attention forcing model; θ denotes the teacher forcing model generating reference alignment. Equation 36 replaces equation 32. In this case, ŷt is not sampled, and is always the mode of the predicted distribution. During inference, the exact search (equation 4) is approximated by greedy search: (Note that for TTS, the main difference between training and inference is the alignment, which influences duration more than quality.) ∀t ŷt = argmax p(yt |ŷ1:t−1 , α̂t , x∗1:L ; θ̂) (37) yt  5 5.1  E XPERIMENTS S PEECH S YNTHESIS  The TTS experiments are conducted on LJ dataset (Ito, 2017), which contains 13,100 utterances from a single speaker. The utterances vary in length from 1 to 10 seconds, totaling approximately 24 hours. A transcription is provided for each waveform, and the corresponding vocoder features are extracted with PML vocoder (Degottex et al., 2016). The training-validation-test split is 1300050-50. The waveform-level model is the Hierarchical Recurrent Neural Network (HRNN) neural vocoder (Mehri et al., 2016; Dou et al., 2018). The model structure is exactly the same as described in Dou et al. (2018), and the model configuration is adjusted for efficiency. The frame-level model is very similar to Tacotron (Wang et al., 2017). The model structure and configuration are the same as described in Wang et al. (2017), except that: 1) the decoder target is vocoder features; 2) the attention mechanism is the hybrid (content-based + location-based) attention (Chorowski et al., 2015); 3) each decoding step predicts 5 vocoder feature frames. The neural vocoder is always trained with teacher forcing. The frame-level model is trained with either teacher forcing or attention forcing. Details of the setup (data, models and training) are presented in appendix A.2.1. Two TTS systems are built: a teacher forcing system and an attention forcing system. For the teacher forcing system, the frame-level model θ is trained in teacher forcing mode. The neural vocoder φ is trained with the vocoder features generated (in teacher forcing mode) by θ. For the attention forcing system, the frame-level model θ̂ is trained in attention forcing mode, with reference attention generated (in teacher forcing mode) by θ. At this stage, θ̂ is updated, while θ is fixed. The neural vocoder φ̂ is trained with the vocoder features generated (in attention forcing mode) by θ̂. At inference stage, all the models operate in free-running mode. 7  Published as a conference paper at ICLR 2020  Figure 4: Result of the listening test comparing teacher forcing and attention forcing For TTS, human perception is the gold-standard. The two systems are compared in a subjective listening test. Over 30 workers from Amazon Mechanical Turk are instructed to listen to pairs of utterances, and indicate which one they prefer in terms of overall quality. Each comparison includes 5 pairs of utterances randomly selected among all the test utterances. Figure 4 shows the result of the listening test. Each number indicates the percentage of a certain preference. Most participants prefer attention forcing. We strongly encourage readers to listens to the generated utterances2 . It is obvious that attention forcing yields utterances that are significantly more natural and expressive. 5.2  M ACHINE TRANSLATION  The NMT experiments are conducted on the English-to-Vietnamese task in IWSLT 2015. It is a low resource NMT task, where training set contains 133K sentence pairs. The Stanford pre-processed data is used. The TED tst2012 is used as a validation set, and BLEU scores on TED tst2013 are reported. The scores use a 4-gram corpus level BLEU with equal weights. Google’s attention-based encoder-decoder LSTM model (Wu et al., 2016) is adopted. Details of the setup (data, model and training) are presented in appendix A.2.2. Our initial experiments show that directly applying attention forcing to NMT can degrade the performance. One concern is that for translation, various re-orderings of the output sequence are valid. In this case, guiding the model with generated output can be problematic, as the reference output can take an ordering that is different from the generated output. To see if this is the reason, we tried a modified attention forcing mode, where the model is guided with reference attention and reference output. The right side of equation 29 becomes: ŝt = f (ŝt−1 , yt−1 ; θ̂s ). ŝt is computed with the reference output y1:t−1 , and matches the reference attention αt Other parts of attention forcing (equations 28 to 31) stay the same, hence ŷt is predicted with y1:t−1 and αt . In the following experiments, two NMT models are compared: one is trained in teacher forcing mode, with the NLL loss in equation 13; the other is trained in the modified attention forcing mode described above, with both the NLL loss and the attention loss in equation 34. An ensemble of 10 models are trained with teacher forcing. Then each model generates reference attention for a corresponding model trained with additional attention loss. The average performance of the teacher forcing models is 26.35 BLEU, and adding the attention loss yields an average +0.35 BLEU gain. 9 of out 10 times, the performance improves. The slight but consistent gain shows that for NMT, guiding the model with generated output is indeed the cause degrading the performance. It also shows that guiding the model with reference attention can be beneficial. One possible reason is that the attention loss regularizes the attention mechanism. Another is that the model does not need to learn to simultaneously infer the output and align it with the input.  6  C ONCLUSION  This paper introduces attention forcing, which guides a seq2seq model with generated output history and reference attention. This approach can train the model to recover from its mistakes, in a stable fashion, without the need for a schedule or a classifier. In addition, it allows the model to generate output sequences aligned with the reference output sequences, which can be important for cascaded systems like many TTS systems. The TTS experiments show that attention forcing yields significant gain in speech quality. The NMT experiments show that for tasks where various re-orderings of the output are valid, guiding the model with generated output history can be problematic, while guiding the model with reference attention yields slight but consistent gain in BLEU score. 2 Generated test utterances are randomly selected and made available at http://mi.eng.cam.ac.uk/ ˜qd212/iclr2020/samples.html  8  Published as a conference paper at ICLR 2020  R EFERENCES Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pp. 1171–1179, 2015. Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. In Advances in Neural Information Processing Systems, pp. 577–585, 2015. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014. Gilles Andre Degottex, Pierre Kim Lanchantin, and Mark John Gales. A pulse model in log-domain for a uniform synthesizer. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 230–236. IEEE, 2016. Qingyun Dou, Moquan Wan, Gilles Degottex, Zhiyi Ma, and Mark JF Gales. Hierarchical rnns for waveform-level speech synthesis. In 2018 IEEE Spoken Language Technology Workshop (SLT), pp. 618–625. IEEE, 2018. Po-Yao Huang, Frederick Liu, Sz-Rung Shiang, Jean Oh, and Chris Dyer. Attention-based multimodal neural machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pp. 639–645, 2016. Keith Ito. The lj speech dataset. https://keithito.com/LJ-Speech-Dataset/, 2017. Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart, Florian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Efficient neural audio synthesis. arXiv preprint arXiv:1802.08435, 2018. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Alex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying Zhang, Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances In Neural Information Processing Systems, pp. 4601–4609, 2016. Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, Ming Liu, and Ming Zhou. Close to human quality tts with transformer. arXiv preprint arXiv:1809.08895, 2018. Jaime Lorenzo-Trueba, Thomas Drugman, Javier Latorre, Thomas Merritt, Bartosz Putrycz, and Roberto Barra-Chicote. Robust universal neural vocoding. arXiv preprint arXiv:1811.06292, 2018. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint a"
